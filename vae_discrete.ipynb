{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"vae_discrete.ipynb","provenance":[],"authorship_tag":"ABX9TyOjOgcvXZCd3a2J/nRTmtvl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iIHJVZ-mnMqr","executionInfo":{"status":"ok","timestamp":1655469134159,"user_tz":-120,"elapsed":2479,"user":{"displayName":"Rajmohan Tumarada","userId":"18412595153668734700"}},"outputId":"6da4c906-45dc-4a6e-d47c-b0bd023fd54e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["%cd '/content/drive/MyDrive/Colab Notebooks/optMl'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3j0EKX47nWeo","executionInfo":{"status":"ok","timestamp":1655469134161,"user_tz":-120,"elapsed":10,"user":{"displayName":"Rajmohan Tumarada","userId":"18412595153668734700"}},"outputId":"579d42ff-25c1-423c-d9dd-c3647bf32fde"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/optMl\n"]}]},{"cell_type":"code","source":["# Code to implement VAE-gumple_softmax in pytorch\n","# author: Devinder Kumar (devinder.kumar@uwaterloo.ca), modified by Yongfei Yan\n","# The code has been modified from pytorch example vae code and inspired by the origianl \\\n","# tensorflow implementation of gumble-softmax by Eric Jang.\n","\n","import sys\n","import argparse\n","import numpy as np\n","\n","import torch\n","import torch.nn.functional as F\n","from torch import nn, optim\n","from torch.nn import functional as F\n","from torchvision import datasets, transforms\n","from torchvision.utils import save_image"],"metadata":{"id":"ESnsMT_vnWpW","executionInfo":{"status":"ok","timestamp":1655469134162,"user_tz":-120,"elapsed":9,"user":{"displayName":"Rajmohan Tumarada","userId":"18412595153668734700"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["parser = argparse.ArgumentParser(description='VAE MNIST Example')\n","parser.add_argument('--batch-size', type=int, default=128, metavar='N',\n","                    help='input batch size for training (default: 100)')\n","parser.add_argument('--epochs', type=int, default=15, metavar='N',\n","                    help='number of epochs to train (default: 10)')\n","parser.add_argument('--temp', type=float, default=1.0, metavar='S',\n","                    help='tau(temperature) (default: 1.0)')\n","parser.add_argument('--no-cuda', action='store_true', default=False,\n","                    help='enables CUDA training')\n","parser.add_argument('--seed', type=int, default=1, metavar='S',\n","                    help='random seed (default: 1)')\n","parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n","                    help='how many batches to wait before logging training status')\n","parser.add_argument('--hard', action='store_true', default=False,\n","                    help='hard Gumbel softmax')\n","\n","sys.argv=['']\n","del sys\n","\n","args = parser.parse_args()\n","args.cuda = not args.no_cuda and torch.cuda.is_available()\n","torch.manual_seed(args.seed)\n","if args.cuda:\n","    torch.cuda.manual_seed(args.seed)\n","\n","device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n","\n","kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}"],"metadata":{"id":"QQRNsG2NnWsf","executionInfo":{"status":"ok","timestamp":1655469135454,"user_tz":-120,"elapsed":5,"user":{"displayName":"Rajmohan Tumarada","userId":"18412595153668734700"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["train_loader = torch.utils.data.DataLoader(\n","    datasets.MNIST('./data/MNIST', train=True, download=True,\n","                   transform=transforms.ToTensor()),\n","    batch_size=args.batch_size, shuffle=True, **kwargs)\n","test_loader = torch.utils.data.DataLoader(\n","    datasets.MNIST('./data/MNIST', train=False, transform=transforms.ToTensor()),\n","    batch_size=args.batch_size, shuffle=True, **kwargs)"],"metadata":{"id":"4mWbpCcxnWvc","executionInfo":{"status":"ok","timestamp":1655469137128,"user_tz":-120,"elapsed":277,"user":{"displayName":"Rajmohan Tumarada","userId":"18412595153668734700"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["class VAE(nn.Module):\n","    def __init__(self):\n","        super(VAE, self).__init__()\n","\n","        self.fc1 = nn.Linear(784, 400)\n","        self.fc21 = nn.Linear(400, 20)\n","        self.fc22 = nn.Linear(400, 20)\n","        self.fc3 = nn.Linear(20, 400)\n","        self.fc4 = nn.Linear(400, 784)\n","\n","    def encode(self, x):\n","        h1 = F.relu(self.fc1(x))\n","        return self.fc21(h1), self.fc22(h1)\n","\n","    def reparameterize(self, mu, logvar):\n","        if self.training:\n","            std = torch.exp(0.5*logvar)\n","            eps = torch.randn_like(std)\n","            return eps.mul(std).add_(mu)\n","        else:\n","            return mu\n","\n","    def decode(self, z):\n","        h3 = F.relu(self.fc3(z))\n","        return F.sigmoid(self.fc4(h3))\n","\n","    def forward(self, x):\n","        mu, logvar = self.encode(x.view(-1, 784))\n","        z = self.reparameterize(mu, logvar)\n","        return self.decode(z), mu, logvar\n"],"metadata":{"id":"yWlnXRObnWyW","executionInfo":{"status":"ok","timestamp":1655469138719,"user_tz":-120,"elapsed":3,"user":{"displayName":"Rajmohan Tumarada","userId":"18412595153668734700"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["model = VAE().to(device)\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)"],"metadata":{"id":"QjNaQlnanW1E","executionInfo":{"status":"ok","timestamp":1655469140038,"user_tz":-120,"elapsed":259,"user":{"displayName":"Rajmohan Tumarada","userId":"18412595153668734700"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# Reconstruction + KL divergence losses summed over all elements and batch\n","def loss_function(recon_x, x, mu, logvar):\n","    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), size_average=False)\n","\n","    # see Appendix B from VAE paper:\n","    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n","    # https://arxiv.org/abs/1312.6114\n","    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n","    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","\n","    return BCE + KLD"],"metadata":{"id":"wBglm2eQnW32","executionInfo":{"status":"ok","timestamp":1655469141644,"user_tz":-120,"elapsed":410,"user":{"displayName":"Rajmohan Tumarada","userId":"18412595153668734700"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["def train(epoch):\n","    model.train()\n","    train_loss = 0\n","    for batch_idx, (data, _) in enumerate(train_loader):\n","        data = data.to(device)\n","        optimizer.zero_grad()\n","        recon_batch, mu, logvar = model(data)\n","        loss = loss_function(recon_batch, data, mu, logvar)\n","        loss.backward()\n","        train_loss += loss.item()\n","        optimizer.step()\n","        if batch_idx % args.log_interval == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader),\n","                loss.item() / len(data)))\n","\n","    print('====> Epoch: {} Average loss: {:.4f}'.format(\n","          epoch, train_loss / len(train_loader.dataset)))"],"metadata":{"id":"exDrMK8unW6u","executionInfo":{"status":"ok","timestamp":1655469142260,"user_tz":-120,"elapsed":6,"user":{"displayName":"Rajmohan Tumarada","userId":"18412595153668734700"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["def test(epoch):\n","    model.eval()\n","    test_loss = 0\n","    with torch.no_grad():\n","        for i, (data, _) in enumerate(test_loader):\n","            data = data.to(device)\n","            recon_batch, mu, logvar = model(data)\n","            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n","            if i == 0:\n","                n = min(data.size(0), 8)\n","                comparison = torch.cat([data[:n],\n","                                      recon_batch.view(args.batch_size, 1, 28, 28)[:n]])\n","                save_image(comparison.cpu(),\n","                         'resultsDiscrete/reconstruction_' + str(epoch) + '.png', nrow=n)\n","\n","    test_loss /= len(test_loader.dataset)\n","    print('====> Test set loss: {:.4f}'.format(test_loss))"],"metadata":{"id":"4z3dQsQlnz_d","executionInfo":{"status":"ok","timestamp":1655469144412,"user_tz":-120,"elapsed":312,"user":{"displayName":"Rajmohan Tumarada","userId":"18412595153668734700"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["def run():\n","  for epoch in range(1, args.epochs + 1):\n","    train(epoch)\n","    test(epoch)\n","    with torch.no_grad():\n","        sample = torch.randn(64, 20).to(device)\n","        sample = model.decode(sample).cpu()\n","        save_image(sample.view(64, 1, 28, 28),\n","                   'resultsDiscrete/sample_' + str(epoch) + '.png')\n","        \n","if __name__ == '__main__':\n","    run()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5BTF7rtjnz73","executionInfo":{"status":"ok","timestamp":1655469398565,"user_tz":-120,"elapsed":253007,"user":{"displayName":"Rajmohan Tumarada","userId":"18412595153668734700"}},"outputId":"2fc59ac9-e883-4db5-e950-3b735c89f28d"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"output_type":"stream","name":"stdout","text":["Train Epoch: 1 [0/60000 (0%)]\tLoss: 550.596191\n","Train Epoch: 1 [1280/60000 (2%)]\tLoss: 306.821533\n","Train Epoch: 1 [2560/60000 (4%)]\tLoss: 239.735825\n","Train Epoch: 1 [3840/60000 (6%)]\tLoss: 219.265350\n","Train Epoch: 1 [5120/60000 (9%)]\tLoss: 215.173065\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 207.980484\n","Train Epoch: 1 [7680/60000 (13%)]\tLoss: 205.002533\n","Train Epoch: 1 [8960/60000 (15%)]\tLoss: 195.018494\n","Train Epoch: 1 [10240/60000 (17%)]\tLoss: 195.951401\n","Train Epoch: 1 [11520/60000 (19%)]\tLoss: 191.046783\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 177.746719\n","Train Epoch: 1 [14080/60000 (23%)]\tLoss: 173.476227\n","Train Epoch: 1 [15360/60000 (26%)]\tLoss: 182.252777\n","Train Epoch: 1 [16640/60000 (28%)]\tLoss: 168.943054\n","Train Epoch: 1 [17920/60000 (30%)]\tLoss: 167.080704\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 163.089142\n","Train Epoch: 1 [20480/60000 (34%)]\tLoss: 161.482941\n","Train Epoch: 1 [21760/60000 (36%)]\tLoss: 152.580917\n","Train Epoch: 1 [23040/60000 (38%)]\tLoss: 156.822372\n","Train Epoch: 1 [24320/60000 (41%)]\tLoss: 154.286819\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 154.853256\n","Train Epoch: 1 [26880/60000 (45%)]\tLoss: 150.406204\n","Train Epoch: 1 [28160/60000 (47%)]\tLoss: 154.873779\n","Train Epoch: 1 [29440/60000 (49%)]\tLoss: 147.339722\n","Train Epoch: 1 [30720/60000 (51%)]\tLoss: 141.410065\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 144.210602\n","Train Epoch: 1 [33280/60000 (55%)]\tLoss: 145.480576\n","Train Epoch: 1 [34560/60000 (58%)]\tLoss: 144.594559\n","Train Epoch: 1 [35840/60000 (60%)]\tLoss: 136.373703\n","Train Epoch: 1 [37120/60000 (62%)]\tLoss: 142.608673\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 139.441666\n","Train Epoch: 1 [39680/60000 (66%)]\tLoss: 134.518555\n","Train Epoch: 1 [40960/60000 (68%)]\tLoss: 143.254974\n","Train Epoch: 1 [42240/60000 (70%)]\tLoss: 140.900070\n","Train Epoch: 1 [43520/60000 (72%)]\tLoss: 141.618835\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 136.003036\n","Train Epoch: 1 [46080/60000 (77%)]\tLoss: 141.084061\n","Train Epoch: 1 [47360/60000 (79%)]\tLoss: 132.052048\n","Train Epoch: 1 [48640/60000 (81%)]\tLoss: 132.223358\n","Train Epoch: 1 [49920/60000 (83%)]\tLoss: 131.405853\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 132.236023\n","Train Epoch: 1 [52480/60000 (87%)]\tLoss: 132.012039\n","Train Epoch: 1 [53760/60000 (90%)]\tLoss: 132.734360\n","Train Epoch: 1 [55040/60000 (92%)]\tLoss: 127.142746\n","Train Epoch: 1 [56320/60000 (94%)]\tLoss: 137.609924\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 130.745758\n","Train Epoch: 1 [58880/60000 (98%)]\tLoss: 133.279709\n","====> Epoch: 1 Average loss: 164.8239\n","====> Test set loss: 121.0333\n","Train Epoch: 2 [0/60000 (0%)]\tLoss: 132.235107\n","Train Epoch: 2 [1280/60000 (2%)]\tLoss: 123.809135\n","Train Epoch: 2 [2560/60000 (4%)]\tLoss: 126.540031\n","Train Epoch: 2 [3840/60000 (6%)]\tLoss: 132.616730\n","Train Epoch: 2 [5120/60000 (9%)]\tLoss: 125.523331\n","Train Epoch: 2 [6400/60000 (11%)]\tLoss: 131.833130\n","Train Epoch: 2 [7680/60000 (13%)]\tLoss: 127.029381\n","Train Epoch: 2 [8960/60000 (15%)]\tLoss: 123.760078\n","Train Epoch: 2 [10240/60000 (17%)]\tLoss: 126.430519\n","Train Epoch: 2 [11520/60000 (19%)]\tLoss: 124.714012\n","Train Epoch: 2 [12800/60000 (21%)]\tLoss: 124.252930\n","Train Epoch: 2 [14080/60000 (23%)]\tLoss: 121.596771\n","Train Epoch: 2 [15360/60000 (26%)]\tLoss: 124.802002\n","Train Epoch: 2 [16640/60000 (28%)]\tLoss: 123.419250\n","Train Epoch: 2 [17920/60000 (30%)]\tLoss: 123.342949\n","Train Epoch: 2 [19200/60000 (32%)]\tLoss: 121.501923\n","Train Epoch: 2 [20480/60000 (34%)]\tLoss: 125.086739\n","Train Epoch: 2 [21760/60000 (36%)]\tLoss: 119.548500\n","Train Epoch: 2 [23040/60000 (38%)]\tLoss: 122.242035\n","Train Epoch: 2 [24320/60000 (41%)]\tLoss: 123.229591\n","Train Epoch: 2 [25600/60000 (43%)]\tLoss: 115.644722\n","Train Epoch: 2 [26880/60000 (45%)]\tLoss: 123.607475\n","Train Epoch: 2 [28160/60000 (47%)]\tLoss: 122.774590\n","Train Epoch: 2 [29440/60000 (49%)]\tLoss: 122.927475\n","Train Epoch: 2 [30720/60000 (51%)]\tLoss: 123.700630\n","Train Epoch: 2 [32000/60000 (53%)]\tLoss: 121.092026\n","Train Epoch: 2 [33280/60000 (55%)]\tLoss: 119.732132\n","Train Epoch: 2 [34560/60000 (58%)]\tLoss: 127.287674\n","Train Epoch: 2 [35840/60000 (60%)]\tLoss: 119.827423\n","Train Epoch: 2 [37120/60000 (62%)]\tLoss: 119.839706\n","Train Epoch: 2 [38400/60000 (64%)]\tLoss: 125.457268\n","Train Epoch: 2 [39680/60000 (66%)]\tLoss: 121.117676\n","Train Epoch: 2 [40960/60000 (68%)]\tLoss: 120.542160\n","Train Epoch: 2 [42240/60000 (70%)]\tLoss: 113.367500\n","Train Epoch: 2 [43520/60000 (72%)]\tLoss: 115.808914\n","Train Epoch: 2 [44800/60000 (75%)]\tLoss: 121.545677\n","Train Epoch: 2 [46080/60000 (77%)]\tLoss: 120.851852\n","Train Epoch: 2 [47360/60000 (79%)]\tLoss: 117.438545\n","Train Epoch: 2 [48640/60000 (81%)]\tLoss: 117.657166\n","Train Epoch: 2 [49920/60000 (83%)]\tLoss: 119.465912\n","Train Epoch: 2 [51200/60000 (85%)]\tLoss: 120.300102\n","Train Epoch: 2 [52480/60000 (87%)]\tLoss: 117.949677\n","Train Epoch: 2 [53760/60000 (90%)]\tLoss: 115.771103\n","Train Epoch: 2 [55040/60000 (92%)]\tLoss: 113.943375\n","Train Epoch: 2 [56320/60000 (94%)]\tLoss: 120.184593\n","Train Epoch: 2 [57600/60000 (96%)]\tLoss: 117.846291\n","Train Epoch: 2 [58880/60000 (98%)]\tLoss: 121.289490\n","====> Epoch: 2 Average loss: 122.2905\n","====> Test set loss: 108.1117\n","Train Epoch: 3 [0/60000 (0%)]\tLoss: 119.423836\n","Train Epoch: 3 [1280/60000 (2%)]\tLoss: 119.670021\n","Train Epoch: 3 [2560/60000 (4%)]\tLoss: 116.956093\n","Train Epoch: 3 [3840/60000 (6%)]\tLoss: 121.160210\n","Train Epoch: 3 [5120/60000 (9%)]\tLoss: 116.219444\n","Train Epoch: 3 [6400/60000 (11%)]\tLoss: 116.003220\n","Train Epoch: 3 [7680/60000 (13%)]\tLoss: 118.446625\n","Train Epoch: 3 [8960/60000 (15%)]\tLoss: 117.835869\n","Train Epoch: 3 [10240/60000 (17%)]\tLoss: 115.019386\n","Train Epoch: 3 [11520/60000 (19%)]\tLoss: 116.151367\n","Train Epoch: 3 [12800/60000 (21%)]\tLoss: 115.231178\n","Train Epoch: 3 [14080/60000 (23%)]\tLoss: 119.878700\n","Train Epoch: 3 [15360/60000 (26%)]\tLoss: 116.031868\n","Train Epoch: 3 [16640/60000 (28%)]\tLoss: 116.961868\n","Train Epoch: 3 [17920/60000 (30%)]\tLoss: 113.665184\n","Train Epoch: 3 [19200/60000 (32%)]\tLoss: 111.035751\n","Train Epoch: 3 [20480/60000 (34%)]\tLoss: 116.332565\n","Train Epoch: 3 [21760/60000 (36%)]\tLoss: 117.910057\n","Train Epoch: 3 [23040/60000 (38%)]\tLoss: 120.088242\n","Train Epoch: 3 [24320/60000 (41%)]\tLoss: 113.850433\n","Train Epoch: 3 [25600/60000 (43%)]\tLoss: 117.927673\n","Train Epoch: 3 [26880/60000 (45%)]\tLoss: 120.002930\n","Train Epoch: 3 [28160/60000 (47%)]\tLoss: 114.873146\n","Train Epoch: 3 [29440/60000 (49%)]\tLoss: 115.551949\n","Train Epoch: 3 [30720/60000 (51%)]\tLoss: 108.945282\n","Train Epoch: 3 [32000/60000 (53%)]\tLoss: 114.455284\n","Train Epoch: 3 [33280/60000 (55%)]\tLoss: 116.822792\n","Train Epoch: 3 [34560/60000 (58%)]\tLoss: 110.921333\n","Train Epoch: 3 [35840/60000 (60%)]\tLoss: 114.697113\n","Train Epoch: 3 [37120/60000 (62%)]\tLoss: 117.640045\n","Train Epoch: 3 [38400/60000 (64%)]\tLoss: 111.151321\n","Train Epoch: 3 [39680/60000 (66%)]\tLoss: 112.055412\n","Train Epoch: 3 [40960/60000 (68%)]\tLoss: 116.121567\n","Train Epoch: 3 [42240/60000 (70%)]\tLoss: 112.564240\n","Train Epoch: 3 [43520/60000 (72%)]\tLoss: 113.065659\n","Train Epoch: 3 [44800/60000 (75%)]\tLoss: 111.023636\n","Train Epoch: 3 [46080/60000 (77%)]\tLoss: 114.039581\n","Train Epoch: 3 [47360/60000 (79%)]\tLoss: 115.636841\n","Train Epoch: 3 [48640/60000 (81%)]\tLoss: 113.255173\n","Train Epoch: 3 [49920/60000 (83%)]\tLoss: 116.215050\n","Train Epoch: 3 [51200/60000 (85%)]\tLoss: 109.353523\n","Train Epoch: 3 [52480/60000 (87%)]\tLoss: 113.662033\n","Train Epoch: 3 [53760/60000 (90%)]\tLoss: 114.622711\n","Train Epoch: 3 [55040/60000 (92%)]\tLoss: 112.446838\n","Train Epoch: 3 [56320/60000 (94%)]\tLoss: 112.093033\n","Train Epoch: 3 [57600/60000 (96%)]\tLoss: 120.523834\n","Train Epoch: 3 [58880/60000 (98%)]\tLoss: 115.336021\n","====> Epoch: 3 Average loss: 114.9079\n","====> Test set loss: 103.1575\n","Train Epoch: 4 [0/60000 (0%)]\tLoss: 112.818901\n","Train Epoch: 4 [1280/60000 (2%)]\tLoss: 110.194328\n","Train Epoch: 4 [2560/60000 (4%)]\tLoss: 116.947281\n","Train Epoch: 4 [3840/60000 (6%)]\tLoss: 111.368858\n","Train Epoch: 4 [5120/60000 (9%)]\tLoss: 114.198471\n","Train Epoch: 4 [6400/60000 (11%)]\tLoss: 115.932373\n","Train Epoch: 4 [7680/60000 (13%)]\tLoss: 115.267334\n","Train Epoch: 4 [8960/60000 (15%)]\tLoss: 105.952599\n","Train Epoch: 4 [10240/60000 (17%)]\tLoss: 111.955742\n","Train Epoch: 4 [11520/60000 (19%)]\tLoss: 113.053581\n","Train Epoch: 4 [12800/60000 (21%)]\tLoss: 116.467300\n","Train Epoch: 4 [14080/60000 (23%)]\tLoss: 109.842880\n","Train Epoch: 4 [15360/60000 (26%)]\tLoss: 109.308464\n","Train Epoch: 4 [16640/60000 (28%)]\tLoss: 108.185661\n","Train Epoch: 4 [17920/60000 (30%)]\tLoss: 108.348473\n","Train Epoch: 4 [19200/60000 (32%)]\tLoss: 112.379913\n","Train Epoch: 4 [20480/60000 (34%)]\tLoss: 115.133011\n","Train Epoch: 4 [21760/60000 (36%)]\tLoss: 109.975189\n","Train Epoch: 4 [23040/60000 (38%)]\tLoss: 111.491547\n","Train Epoch: 4 [24320/60000 (41%)]\tLoss: 113.505356\n","Train Epoch: 4 [25600/60000 (43%)]\tLoss: 115.292786\n","Train Epoch: 4 [26880/60000 (45%)]\tLoss: 113.701767\n","Train Epoch: 4 [28160/60000 (47%)]\tLoss: 111.109894\n","Train Epoch: 4 [29440/60000 (49%)]\tLoss: 112.646729\n","Train Epoch: 4 [30720/60000 (51%)]\tLoss: 112.374542\n","Train Epoch: 4 [32000/60000 (53%)]\tLoss: 107.419014\n","Train Epoch: 4 [33280/60000 (55%)]\tLoss: 110.328392\n","Train Epoch: 4 [34560/60000 (58%)]\tLoss: 113.445465\n","Train Epoch: 4 [35840/60000 (60%)]\tLoss: 113.016434\n","Train Epoch: 4 [37120/60000 (62%)]\tLoss: 111.204872\n","Train Epoch: 4 [38400/60000 (64%)]\tLoss: 115.133575\n","Train Epoch: 4 [39680/60000 (66%)]\tLoss: 110.583862\n","Train Epoch: 4 [40960/60000 (68%)]\tLoss: 113.248055\n","Train Epoch: 4 [42240/60000 (70%)]\tLoss: 113.851379\n","Train Epoch: 4 [43520/60000 (72%)]\tLoss: 111.860519\n","Train Epoch: 4 [44800/60000 (75%)]\tLoss: 108.971451\n","Train Epoch: 4 [46080/60000 (77%)]\tLoss: 114.594223\n","Train Epoch: 4 [47360/60000 (79%)]\tLoss: 110.657974\n","Train Epoch: 4 [48640/60000 (81%)]\tLoss: 112.250458\n","Train Epoch: 4 [49920/60000 (83%)]\tLoss: 110.085594\n","Train Epoch: 4 [51200/60000 (85%)]\tLoss: 105.968231\n","Train Epoch: 4 [52480/60000 (87%)]\tLoss: 106.123390\n","Train Epoch: 4 [53760/60000 (90%)]\tLoss: 110.453247\n","Train Epoch: 4 [55040/60000 (92%)]\tLoss: 109.295303\n","Train Epoch: 4 [56320/60000 (94%)]\tLoss: 110.954056\n","Train Epoch: 4 [57600/60000 (96%)]\tLoss: 105.760414\n","Train Epoch: 4 [58880/60000 (98%)]\tLoss: 107.052544\n","====> Epoch: 4 Average loss: 111.7565\n","====> Test set loss: 101.9629\n","Train Epoch: 5 [0/60000 (0%)]\tLoss: 112.384010\n","Train Epoch: 5 [1280/60000 (2%)]\tLoss: 108.335930\n","Train Epoch: 5 [2560/60000 (4%)]\tLoss: 105.761353\n","Train Epoch: 5 [3840/60000 (6%)]\tLoss: 110.105385\n","Train Epoch: 5 [5120/60000 (9%)]\tLoss: 114.456741\n","Train Epoch: 5 [6400/60000 (11%)]\tLoss: 104.825996\n","Train Epoch: 5 [7680/60000 (13%)]\tLoss: 109.121414\n","Train Epoch: 5 [8960/60000 (15%)]\tLoss: 104.944405\n","Train Epoch: 5 [10240/60000 (17%)]\tLoss: 113.467133\n","Train Epoch: 5 [11520/60000 (19%)]\tLoss: 109.538826\n","Train Epoch: 5 [12800/60000 (21%)]\tLoss: 104.589821\n","Train Epoch: 5 [14080/60000 (23%)]\tLoss: 106.155342\n","Train Epoch: 5 [15360/60000 (26%)]\tLoss: 104.681465\n","Train Epoch: 5 [16640/60000 (28%)]\tLoss: 113.620430\n","Train Epoch: 5 [17920/60000 (30%)]\tLoss: 104.874664\n","Train Epoch: 5 [19200/60000 (32%)]\tLoss: 113.800102\n","Train Epoch: 5 [20480/60000 (34%)]\tLoss: 112.480164\n","Train Epoch: 5 [21760/60000 (36%)]\tLoss: 108.614761\n","Train Epoch: 5 [23040/60000 (38%)]\tLoss: 109.713516\n","Train Epoch: 5 [24320/60000 (41%)]\tLoss: 111.028343\n","Train Epoch: 5 [25600/60000 (43%)]\tLoss: 107.885925\n","Train Epoch: 5 [26880/60000 (45%)]\tLoss: 108.343918\n","Train Epoch: 5 [28160/60000 (47%)]\tLoss: 110.772690\n","Train Epoch: 5 [29440/60000 (49%)]\tLoss: 108.540062\n","Train Epoch: 5 [30720/60000 (51%)]\tLoss: 108.443878\n","Train Epoch: 5 [32000/60000 (53%)]\tLoss: 109.733826\n","Train Epoch: 5 [33280/60000 (55%)]\tLoss: 110.445480\n","Train Epoch: 5 [34560/60000 (58%)]\tLoss: 111.135971\n","Train Epoch: 5 [35840/60000 (60%)]\tLoss: 109.819351\n","Train Epoch: 5 [37120/60000 (62%)]\tLoss: 109.628357\n","Train Epoch: 5 [38400/60000 (64%)]\tLoss: 110.842224\n","Train Epoch: 5 [39680/60000 (66%)]\tLoss: 109.108917\n","Train Epoch: 5 [40960/60000 (68%)]\tLoss: 105.570641\n","Train Epoch: 5 [42240/60000 (70%)]\tLoss: 111.578445\n","Train Epoch: 5 [43520/60000 (72%)]\tLoss: 110.440895\n","Train Epoch: 5 [44800/60000 (75%)]\tLoss: 109.339081\n","Train Epoch: 5 [46080/60000 (77%)]\tLoss: 108.539001\n","Train Epoch: 5 [47360/60000 (79%)]\tLoss: 110.272369\n","Train Epoch: 5 [48640/60000 (81%)]\tLoss: 107.544167\n","Train Epoch: 5 [49920/60000 (83%)]\tLoss: 112.545776\n","Train Epoch: 5 [51200/60000 (85%)]\tLoss: 112.571907\n","Train Epoch: 5 [52480/60000 (87%)]\tLoss: 109.761368\n","Train Epoch: 5 [53760/60000 (90%)]\tLoss: 110.206627\n","Train Epoch: 5 [55040/60000 (92%)]\tLoss: 105.712418\n","Train Epoch: 5 [56320/60000 (94%)]\tLoss: 106.664391\n","Train Epoch: 5 [57600/60000 (96%)]\tLoss: 111.223694\n","Train Epoch: 5 [58880/60000 (98%)]\tLoss: 110.576134\n","====> Epoch: 5 Average loss: 109.9075\n","====> Test set loss: 100.0819\n","Train Epoch: 6 [0/60000 (0%)]\tLoss: 111.506958\n","Train Epoch: 6 [1280/60000 (2%)]\tLoss: 110.391640\n","Train Epoch: 6 [2560/60000 (4%)]\tLoss: 105.453850\n","Train Epoch: 6 [3840/60000 (6%)]\tLoss: 109.716400\n","Train Epoch: 6 [5120/60000 (9%)]\tLoss: 109.038757\n","Train Epoch: 6 [6400/60000 (11%)]\tLoss: 113.072174\n","Train Epoch: 6 [7680/60000 (13%)]\tLoss: 109.407440\n","Train Epoch: 6 [8960/60000 (15%)]\tLoss: 110.278152\n","Train Epoch: 6 [10240/60000 (17%)]\tLoss: 114.716492\n","Train Epoch: 6 [11520/60000 (19%)]\tLoss: 109.523743\n","Train Epoch: 6 [12800/60000 (21%)]\tLoss: 105.266022\n","Train Epoch: 6 [14080/60000 (23%)]\tLoss: 104.512894\n","Train Epoch: 6 [15360/60000 (26%)]\tLoss: 106.912560\n","Train Epoch: 6 [16640/60000 (28%)]\tLoss: 107.314026\n","Train Epoch: 6 [17920/60000 (30%)]\tLoss: 107.890015\n","Train Epoch: 6 [19200/60000 (32%)]\tLoss: 112.277802\n","Train Epoch: 6 [20480/60000 (34%)]\tLoss: 111.949135\n","Train Epoch: 6 [21760/60000 (36%)]\tLoss: 108.539490\n","Train Epoch: 6 [23040/60000 (38%)]\tLoss: 113.906769\n","Train Epoch: 6 [24320/60000 (41%)]\tLoss: 109.054955\n","Train Epoch: 6 [25600/60000 (43%)]\tLoss: 108.509758\n","Train Epoch: 6 [26880/60000 (45%)]\tLoss: 106.994751\n","Train Epoch: 6 [28160/60000 (47%)]\tLoss: 106.237427\n","Train Epoch: 6 [29440/60000 (49%)]\tLoss: 107.067154\n","Train Epoch: 6 [30720/60000 (51%)]\tLoss: 107.883789\n","Train Epoch: 6 [32000/60000 (53%)]\tLoss: 106.811325\n","Train Epoch: 6 [33280/60000 (55%)]\tLoss: 110.822426\n","Train Epoch: 6 [34560/60000 (58%)]\tLoss: 111.362564\n","Train Epoch: 6 [35840/60000 (60%)]\tLoss: 103.774612\n","Train Epoch: 6 [37120/60000 (62%)]\tLoss: 105.594162\n","Train Epoch: 6 [38400/60000 (64%)]\tLoss: 111.584969\n","Train Epoch: 6 [39680/60000 (66%)]\tLoss: 108.936508\n","Train Epoch: 6 [40960/60000 (68%)]\tLoss: 111.961128\n","Train Epoch: 6 [42240/60000 (70%)]\tLoss: 111.874870\n","Train Epoch: 6 [43520/60000 (72%)]\tLoss: 109.613609\n","Train Epoch: 6 [44800/60000 (75%)]\tLoss: 106.371780\n","Train Epoch: 6 [46080/60000 (77%)]\tLoss: 109.954407\n","Train Epoch: 6 [47360/60000 (79%)]\tLoss: 103.561241\n","Train Epoch: 6 [48640/60000 (81%)]\tLoss: 111.074394\n","Train Epoch: 6 [49920/60000 (83%)]\tLoss: 106.938522\n","Train Epoch: 6 [51200/60000 (85%)]\tLoss: 109.765038\n","Train Epoch: 6 [52480/60000 (87%)]\tLoss: 106.528740\n","Train Epoch: 6 [53760/60000 (90%)]\tLoss: 107.612587\n","Train Epoch: 6 [55040/60000 (92%)]\tLoss: 112.293961\n","Train Epoch: 6 [56320/60000 (94%)]\tLoss: 105.167664\n","Train Epoch: 6 [57600/60000 (96%)]\tLoss: 109.842377\n","Train Epoch: 6 [58880/60000 (98%)]\tLoss: 102.846504\n","====> Epoch: 6 Average loss: 108.6767\n","====> Test set loss: 98.9434\n","Train Epoch: 7 [0/60000 (0%)]\tLoss: 109.419266\n","Train Epoch: 7 [1280/60000 (2%)]\tLoss: 110.923248\n","Train Epoch: 7 [2560/60000 (4%)]\tLoss: 109.460236\n","Train Epoch: 7 [3840/60000 (6%)]\tLoss: 110.701294\n","Train Epoch: 7 [5120/60000 (9%)]\tLoss: 109.803986\n","Train Epoch: 7 [6400/60000 (11%)]\tLoss: 109.665024\n","Train Epoch: 7 [7680/60000 (13%)]\tLoss: 109.165665\n","Train Epoch: 7 [8960/60000 (15%)]\tLoss: 108.538780\n","Train Epoch: 7 [10240/60000 (17%)]\tLoss: 109.042786\n","Train Epoch: 7 [11520/60000 (19%)]\tLoss: 106.202530\n","Train Epoch: 7 [12800/60000 (21%)]\tLoss: 105.215515\n","Train Epoch: 7 [14080/60000 (23%)]\tLoss: 108.171921\n","Train Epoch: 7 [15360/60000 (26%)]\tLoss: 106.689056\n","Train Epoch: 7 [16640/60000 (28%)]\tLoss: 108.372787\n","Train Epoch: 7 [17920/60000 (30%)]\tLoss: 106.730736\n","Train Epoch: 7 [19200/60000 (32%)]\tLoss: 112.584824\n","Train Epoch: 7 [20480/60000 (34%)]\tLoss: 107.382530\n","Train Epoch: 7 [21760/60000 (36%)]\tLoss: 105.159157\n","Train Epoch: 7 [23040/60000 (38%)]\tLoss: 107.412743\n","Train Epoch: 7 [24320/60000 (41%)]\tLoss: 106.617737\n","Train Epoch: 7 [25600/60000 (43%)]\tLoss: 108.186447\n","Train Epoch: 7 [26880/60000 (45%)]\tLoss: 111.344643\n","Train Epoch: 7 [28160/60000 (47%)]\tLoss: 110.079811\n","Train Epoch: 7 [29440/60000 (49%)]\tLoss: 105.517715\n","Train Epoch: 7 [30720/60000 (51%)]\tLoss: 105.758179\n","Train Epoch: 7 [32000/60000 (53%)]\tLoss: 109.440750\n","Train Epoch: 7 [33280/60000 (55%)]\tLoss: 108.556168\n","Train Epoch: 7 [34560/60000 (58%)]\tLoss: 108.830566\n","Train Epoch: 7 [35840/60000 (60%)]\tLoss: 112.064850\n","Train Epoch: 7 [37120/60000 (62%)]\tLoss: 106.344177\n","Train Epoch: 7 [38400/60000 (64%)]\tLoss: 107.895821\n","Train Epoch: 7 [39680/60000 (66%)]\tLoss: 108.317116\n","Train Epoch: 7 [40960/60000 (68%)]\tLoss: 110.775627\n","Train Epoch: 7 [42240/60000 (70%)]\tLoss: 107.763596\n","Train Epoch: 7 [43520/60000 (72%)]\tLoss: 106.700775\n","Train Epoch: 7 [44800/60000 (75%)]\tLoss: 104.873802\n","Train Epoch: 7 [46080/60000 (77%)]\tLoss: 107.511795\n","Train Epoch: 7 [47360/60000 (79%)]\tLoss: 107.758270\n","Train Epoch: 7 [48640/60000 (81%)]\tLoss: 110.148407\n","Train Epoch: 7 [49920/60000 (83%)]\tLoss: 107.584442\n","Train Epoch: 7 [51200/60000 (85%)]\tLoss: 106.933258\n","Train Epoch: 7 [52480/60000 (87%)]\tLoss: 106.439278\n","Train Epoch: 7 [53760/60000 (90%)]\tLoss: 107.024681\n","Train Epoch: 7 [55040/60000 (92%)]\tLoss: 106.438164\n","Train Epoch: 7 [56320/60000 (94%)]\tLoss: 109.248726\n","Train Epoch: 7 [57600/60000 (96%)]\tLoss: 106.469147\n","Train Epoch: 7 [58880/60000 (98%)]\tLoss: 109.534134\n","====> Epoch: 7 Average loss: 107.8365\n","====> Test set loss: 98.3975\n","Train Epoch: 8 [0/60000 (0%)]\tLoss: 113.756348\n","Train Epoch: 8 [1280/60000 (2%)]\tLoss: 110.309822\n","Train Epoch: 8 [2560/60000 (4%)]\tLoss: 103.500504\n","Train Epoch: 8 [3840/60000 (6%)]\tLoss: 103.604279\n","Train Epoch: 8 [5120/60000 (9%)]\tLoss: 107.562302\n","Train Epoch: 8 [6400/60000 (11%)]\tLoss: 104.113968\n","Train Epoch: 8 [7680/60000 (13%)]\tLoss: 109.034012\n","Train Epoch: 8 [8960/60000 (15%)]\tLoss: 106.753845\n","Train Epoch: 8 [10240/60000 (17%)]\tLoss: 104.421677\n","Train Epoch: 8 [11520/60000 (19%)]\tLoss: 108.347214\n","Train Epoch: 8 [12800/60000 (21%)]\tLoss: 109.705101\n","Train Epoch: 8 [14080/60000 (23%)]\tLoss: 108.932274\n","Train Epoch: 8 [15360/60000 (26%)]\tLoss: 110.495560\n","Train Epoch: 8 [16640/60000 (28%)]\tLoss: 106.824989\n","Train Epoch: 8 [17920/60000 (30%)]\tLoss: 108.969711\n","Train Epoch: 8 [19200/60000 (32%)]\tLoss: 111.342850\n","Train Epoch: 8 [20480/60000 (34%)]\tLoss: 105.504265\n","Train Epoch: 8 [21760/60000 (36%)]\tLoss: 104.746696\n","Train Epoch: 8 [23040/60000 (38%)]\tLoss: 107.400963\n","Train Epoch: 8 [24320/60000 (41%)]\tLoss: 110.971252\n","Train Epoch: 8 [25600/60000 (43%)]\tLoss: 106.556572\n","Train Epoch: 8 [26880/60000 (45%)]\tLoss: 107.363029\n","Train Epoch: 8 [28160/60000 (47%)]\tLoss: 109.901253\n","Train Epoch: 8 [29440/60000 (49%)]\tLoss: 107.447342\n","Train Epoch: 8 [30720/60000 (51%)]\tLoss: 109.478630\n","Train Epoch: 8 [32000/60000 (53%)]\tLoss: 107.128998\n","Train Epoch: 8 [33280/60000 (55%)]\tLoss: 105.109116\n","Train Epoch: 8 [34560/60000 (58%)]\tLoss: 105.439240\n","Train Epoch: 8 [35840/60000 (60%)]\tLoss: 107.414948\n","Train Epoch: 8 [37120/60000 (62%)]\tLoss: 108.837837\n","Train Epoch: 8 [38400/60000 (64%)]\tLoss: 102.642685\n","Train Epoch: 8 [39680/60000 (66%)]\tLoss: 107.994202\n","Train Epoch: 8 [40960/60000 (68%)]\tLoss: 102.037430\n","Train Epoch: 8 [42240/60000 (70%)]\tLoss: 106.724098\n","Train Epoch: 8 [43520/60000 (72%)]\tLoss: 104.958206\n","Train Epoch: 8 [44800/60000 (75%)]\tLoss: 109.671417\n","Train Epoch: 8 [46080/60000 (77%)]\tLoss: 108.911438\n","Train Epoch: 8 [47360/60000 (79%)]\tLoss: 108.274643\n","Train Epoch: 8 [48640/60000 (81%)]\tLoss: 109.708908\n","Train Epoch: 8 [49920/60000 (83%)]\tLoss: 106.330864\n","Train Epoch: 8 [51200/60000 (85%)]\tLoss: 106.624580\n","Train Epoch: 8 [52480/60000 (87%)]\tLoss: 111.057686\n","Train Epoch: 8 [53760/60000 (90%)]\tLoss: 108.110588\n","Train Epoch: 8 [55040/60000 (92%)]\tLoss: 107.720024\n","Train Epoch: 8 [56320/60000 (94%)]\tLoss: 104.521080\n","Train Epoch: 8 [57600/60000 (96%)]\tLoss: 104.828201\n","Train Epoch: 8 [58880/60000 (98%)]\tLoss: 105.722321\n","====> Epoch: 8 Average loss: 107.1890\n","====> Test set loss: 97.6191\n","Train Epoch: 9 [0/60000 (0%)]\tLoss: 106.554207\n","Train Epoch: 9 [1280/60000 (2%)]\tLoss: 105.843369\n","Train Epoch: 9 [2560/60000 (4%)]\tLoss: 104.860840\n","Train Epoch: 9 [3840/60000 (6%)]\tLoss: 103.190590\n","Train Epoch: 9 [5120/60000 (9%)]\tLoss: 107.583832\n","Train Epoch: 9 [6400/60000 (11%)]\tLoss: 107.748917\n","Train Epoch: 9 [7680/60000 (13%)]\tLoss: 102.700577\n","Train Epoch: 9 [8960/60000 (15%)]\tLoss: 108.225723\n","Train Epoch: 9 [10240/60000 (17%)]\tLoss: 107.605804\n","Train Epoch: 9 [11520/60000 (19%)]\tLoss: 111.749939\n","Train Epoch: 9 [12800/60000 (21%)]\tLoss: 103.648788\n","Train Epoch: 9 [14080/60000 (23%)]\tLoss: 106.297134\n","Train Epoch: 9 [15360/60000 (26%)]\tLoss: 103.158768\n","Train Epoch: 9 [16640/60000 (28%)]\tLoss: 108.139427\n","Train Epoch: 9 [17920/60000 (30%)]\tLoss: 107.689438\n","Train Epoch: 9 [19200/60000 (32%)]\tLoss: 106.226547\n","Train Epoch: 9 [20480/60000 (34%)]\tLoss: 106.312714\n","Train Epoch: 9 [21760/60000 (36%)]\tLoss: 107.037041\n","Train Epoch: 9 [23040/60000 (38%)]\tLoss: 108.665512\n","Train Epoch: 9 [24320/60000 (41%)]\tLoss: 105.826874\n","Train Epoch: 9 [25600/60000 (43%)]\tLoss: 102.295448\n","Train Epoch: 9 [26880/60000 (45%)]\tLoss: 105.193108\n","Train Epoch: 9 [28160/60000 (47%)]\tLoss: 110.652702\n","Train Epoch: 9 [29440/60000 (49%)]\tLoss: 109.463440\n","Train Epoch: 9 [30720/60000 (51%)]\tLoss: 103.421524\n","Train Epoch: 9 [32000/60000 (53%)]\tLoss: 109.207863\n","Train Epoch: 9 [33280/60000 (55%)]\tLoss: 107.452301\n","Train Epoch: 9 [34560/60000 (58%)]\tLoss: 109.265793\n","Train Epoch: 9 [35840/60000 (60%)]\tLoss: 108.801704\n","Train Epoch: 9 [37120/60000 (62%)]\tLoss: 105.342995\n","Train Epoch: 9 [38400/60000 (64%)]\tLoss: 111.028122\n","Train Epoch: 9 [39680/60000 (66%)]\tLoss: 101.493393\n","Train Epoch: 9 [40960/60000 (68%)]\tLoss: 107.499565\n","Train Epoch: 9 [42240/60000 (70%)]\tLoss: 108.136047\n","Train Epoch: 9 [43520/60000 (72%)]\tLoss: 104.875526\n","Train Epoch: 9 [44800/60000 (75%)]\tLoss: 109.035400\n","Train Epoch: 9 [46080/60000 (77%)]\tLoss: 106.497612\n","Train Epoch: 9 [47360/60000 (79%)]\tLoss: 101.807091\n","Train Epoch: 9 [48640/60000 (81%)]\tLoss: 105.247749\n","Train Epoch: 9 [49920/60000 (83%)]\tLoss: 101.941895\n","Train Epoch: 9 [51200/60000 (85%)]\tLoss: 105.684990\n","Train Epoch: 9 [52480/60000 (87%)]\tLoss: 109.272995\n","Train Epoch: 9 [53760/60000 (90%)]\tLoss: 101.312927\n","Train Epoch: 9 [55040/60000 (92%)]\tLoss: 102.867500\n","Train Epoch: 9 [56320/60000 (94%)]\tLoss: 105.283676\n","Train Epoch: 9 [57600/60000 (96%)]\tLoss: 108.788605\n","Train Epoch: 9 [58880/60000 (98%)]\tLoss: 101.927742\n","====> Epoch: 9 Average loss: 106.6724\n","====> Test set loss: 96.9639\n","Train Epoch: 10 [0/60000 (0%)]\tLoss: 105.803696\n","Train Epoch: 10 [1280/60000 (2%)]\tLoss: 105.303650\n","Train Epoch: 10 [2560/60000 (4%)]\tLoss: 100.944649\n","Train Epoch: 10 [3840/60000 (6%)]\tLoss: 107.474915\n","Train Epoch: 10 [5120/60000 (9%)]\tLoss: 107.139389\n","Train Epoch: 10 [6400/60000 (11%)]\tLoss: 103.666374\n","Train Epoch: 10 [7680/60000 (13%)]\tLoss: 107.368492\n","Train Epoch: 10 [8960/60000 (15%)]\tLoss: 107.326431\n","Train Epoch: 10 [10240/60000 (17%)]\tLoss: 107.876984\n","Train Epoch: 10 [11520/60000 (19%)]\tLoss: 102.872543\n","Train Epoch: 10 [12800/60000 (21%)]\tLoss: 106.769669\n","Train Epoch: 10 [14080/60000 (23%)]\tLoss: 106.212234\n","Train Epoch: 10 [15360/60000 (26%)]\tLoss: 106.058144\n","Train Epoch: 10 [16640/60000 (28%)]\tLoss: 107.254929\n","Train Epoch: 10 [17920/60000 (30%)]\tLoss: 104.657135\n","Train Epoch: 10 [19200/60000 (32%)]\tLoss: 107.408936\n","Train Epoch: 10 [20480/60000 (34%)]\tLoss: 110.890038\n","Train Epoch: 10 [21760/60000 (36%)]\tLoss: 106.523384\n","Train Epoch: 10 [23040/60000 (38%)]\tLoss: 104.777504\n","Train Epoch: 10 [24320/60000 (41%)]\tLoss: 109.081863\n","Train Epoch: 10 [25600/60000 (43%)]\tLoss: 105.160873\n","Train Epoch: 10 [26880/60000 (45%)]\tLoss: 108.232903\n","Train Epoch: 10 [28160/60000 (47%)]\tLoss: 104.035965\n","Train Epoch: 10 [29440/60000 (49%)]\tLoss: 103.120941\n","Train Epoch: 10 [30720/60000 (51%)]\tLoss: 105.690247\n","Train Epoch: 10 [32000/60000 (53%)]\tLoss: 106.839172\n","Train Epoch: 10 [33280/60000 (55%)]\tLoss: 110.152893\n","Train Epoch: 10 [34560/60000 (58%)]\tLoss: 108.983276\n","Train Epoch: 10 [35840/60000 (60%)]\tLoss: 102.927902\n","Train Epoch: 10 [37120/60000 (62%)]\tLoss: 103.856888\n","Train Epoch: 10 [38400/60000 (64%)]\tLoss: 103.173401\n","Train Epoch: 10 [39680/60000 (66%)]\tLoss: 106.252823\n","Train Epoch: 10 [40960/60000 (68%)]\tLoss: 109.164879\n","Train Epoch: 10 [42240/60000 (70%)]\tLoss: 108.111946\n","Train Epoch: 10 [43520/60000 (72%)]\tLoss: 105.462646\n","Train Epoch: 10 [44800/60000 (75%)]\tLoss: 106.760811\n","Train Epoch: 10 [46080/60000 (77%)]\tLoss: 106.052475\n","Train Epoch: 10 [47360/60000 (79%)]\tLoss: 108.188660\n","Train Epoch: 10 [48640/60000 (81%)]\tLoss: 108.816551\n","Train Epoch: 10 [49920/60000 (83%)]\tLoss: 104.143509\n","Train Epoch: 10 [51200/60000 (85%)]\tLoss: 105.820526\n","Train Epoch: 10 [52480/60000 (87%)]\tLoss: 110.384094\n","Train Epoch: 10 [53760/60000 (90%)]\tLoss: 107.728577\n","Train Epoch: 10 [55040/60000 (92%)]\tLoss: 108.540764\n","Train Epoch: 10 [56320/60000 (94%)]\tLoss: 105.161621\n","Train Epoch: 10 [57600/60000 (96%)]\tLoss: 107.807220\n","Train Epoch: 10 [58880/60000 (98%)]\tLoss: 104.751595\n","====> Epoch: 10 Average loss: 106.2360\n","====> Test set loss: 96.4758\n","Train Epoch: 11 [0/60000 (0%)]\tLoss: 101.507492\n","Train Epoch: 11 [1280/60000 (2%)]\tLoss: 108.014153\n","Train Epoch: 11 [2560/60000 (4%)]\tLoss: 105.666153\n","Train Epoch: 11 [3840/60000 (6%)]\tLoss: 105.745987\n","Train Epoch: 11 [5120/60000 (9%)]\tLoss: 103.731529\n","Train Epoch: 11 [6400/60000 (11%)]\tLoss: 109.093224\n","Train Epoch: 11 [7680/60000 (13%)]\tLoss: 106.156372\n","Train Epoch: 11 [8960/60000 (15%)]\tLoss: 107.975075\n","Train Epoch: 11 [10240/60000 (17%)]\tLoss: 105.266937\n","Train Epoch: 11 [11520/60000 (19%)]\tLoss: 105.403107\n","Train Epoch: 11 [12800/60000 (21%)]\tLoss: 107.636780\n","Train Epoch: 11 [14080/60000 (23%)]\tLoss: 107.666870\n","Train Epoch: 11 [15360/60000 (26%)]\tLoss: 107.116013\n","Train Epoch: 11 [16640/60000 (28%)]\tLoss: 107.550346\n","Train Epoch: 11 [17920/60000 (30%)]\tLoss: 105.081139\n","Train Epoch: 11 [19200/60000 (32%)]\tLoss: 107.152420\n","Train Epoch: 11 [20480/60000 (34%)]\tLoss: 106.416489\n","Train Epoch: 11 [21760/60000 (36%)]\tLoss: 105.167549\n","Train Epoch: 11 [23040/60000 (38%)]\tLoss: 100.648918\n","Train Epoch: 11 [24320/60000 (41%)]\tLoss: 106.068245\n","Train Epoch: 11 [25600/60000 (43%)]\tLoss: 104.931625\n","Train Epoch: 11 [26880/60000 (45%)]\tLoss: 106.460007\n","Train Epoch: 11 [28160/60000 (47%)]\tLoss: 107.581161\n","Train Epoch: 11 [29440/60000 (49%)]\tLoss: 105.592911\n","Train Epoch: 11 [30720/60000 (51%)]\tLoss: 104.094421\n","Train Epoch: 11 [32000/60000 (53%)]\tLoss: 101.550858\n","Train Epoch: 11 [33280/60000 (55%)]\tLoss: 104.887863\n","Train Epoch: 11 [34560/60000 (58%)]\tLoss: 106.894699\n","Train Epoch: 11 [35840/60000 (60%)]\tLoss: 107.520065\n","Train Epoch: 11 [37120/60000 (62%)]\tLoss: 104.662743\n","Train Epoch: 11 [38400/60000 (64%)]\tLoss: 106.013565\n","Train Epoch: 11 [39680/60000 (66%)]\tLoss: 103.259239\n","Train Epoch: 11 [40960/60000 (68%)]\tLoss: 111.219124\n","Train Epoch: 11 [42240/60000 (70%)]\tLoss: 110.482635\n","Train Epoch: 11 [43520/60000 (72%)]\tLoss: 103.193527\n","Train Epoch: 11 [44800/60000 (75%)]\tLoss: 106.337173\n","Train Epoch: 11 [46080/60000 (77%)]\tLoss: 107.160370\n","Train Epoch: 11 [47360/60000 (79%)]\tLoss: 105.080017\n","Train Epoch: 11 [48640/60000 (81%)]\tLoss: 109.257339\n","Train Epoch: 11 [49920/60000 (83%)]\tLoss: 107.041977\n","Train Epoch: 11 [51200/60000 (85%)]\tLoss: 104.233215\n","Train Epoch: 11 [52480/60000 (87%)]\tLoss: 107.514282\n","Train Epoch: 11 [53760/60000 (90%)]\tLoss: 106.806648\n","Train Epoch: 11 [55040/60000 (92%)]\tLoss: 107.434937\n","Train Epoch: 11 [56320/60000 (94%)]\tLoss: 107.965004\n","Train Epoch: 11 [57600/60000 (96%)]\tLoss: 105.417778\n","Train Epoch: 11 [58880/60000 (98%)]\tLoss: 102.591492\n","====> Epoch: 11 Average loss: 105.8584\n","====> Test set loss: 96.5807\n","Train Epoch: 12 [0/60000 (0%)]\tLoss: 104.829994\n","Train Epoch: 12 [1280/60000 (2%)]\tLoss: 106.627014\n","Train Epoch: 12 [2560/60000 (4%)]\tLoss: 103.599182\n","Train Epoch: 12 [3840/60000 (6%)]\tLoss: 112.228378\n","Train Epoch: 12 [5120/60000 (9%)]\tLoss: 107.672287\n","Train Epoch: 12 [6400/60000 (11%)]\tLoss: 101.206795\n","Train Epoch: 12 [7680/60000 (13%)]\tLoss: 106.292969\n","Train Epoch: 12 [8960/60000 (15%)]\tLoss: 104.143280\n","Train Epoch: 12 [10240/60000 (17%)]\tLoss: 101.276825\n","Train Epoch: 12 [11520/60000 (19%)]\tLoss: 104.500175\n","Train Epoch: 12 [12800/60000 (21%)]\tLoss: 103.417496\n","Train Epoch: 12 [14080/60000 (23%)]\tLoss: 105.890335\n","Train Epoch: 12 [15360/60000 (26%)]\tLoss: 103.768936\n","Train Epoch: 12 [16640/60000 (28%)]\tLoss: 108.842598\n","Train Epoch: 12 [17920/60000 (30%)]\tLoss: 104.949150\n","Train Epoch: 12 [19200/60000 (32%)]\tLoss: 107.754005\n","Train Epoch: 12 [20480/60000 (34%)]\tLoss: 106.283997\n","Train Epoch: 12 [21760/60000 (36%)]\tLoss: 104.301491\n","Train Epoch: 12 [23040/60000 (38%)]\tLoss: 104.652756\n","Train Epoch: 12 [24320/60000 (41%)]\tLoss: 105.953796\n","Train Epoch: 12 [25600/60000 (43%)]\tLoss: 107.162071\n","Train Epoch: 12 [26880/60000 (45%)]\tLoss: 108.954781\n","Train Epoch: 12 [28160/60000 (47%)]\tLoss: 107.974319\n","Train Epoch: 12 [29440/60000 (49%)]\tLoss: 100.483963\n","Train Epoch: 12 [30720/60000 (51%)]\tLoss: 104.271370\n","Train Epoch: 12 [32000/60000 (53%)]\tLoss: 105.072769\n","Train Epoch: 12 [33280/60000 (55%)]\tLoss: 106.413887\n","Train Epoch: 12 [34560/60000 (58%)]\tLoss: 108.136253\n","Train Epoch: 12 [35840/60000 (60%)]\tLoss: 106.037537\n","Train Epoch: 12 [37120/60000 (62%)]\tLoss: 107.826569\n","Train Epoch: 12 [38400/60000 (64%)]\tLoss: 107.993942\n","Train Epoch: 12 [39680/60000 (66%)]\tLoss: 107.271484\n","Train Epoch: 12 [40960/60000 (68%)]\tLoss: 104.960678\n","Train Epoch: 12 [42240/60000 (70%)]\tLoss: 105.069351\n","Train Epoch: 12 [43520/60000 (72%)]\tLoss: 110.236496\n","Train Epoch: 12 [44800/60000 (75%)]\tLoss: 103.567230\n","Train Epoch: 12 [46080/60000 (77%)]\tLoss: 107.406311\n","Train Epoch: 12 [47360/60000 (79%)]\tLoss: 104.193665\n","Train Epoch: 12 [48640/60000 (81%)]\tLoss: 106.519516\n","Train Epoch: 12 [49920/60000 (83%)]\tLoss: 110.880096\n","Train Epoch: 12 [51200/60000 (85%)]\tLoss: 102.683289\n","Train Epoch: 12 [52480/60000 (87%)]\tLoss: 107.101814\n","Train Epoch: 12 [53760/60000 (90%)]\tLoss: 105.469765\n","Train Epoch: 12 [55040/60000 (92%)]\tLoss: 104.802719\n","Train Epoch: 12 [56320/60000 (94%)]\tLoss: 109.470879\n","Train Epoch: 12 [57600/60000 (96%)]\tLoss: 107.553062\n","Train Epoch: 12 [58880/60000 (98%)]\tLoss: 106.622711\n","====> Epoch: 12 Average loss: 105.5259\n","====> Test set loss: 96.1537\n","Train Epoch: 13 [0/60000 (0%)]\tLoss: 105.205193\n","Train Epoch: 13 [1280/60000 (2%)]\tLoss: 102.133942\n","Train Epoch: 13 [2560/60000 (4%)]\tLoss: 107.683929\n","Train Epoch: 13 [3840/60000 (6%)]\tLoss: 104.043655\n","Train Epoch: 13 [5120/60000 (9%)]\tLoss: 108.096329\n","Train Epoch: 13 [6400/60000 (11%)]\tLoss: 106.179405\n","Train Epoch: 13 [7680/60000 (13%)]\tLoss: 110.827812\n","Train Epoch: 13 [8960/60000 (15%)]\tLoss: 104.546021\n","Train Epoch: 13 [10240/60000 (17%)]\tLoss: 102.189117\n","Train Epoch: 13 [11520/60000 (19%)]\tLoss: 105.801231\n","Train Epoch: 13 [12800/60000 (21%)]\tLoss: 107.237328\n","Train Epoch: 13 [14080/60000 (23%)]\tLoss: 108.502022\n","Train Epoch: 13 [15360/60000 (26%)]\tLoss: 104.626160\n","Train Epoch: 13 [16640/60000 (28%)]\tLoss: 105.792542\n","Train Epoch: 13 [17920/60000 (30%)]\tLoss: 110.345306\n","Train Epoch: 13 [19200/60000 (32%)]\tLoss: 105.588966\n","Train Epoch: 13 [20480/60000 (34%)]\tLoss: 101.345428\n","Train Epoch: 13 [21760/60000 (36%)]\tLoss: 108.307495\n","Train Epoch: 13 [23040/60000 (38%)]\tLoss: 103.194191\n","Train Epoch: 13 [24320/60000 (41%)]\tLoss: 102.562141\n","Train Epoch: 13 [25600/60000 (43%)]\tLoss: 107.085846\n","Train Epoch: 13 [26880/60000 (45%)]\tLoss: 105.698608\n","Train Epoch: 13 [28160/60000 (47%)]\tLoss: 105.625870\n","Train Epoch: 13 [29440/60000 (49%)]\tLoss: 99.619354\n","Train Epoch: 13 [30720/60000 (51%)]\tLoss: 104.938255\n","Train Epoch: 13 [32000/60000 (53%)]\tLoss: 105.190880\n","Train Epoch: 13 [33280/60000 (55%)]\tLoss: 107.329742\n","Train Epoch: 13 [34560/60000 (58%)]\tLoss: 104.211510\n","Train Epoch: 13 [35840/60000 (60%)]\tLoss: 104.933586\n","Train Epoch: 13 [37120/60000 (62%)]\tLoss: 103.151443\n","Train Epoch: 13 [38400/60000 (64%)]\tLoss: 101.964218\n","Train Epoch: 13 [39680/60000 (66%)]\tLoss: 100.619827\n","Train Epoch: 13 [40960/60000 (68%)]\tLoss: 103.878883\n","Train Epoch: 13 [42240/60000 (70%)]\tLoss: 105.502617\n","Train Epoch: 13 [43520/60000 (72%)]\tLoss: 105.279755\n","Train Epoch: 13 [44800/60000 (75%)]\tLoss: 104.755966\n","Train Epoch: 13 [46080/60000 (77%)]\tLoss: 106.220543\n","Train Epoch: 13 [47360/60000 (79%)]\tLoss: 105.581619\n","Train Epoch: 13 [48640/60000 (81%)]\tLoss: 109.695068\n","Train Epoch: 13 [49920/60000 (83%)]\tLoss: 105.188858\n","Train Epoch: 13 [51200/60000 (85%)]\tLoss: 106.452957\n","Train Epoch: 13 [52480/60000 (87%)]\tLoss: 104.151268\n","Train Epoch: 13 [53760/60000 (90%)]\tLoss: 103.363625\n","Train Epoch: 13 [55040/60000 (92%)]\tLoss: 106.919151\n","Train Epoch: 13 [56320/60000 (94%)]\tLoss: 108.269096\n","Train Epoch: 13 [57600/60000 (96%)]\tLoss: 102.896614\n","Train Epoch: 13 [58880/60000 (98%)]\tLoss: 103.216507\n","====> Epoch: 13 Average loss: 105.2597\n","====> Test set loss: 95.6565\n","Train Epoch: 14 [0/60000 (0%)]\tLoss: 106.441315\n","Train Epoch: 14 [1280/60000 (2%)]\tLoss: 107.644363\n","Train Epoch: 14 [2560/60000 (4%)]\tLoss: 106.860703\n","Train Epoch: 14 [3840/60000 (6%)]\tLoss: 105.957512\n","Train Epoch: 14 [5120/60000 (9%)]\tLoss: 110.041008\n","Train Epoch: 14 [6400/60000 (11%)]\tLoss: 106.265961\n","Train Epoch: 14 [7680/60000 (13%)]\tLoss: 104.442581\n","Train Epoch: 14 [8960/60000 (15%)]\tLoss: 103.558456\n","Train Epoch: 14 [10240/60000 (17%)]\tLoss: 108.031708\n","Train Epoch: 14 [11520/60000 (19%)]\tLoss: 108.822319\n","Train Epoch: 14 [12800/60000 (21%)]\tLoss: 104.108032\n","Train Epoch: 14 [14080/60000 (23%)]\tLoss: 108.384529\n","Train Epoch: 14 [15360/60000 (26%)]\tLoss: 104.410614\n","Train Epoch: 14 [16640/60000 (28%)]\tLoss: 103.465996\n","Train Epoch: 14 [17920/60000 (30%)]\tLoss: 103.714333\n","Train Epoch: 14 [19200/60000 (32%)]\tLoss: 104.606270\n","Train Epoch: 14 [20480/60000 (34%)]\tLoss: 103.357262\n","Train Epoch: 14 [21760/60000 (36%)]\tLoss: 105.886505\n","Train Epoch: 14 [23040/60000 (38%)]\tLoss: 103.536850\n","Train Epoch: 14 [24320/60000 (41%)]\tLoss: 102.301384\n","Train Epoch: 14 [25600/60000 (43%)]\tLoss: 105.986977\n","Train Epoch: 14 [26880/60000 (45%)]\tLoss: 102.960388\n","Train Epoch: 14 [28160/60000 (47%)]\tLoss: 102.685516\n","Train Epoch: 14 [29440/60000 (49%)]\tLoss: 99.125664\n","Train Epoch: 14 [30720/60000 (51%)]\tLoss: 107.479919\n","Train Epoch: 14 [32000/60000 (53%)]\tLoss: 105.581795\n","Train Epoch: 14 [33280/60000 (55%)]\tLoss: 102.715546\n","Train Epoch: 14 [34560/60000 (58%)]\tLoss: 110.452911\n","Train Epoch: 14 [35840/60000 (60%)]\tLoss: 102.731560\n","Train Epoch: 14 [37120/60000 (62%)]\tLoss: 105.264427\n","Train Epoch: 14 [38400/60000 (64%)]\tLoss: 102.825134\n","Train Epoch: 14 [39680/60000 (66%)]\tLoss: 103.131851\n","Train Epoch: 14 [40960/60000 (68%)]\tLoss: 104.087875\n","Train Epoch: 14 [42240/60000 (70%)]\tLoss: 102.818314\n","Train Epoch: 14 [43520/60000 (72%)]\tLoss: 103.745026\n","Train Epoch: 14 [44800/60000 (75%)]\tLoss: 101.843048\n","Train Epoch: 14 [46080/60000 (77%)]\tLoss: 108.115120\n","Train Epoch: 14 [47360/60000 (79%)]\tLoss: 105.365936\n","Train Epoch: 14 [48640/60000 (81%)]\tLoss: 101.810257\n","Train Epoch: 14 [49920/60000 (83%)]\tLoss: 102.511139\n","Train Epoch: 14 [51200/60000 (85%)]\tLoss: 102.345810\n","Train Epoch: 14 [52480/60000 (87%)]\tLoss: 103.580597\n","Train Epoch: 14 [53760/60000 (90%)]\tLoss: 103.433243\n","Train Epoch: 14 [55040/60000 (92%)]\tLoss: 101.049141\n","Train Epoch: 14 [56320/60000 (94%)]\tLoss: 105.562111\n","Train Epoch: 14 [57600/60000 (96%)]\tLoss: 103.899040\n","Train Epoch: 14 [58880/60000 (98%)]\tLoss: 106.684448\n","====> Epoch: 14 Average loss: 105.0138\n","====> Test set loss: 95.8217\n","Train Epoch: 15 [0/60000 (0%)]\tLoss: 111.267960\n","Train Epoch: 15 [1280/60000 (2%)]\tLoss: 106.300415\n","Train Epoch: 15 [2560/60000 (4%)]\tLoss: 105.599617\n","Train Epoch: 15 [3840/60000 (6%)]\tLoss: 106.893753\n","Train Epoch: 15 [5120/60000 (9%)]\tLoss: 103.801552\n","Train Epoch: 15 [6400/60000 (11%)]\tLoss: 105.461838\n","Train Epoch: 15 [7680/60000 (13%)]\tLoss: 98.132103\n","Train Epoch: 15 [8960/60000 (15%)]\tLoss: 99.545944\n","Train Epoch: 15 [10240/60000 (17%)]\tLoss: 103.134392\n","Train Epoch: 15 [11520/60000 (19%)]\tLoss: 104.747116\n","Train Epoch: 15 [12800/60000 (21%)]\tLoss: 104.794098\n","Train Epoch: 15 [14080/60000 (23%)]\tLoss: 103.336464\n","Train Epoch: 15 [15360/60000 (26%)]\tLoss: 108.501274\n","Train Epoch: 15 [16640/60000 (28%)]\tLoss: 103.558151\n","Train Epoch: 15 [17920/60000 (30%)]\tLoss: 107.408516\n","Train Epoch: 15 [19200/60000 (32%)]\tLoss: 99.916466\n","Train Epoch: 15 [20480/60000 (34%)]\tLoss: 102.844749\n","Train Epoch: 15 [21760/60000 (36%)]\tLoss: 106.640526\n","Train Epoch: 15 [23040/60000 (38%)]\tLoss: 101.837463\n","Train Epoch: 15 [24320/60000 (41%)]\tLoss: 100.257568\n","Train Epoch: 15 [25600/60000 (43%)]\tLoss: 108.503464\n","Train Epoch: 15 [26880/60000 (45%)]\tLoss: 108.167419\n","Train Epoch: 15 [28160/60000 (47%)]\tLoss: 106.017540\n","Train Epoch: 15 [29440/60000 (49%)]\tLoss: 106.353874\n","Train Epoch: 15 [30720/60000 (51%)]\tLoss: 104.665771\n","Train Epoch: 15 [32000/60000 (53%)]\tLoss: 104.963142\n","Train Epoch: 15 [33280/60000 (55%)]\tLoss: 105.648247\n","Train Epoch: 15 [34560/60000 (58%)]\tLoss: 103.435242\n","Train Epoch: 15 [35840/60000 (60%)]\tLoss: 102.535141\n","Train Epoch: 15 [37120/60000 (62%)]\tLoss: 105.539673\n","Train Epoch: 15 [38400/60000 (64%)]\tLoss: 104.072487\n","Train Epoch: 15 [39680/60000 (66%)]\tLoss: 102.983856\n","Train Epoch: 15 [40960/60000 (68%)]\tLoss: 104.019905\n","Train Epoch: 15 [42240/60000 (70%)]\tLoss: 103.890266\n","Train Epoch: 15 [43520/60000 (72%)]\tLoss: 107.785767\n","Train Epoch: 15 [44800/60000 (75%)]\tLoss: 102.936188\n","Train Epoch: 15 [46080/60000 (77%)]\tLoss: 101.906265\n","Train Epoch: 15 [47360/60000 (79%)]\tLoss: 104.485565\n","Train Epoch: 15 [48640/60000 (81%)]\tLoss: 105.492035\n","Train Epoch: 15 [49920/60000 (83%)]\tLoss: 104.651482\n","Train Epoch: 15 [51200/60000 (85%)]\tLoss: 105.057259\n","Train Epoch: 15 [52480/60000 (87%)]\tLoss: 104.570068\n","Train Epoch: 15 [53760/60000 (90%)]\tLoss: 108.235962\n","Train Epoch: 15 [55040/60000 (92%)]\tLoss: 106.052155\n","Train Epoch: 15 [56320/60000 (94%)]\tLoss: 102.449280\n","Train Epoch: 15 [57600/60000 (96%)]\tLoss: 101.742172\n","Train Epoch: 15 [58880/60000 (98%)]\tLoss: 104.218536\n","====> Epoch: 15 Average loss: 104.7919\n","====> Test set loss: 95.3495\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"ty3l5dGjnz4w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"3xQfsUiTnz11"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"aeAxvk0CnW9d"},"execution_count":null,"outputs":[]}]}