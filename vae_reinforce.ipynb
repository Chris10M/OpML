{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"vae_reinforce.ipynb","provenance":[],"authorship_tag":"ABX9TyNrBRWc7bcGCJX2LO1BXflj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dos7kAfMqPhU","executionInfo":{"status":"ok","timestamp":1655470026785,"user_tz":-120,"elapsed":16806,"user":{"displayName":"Rajmohan Tumarada","userId":"18412595153668734700"}},"outputId":"1b55dd1d-a011-4c21-bd93-f92ded0d7bf1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["%cd '/content/drive/MyDrive/Colab Notebooks/optMl'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ItJZc7jXqZ5J","executionInfo":{"status":"ok","timestamp":1655470029538,"user_tz":-120,"elapsed":6,"user":{"displayName":"Rajmohan Tumarada","userId":"18412595153668734700"}},"outputId":"460eee81-9ca1-4af4-ec17-48e8e06c08c5"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/optMl\n"]}]},{"cell_type":"code","source":["# Code to implement VAE-gumple_softmax in pytorch\n","# author: Devinder Kumar (devinder.kumar@uwaterloo.ca), modified by Yongfei Yan\n","# The code has been modified from pytorch example vae code and inspired by the origianl \\\n","# tensorflow implementation of gumble-softmax by Eric Jang.\n","\n","import sys\n","import argparse\n","import numpy as np\n","\n","import torch\n","import torch.nn.functional as F\n","from torch import nn, optim\n","from torch.nn import functional as F\n","from torchvision import datasets, transforms\n","from torchvision.utils import save_image\n","from torch.distributions import OneHotCategorical"],"metadata":{"id":"Kbq-NhZdqZ8I","executionInfo":{"status":"ok","timestamp":1655470036466,"user_tz":-120,"elapsed":3602,"user":{"displayName":"Rajmohan Tumarada","userId":"18412595153668734700"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["parser = argparse.ArgumentParser(description='VAE MNIST Example')\n","parser.add_argument('--batch-size', type=int, default=128, metavar='N',\n","                    help='input batch size for training (default: 100)')\n","parser.add_argument('--epochs', type=int, default=15, metavar='N',\n","                    help='number of epochs to train (default: 10)')\n","parser.add_argument('--temp', type=float, default=1.0, metavar='S',\n","                    help='tau(temperature) (default: 1.0)')\n","parser.add_argument('--no-cuda', action='store_true', default=False,\n","                    help='enables CUDA training')\n","parser.add_argument('--seed', type=int, default=1, metavar='S',\n","                    help='random seed (default: 1)')\n","parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n","                    help='how many batches to wait before logging training status')\n","parser.add_argument('--hard', action='store_true', default=False,\n","                    help='hard Gumbel softmax')\n","\n","sys.argv=['']\n","del sys\n","\n","args = parser.parse_args()\n","args.cuda = not args.no_cuda and torch.cuda.is_available()\n","torch.manual_seed(args.seed)\n","if args.cuda:\n","    torch.cuda.manual_seed(args.seed)\n","\n","device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n","\n","kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}"],"metadata":{"id":"9iDNt4G_qZ_a","executionInfo":{"status":"ok","timestamp":1655470039532,"user_tz":-120,"elapsed":391,"user":{"displayName":"Rajmohan Tumarada","userId":"18412595153668734700"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["train_loader = torch.utils.data.DataLoader(\n","    datasets.MNIST('./data/MNIST', train=True, download=True,\n","                   transform=transforms.ToTensor()),\n","    batch_size=args.batch_size, shuffle=True, **kwargs)\n","test_loader = torch.utils.data.DataLoader(\n","    datasets.MNIST('./data/MNIST', train=False, transform=transforms.ToTensor()),\n","    batch_size=args.batch_size, shuffle=True, **kwargs)"],"metadata":{"id":"AvQO-Vw3qaCH","executionInfo":{"status":"ok","timestamp":1655470043357,"user_tz":-120,"elapsed":1743,"user":{"displayName":"Rajmohan Tumarada","userId":"18412595153668734700"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class VAE(nn.Module):\n","    def __init__(self):\n","        super(VAE, self).__init__()\n","\n","        self.fc1 = nn.Linear(784, 400)\n","        self.fc2 = nn.Linear(400, 20*256)\n","        self.fc3 = nn.Linear(20*256, 400)\n","        self.fc4 = nn.Linear(400, 784)\n","\n","    def encode(self, x):\n","        h1 = F.relu(self.fc1(x))\n","        probs = F.softmax(self.fc2(h1).view(len(x), 20, 256), -1)\n","        return probs\n","\n","    def decode(self, z):\n","        h3 = F.relu(self.fc3(z.view(len(z), 20*256)))\n","        return F.sigmoid(self.fc4(h3))\n","\n","    def forward(self, x):\n","        # For convenience we use torch.distributions to sample and compute the values of interest for the distribution see (https://pytorch.org/docs/stable/distributions.html) for more details.\n","        probs = self.encode(x.view(-1, 784))\n","        m = OneHotCategorical(probs)\n","        action = m.sample()\n","        log_prob = m.log_prob(action)\n","        entropy = m.entropy()\n","        return self.decode(action), log_prob, entropy"],"metadata":{"id":"fl3SjFnPqaEm","executionInfo":{"status":"ok","timestamp":1655470043358,"user_tz":-120,"elapsed":7,"user":{"displayName":"Rajmohan Tumarada","userId":"18412595153668734700"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["model = VAE().to(device)\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)"],"metadata":{"id":"CqkwoEXMqaHw","executionInfo":{"status":"ok","timestamp":1655470056769,"user_tz":-120,"elapsed":12265,"user":{"displayName":"Rajmohan Tumarada","userId":"18412595153668734700"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Reconstruction + KL divergence losses summed over all elements and batch\n","def loss_function(recon_x, x, log_prob, entropy):\n","    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduce=False).sum(-1)\n","    # We make the assumption that q(z1,..,zd|x) = q(z1|x)...q(zd|x)\n","    log_prob = log_prob.sum(-1)\n","    # The Reinforce loss is just log_prob*loss\n","    reinforce_loss = torch.sum(log_prob*BCE.detach())\n","    # If the prior on the latent is uniform then the KL is just the entropy of q(z|x)\n","    # We add reinforce_loss - reinforce_loss.detach() so we can backpropagate through the encoder with REINFORCE but it doesn't modify the loss.\n","    loss = BCE.sum() + reinforce_loss - reinforce_loss.detach() + entropy.sum()\n","    return loss"],"metadata":{"id":"oJgPO-gbqaKX","executionInfo":{"status":"ok","timestamp":1655470056770,"user_tz":-120,"elapsed":43,"user":{"displayName":"Rajmohan Tumarada","userId":"18412595153668734700"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def train(epoch):\n","    model.train()\n","    train_loss = 0\n","    for batch_idx, (data, _) in enumerate(train_loader):\n","        data = data.to(device)\n","        optimizer.zero_grad()\n","        recon_batch, log_prob, entropy = model(data)\n","        loss = loss_function(recon_batch, data, log_prob, entropy)\n","        loss.backward()\n","        train_loss += loss.item()\n","        optimizer.step()\n","        if batch_idx % args.log_interval == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader),\n","                loss.item() / len(data)))\n","\n","    print('====> Epoch: {} Average loss: {:.4f}'.format(\n","          epoch, train_loss / len(train_loader.dataset)))"],"metadata":{"id":"-qC3QolvqaNK","executionInfo":{"status":"ok","timestamp":1655470056772,"user_tz":-120,"elapsed":42,"user":{"displayName":"Rajmohan Tumarada","userId":"18412595153668734700"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def test(epoch):\n","    model.eval()\n","    test_loss = 0\n","    with torch.no_grad():\n","        for i, (data, _) in enumerate(test_loader):\n","            data = data.to(device)\n","            recon_batch, log_prob, entropy = model(data)\n","            test_loss += loss_function(recon_batch, data, log_prob, entropy).item()\n","            if i == 0:\n","                n = min(data.size(0), 8)\n","                comparison = torch.cat([data[:n],\n","                                      recon_batch.view(args.batch_size, 1, 28, 28)[:n]])\n","                save_image(comparison.cpu(),\n","                         'resultsReinforce/reconstruction_' + str(epoch) + '.png', nrow=n)\n","\n","    test_loss /= len(test_loader.dataset)\n","    print('====> Test set loss: {:.4f}'.format(test_loss))"],"metadata":{"id":"nipD7wJlqaQL","executionInfo":{"status":"ok","timestamp":1655470056773,"user_tz":-120,"elapsed":41,"user":{"displayName":"Rajmohan Tumarada","userId":"18412595153668734700"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def run():\n","  for epoch in range(1, args.epochs + 1):\n","    train(epoch)\n","    test(epoch)\n","    with torch.no_grad():\n","        m = OneHotCategorical(torch.ones(256)/256.)\n","        sample = m.sample((64, 20))\n","        sample = sample.to(device)\n","        sample = model.decode(sample).cpu()\n","        save_image(sample.view(64, 1, 28, 28),\n","                   'resultsReinforce/sample_' + str(epoch) + '.png')\n","        \n","if __name__ == '__main__':\n","    run()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DCrLm5fRqaS_","executionInfo":{"status":"ok","timestamp":1655470196568,"user_tz":-120,"elapsed":139835,"user":{"displayName":"Rajmohan Tumarada","userId":"18412595153668734700"}},"outputId":"996b7829-8bb6-4993-901a-8e22afa80824"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"output_type":"stream","name":"stdout","text":["Train Epoch: 1 [0/60000 (0%)]\tLoss: 654.527527\n","Train Epoch: 1 [1280/60000 (2%)]\tLoss: 579.028687\n","Train Epoch: 1 [2560/60000 (4%)]\tLoss: 386.690308\n","Train Epoch: 1 [3840/60000 (6%)]\tLoss: 332.681152\n","Train Epoch: 1 [5120/60000 (9%)]\tLoss: 322.499573\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 306.924103\n","Train Epoch: 1 [7680/60000 (13%)]\tLoss: 305.222351\n","Train Epoch: 1 [8960/60000 (15%)]\tLoss: 302.228882\n","Train Epoch: 1 [10240/60000 (17%)]\tLoss: 295.811829\n","Train Epoch: 1 [11520/60000 (19%)]\tLoss: 288.683838\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 274.801025\n","Train Epoch: 1 [14080/60000 (23%)]\tLoss: 267.148560\n","Train Epoch: 1 [15360/60000 (26%)]\tLoss: 250.772507\n","Train Epoch: 1 [16640/60000 (28%)]\tLoss: 239.676605\n","Train Epoch: 1 [17920/60000 (30%)]\tLoss: 234.244049\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 228.149246\n","Train Epoch: 1 [20480/60000 (34%)]\tLoss: 226.102417\n","Train Epoch: 1 [21760/60000 (36%)]\tLoss: 222.569153\n","Train Epoch: 1 [23040/60000 (38%)]\tLoss: 217.711426\n","Train Epoch: 1 [24320/60000 (41%)]\tLoss: 208.980453\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 216.186661\n","Train Epoch: 1 [26880/60000 (45%)]\tLoss: 205.435135\n","Train Epoch: 1 [28160/60000 (47%)]\tLoss: 211.506332\n","Train Epoch: 1 [29440/60000 (49%)]\tLoss: 199.949875\n","Train Epoch: 1 [30720/60000 (51%)]\tLoss: 206.301331\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 199.283798\n","Train Epoch: 1 [33280/60000 (55%)]\tLoss: 206.178482\n","Train Epoch: 1 [34560/60000 (58%)]\tLoss: 208.212769\n","Train Epoch: 1 [35840/60000 (60%)]\tLoss: 200.298630\n","Train Epoch: 1 [37120/60000 (62%)]\tLoss: 205.695557\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 204.249481\n","Train Epoch: 1 [39680/60000 (66%)]\tLoss: 207.803818\n","Train Epoch: 1 [40960/60000 (68%)]\tLoss: 208.901794\n","Train Epoch: 1 [42240/60000 (70%)]\tLoss: 199.194031\n","Train Epoch: 1 [43520/60000 (72%)]\tLoss: 212.360947\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 210.763367\n","Train Epoch: 1 [46080/60000 (77%)]\tLoss: 199.234848\n","Train Epoch: 1 [47360/60000 (79%)]\tLoss: 205.469391\n","Train Epoch: 1 [48640/60000 (81%)]\tLoss: 203.542633\n","Train Epoch: 1 [49920/60000 (83%)]\tLoss: 203.400620\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 203.481842\n","Train Epoch: 1 [52480/60000 (87%)]\tLoss: 204.173904\n","Train Epoch: 1 [53760/60000 (90%)]\tLoss: 206.424194\n","Train Epoch: 1 [55040/60000 (92%)]\tLoss: 208.142990\n","Train Epoch: 1 [56320/60000 (94%)]\tLoss: 199.570786\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 206.229233\n","Train Epoch: 1 [58880/60000 (98%)]\tLoss: 196.677902\n","====> Epoch: 1 Average loss: 244.3382\n","====> Test set loss: 201.6299\n","Train Epoch: 2 [0/60000 (0%)]\tLoss: 203.287933\n","Train Epoch: 2 [1280/60000 (2%)]\tLoss: 198.699219\n","Train Epoch: 2 [2560/60000 (4%)]\tLoss: 200.189529\n","Train Epoch: 2 [3840/60000 (6%)]\tLoss: 204.062454\n","Train Epoch: 2 [5120/60000 (9%)]\tLoss: 203.793198\n","Train Epoch: 2 [6400/60000 (11%)]\tLoss: 199.417130\n","Train Epoch: 2 [7680/60000 (13%)]\tLoss: 201.963928\n","Train Epoch: 2 [8960/60000 (15%)]\tLoss: 209.017471\n","Train Epoch: 2 [10240/60000 (17%)]\tLoss: 198.499924\n","Train Epoch: 2 [11520/60000 (19%)]\tLoss: 197.869614\n","Train Epoch: 2 [12800/60000 (21%)]\tLoss: 209.055542\n","Train Epoch: 2 [14080/60000 (23%)]\tLoss: 193.930435\n","Train Epoch: 2 [15360/60000 (26%)]\tLoss: 201.636261\n","Train Epoch: 2 [16640/60000 (28%)]\tLoss: 201.242218\n","Train Epoch: 2 [17920/60000 (30%)]\tLoss: 188.771683\n","Train Epoch: 2 [19200/60000 (32%)]\tLoss: 200.340439\n","Train Epoch: 2 [20480/60000 (34%)]\tLoss: 192.724854\n","Train Epoch: 2 [21760/60000 (36%)]\tLoss: 199.607880\n","Train Epoch: 2 [23040/60000 (38%)]\tLoss: 198.011795\n","Train Epoch: 2 [24320/60000 (41%)]\tLoss: 202.940643\n","Train Epoch: 2 [25600/60000 (43%)]\tLoss: 198.051468\n","Train Epoch: 2 [26880/60000 (45%)]\tLoss: 193.689911\n","Train Epoch: 2 [28160/60000 (47%)]\tLoss: 201.164215\n","Train Epoch: 2 [29440/60000 (49%)]\tLoss: 195.284958\n","Train Epoch: 2 [30720/60000 (51%)]\tLoss: 199.008469\n","Train Epoch: 2 [32000/60000 (53%)]\tLoss: 195.515701\n","Train Epoch: 2 [33280/60000 (55%)]\tLoss: 202.856033\n","Train Epoch: 2 [34560/60000 (58%)]\tLoss: 197.249435\n","Train Epoch: 2 [35840/60000 (60%)]\tLoss: 197.234360\n","Train Epoch: 2 [37120/60000 (62%)]\tLoss: 199.361526\n","Train Epoch: 2 [38400/60000 (64%)]\tLoss: 200.677856\n","Train Epoch: 2 [39680/60000 (66%)]\tLoss: 195.315720\n","Train Epoch: 2 [40960/60000 (68%)]\tLoss: 189.986130\n","Train Epoch: 2 [42240/60000 (70%)]\tLoss: 187.068604\n","Train Epoch: 2 [43520/60000 (72%)]\tLoss: 202.461960\n","Train Epoch: 2 [44800/60000 (75%)]\tLoss: 201.917572\n","Train Epoch: 2 [46080/60000 (77%)]\tLoss: 200.405716\n","Train Epoch: 2 [47360/60000 (79%)]\tLoss: 205.521317\n","Train Epoch: 2 [48640/60000 (81%)]\tLoss: 191.262299\n","Train Epoch: 2 [49920/60000 (83%)]\tLoss: 201.229904\n","Train Epoch: 2 [51200/60000 (85%)]\tLoss: 196.426529\n","Train Epoch: 2 [52480/60000 (87%)]\tLoss: 195.276764\n","Train Epoch: 2 [53760/60000 (90%)]\tLoss: 195.573029\n","Train Epoch: 2 [55040/60000 (92%)]\tLoss: 194.754013\n","Train Epoch: 2 [56320/60000 (94%)]\tLoss: 197.315292\n","Train Epoch: 2 [57600/60000 (96%)]\tLoss: 195.947189\n","Train Epoch: 2 [58880/60000 (98%)]\tLoss: 208.308670\n","====> Epoch: 2 Average loss: 199.2017\n","====> Test set loss: 198.9545\n","Train Epoch: 3 [0/60000 (0%)]\tLoss: 199.651978\n","Train Epoch: 3 [1280/60000 (2%)]\tLoss: 197.417496\n","Train Epoch: 3 [2560/60000 (4%)]\tLoss: 203.568619\n","Train Epoch: 3 [3840/60000 (6%)]\tLoss: 195.342148\n","Train Epoch: 3 [5120/60000 (9%)]\tLoss: 197.364136\n","Train Epoch: 3 [6400/60000 (11%)]\tLoss: 198.785995\n","Train Epoch: 3 [7680/60000 (13%)]\tLoss: 197.393341\n","Train Epoch: 3 [8960/60000 (15%)]\tLoss: 198.205612\n","Train Epoch: 3 [10240/60000 (17%)]\tLoss: 208.176483\n","Train Epoch: 3 [11520/60000 (19%)]\tLoss: 195.602142\n","Train Epoch: 3 [12800/60000 (21%)]\tLoss: 199.547379\n","Train Epoch: 3 [14080/60000 (23%)]\tLoss: 202.462402\n","Train Epoch: 3 [15360/60000 (26%)]\tLoss: 201.424530\n","Train Epoch: 3 [16640/60000 (28%)]\tLoss: 192.470123\n","Train Epoch: 3 [17920/60000 (30%)]\tLoss: 189.732529\n","Train Epoch: 3 [19200/60000 (32%)]\tLoss: 201.015854\n","Train Epoch: 3 [20480/60000 (34%)]\tLoss: 200.790878\n","Train Epoch: 3 [21760/60000 (36%)]\tLoss: 199.328308\n","Train Epoch: 3 [23040/60000 (38%)]\tLoss: 206.100647\n","Train Epoch: 3 [24320/60000 (41%)]\tLoss: 199.168640\n","Train Epoch: 3 [25600/60000 (43%)]\tLoss: 199.834610\n","Train Epoch: 3 [26880/60000 (45%)]\tLoss: 196.312973\n","Train Epoch: 3 [28160/60000 (47%)]\tLoss: 191.541138\n","Train Epoch: 3 [29440/60000 (49%)]\tLoss: 189.753342\n","Train Epoch: 3 [30720/60000 (51%)]\tLoss: 197.347397\n","Train Epoch: 3 [32000/60000 (53%)]\tLoss: 191.519455\n","Train Epoch: 3 [33280/60000 (55%)]\tLoss: 195.635239\n","Train Epoch: 3 [34560/60000 (58%)]\tLoss: 191.684494\n","Train Epoch: 3 [35840/60000 (60%)]\tLoss: 186.747513\n","Train Epoch: 3 [37120/60000 (62%)]\tLoss: 197.229782\n","Train Epoch: 3 [38400/60000 (64%)]\tLoss: 193.974457\n","Train Epoch: 3 [39680/60000 (66%)]\tLoss: 194.510345\n","Train Epoch: 3 [40960/60000 (68%)]\tLoss: 185.960007\n","Train Epoch: 3 [42240/60000 (70%)]\tLoss: 194.923569\n","Train Epoch: 3 [43520/60000 (72%)]\tLoss: 192.980560\n","Train Epoch: 3 [44800/60000 (75%)]\tLoss: 191.128540\n","Train Epoch: 3 [46080/60000 (77%)]\tLoss: 189.491928\n","Train Epoch: 3 [47360/60000 (79%)]\tLoss: 186.178207\n","Train Epoch: 3 [48640/60000 (81%)]\tLoss: 191.466049\n","Train Epoch: 3 [49920/60000 (83%)]\tLoss: 190.484222\n","Train Epoch: 3 [51200/60000 (85%)]\tLoss: 202.773575\n","Train Epoch: 3 [52480/60000 (87%)]\tLoss: 182.295776\n","Train Epoch: 3 [53760/60000 (90%)]\tLoss: 200.552109\n","Train Epoch: 3 [55040/60000 (92%)]\tLoss: 192.455017\n","Train Epoch: 3 [56320/60000 (94%)]\tLoss: 193.796127\n","Train Epoch: 3 [57600/60000 (96%)]\tLoss: 187.522354\n","Train Epoch: 3 [58880/60000 (98%)]\tLoss: 192.682434\n","====> Epoch: 3 Average loss: 195.3470\n","====> Test set loss: 191.8637\n","Train Epoch: 4 [0/60000 (0%)]\tLoss: 191.512192\n","Train Epoch: 4 [1280/60000 (2%)]\tLoss: 191.255463\n","Train Epoch: 4 [2560/60000 (4%)]\tLoss: 192.066528\n","Train Epoch: 4 [3840/60000 (6%)]\tLoss: 195.287308\n","Train Epoch: 4 [5120/60000 (9%)]\tLoss: 188.894485\n","Train Epoch: 4 [6400/60000 (11%)]\tLoss: 192.944641\n","Train Epoch: 4 [7680/60000 (13%)]\tLoss: 192.024948\n","Train Epoch: 4 [8960/60000 (15%)]\tLoss: 200.798904\n","Train Epoch: 4 [10240/60000 (17%)]\tLoss: 190.195374\n","Train Epoch: 4 [11520/60000 (19%)]\tLoss: 196.544876\n","Train Epoch: 4 [12800/60000 (21%)]\tLoss: 181.062057\n","Train Epoch: 4 [14080/60000 (23%)]\tLoss: 193.991043\n","Train Epoch: 4 [15360/60000 (26%)]\tLoss: 187.472626\n","Train Epoch: 4 [16640/60000 (28%)]\tLoss: 190.016586\n","Train Epoch: 4 [17920/60000 (30%)]\tLoss: 190.195175\n","Train Epoch: 4 [19200/60000 (32%)]\tLoss: 190.662918\n","Train Epoch: 4 [20480/60000 (34%)]\tLoss: 180.072235\n","Train Epoch: 4 [21760/60000 (36%)]\tLoss: 186.518402\n","Train Epoch: 4 [23040/60000 (38%)]\tLoss: 193.476837\n","Train Epoch: 4 [24320/60000 (41%)]\tLoss: 188.234680\n","Train Epoch: 4 [25600/60000 (43%)]\tLoss: 197.142136\n","Train Epoch: 4 [26880/60000 (45%)]\tLoss: 181.464661\n","Train Epoch: 4 [28160/60000 (47%)]\tLoss: 191.568100\n","Train Epoch: 4 [29440/60000 (49%)]\tLoss: 186.159683\n","Train Epoch: 4 [30720/60000 (51%)]\tLoss: 180.071106\n","Train Epoch: 4 [32000/60000 (53%)]\tLoss: 178.382080\n","Train Epoch: 4 [33280/60000 (55%)]\tLoss: 189.796432\n","Train Epoch: 4 [34560/60000 (58%)]\tLoss: 186.511993\n","Train Epoch: 4 [35840/60000 (60%)]\tLoss: 183.932343\n","Train Epoch: 4 [37120/60000 (62%)]\tLoss: 187.457321\n","Train Epoch: 4 [38400/60000 (64%)]\tLoss: 186.300308\n","Train Epoch: 4 [39680/60000 (66%)]\tLoss: 188.372391\n","Train Epoch: 4 [40960/60000 (68%)]\tLoss: 186.374161\n","Train Epoch: 4 [42240/60000 (70%)]\tLoss: 182.194092\n","Train Epoch: 4 [43520/60000 (72%)]\tLoss: 182.850494\n","Train Epoch: 4 [44800/60000 (75%)]\tLoss: 185.752625\n","Train Epoch: 4 [46080/60000 (77%)]\tLoss: 188.051422\n","Train Epoch: 4 [47360/60000 (79%)]\tLoss: 183.982620\n","Train Epoch: 4 [48640/60000 (81%)]\tLoss: 188.959351\n","Train Epoch: 4 [49920/60000 (83%)]\tLoss: 181.606079\n","Train Epoch: 4 [51200/60000 (85%)]\tLoss: 176.112305\n","Train Epoch: 4 [52480/60000 (87%)]\tLoss: 192.127945\n","Train Epoch: 4 [53760/60000 (90%)]\tLoss: 190.024002\n","Train Epoch: 4 [55040/60000 (92%)]\tLoss: 185.117432\n","Train Epoch: 4 [56320/60000 (94%)]\tLoss: 176.820541\n","Train Epoch: 4 [57600/60000 (96%)]\tLoss: 176.518845\n","Train Epoch: 4 [58880/60000 (98%)]\tLoss: 183.839050\n","====> Epoch: 4 Average loss: 187.0729\n","====> Test set loss: 184.0291\n","Train Epoch: 5 [0/60000 (0%)]\tLoss: 180.972244\n","Train Epoch: 5 [1280/60000 (2%)]\tLoss: 182.330048\n","Train Epoch: 5 [2560/60000 (4%)]\tLoss: 188.632202\n","Train Epoch: 5 [3840/60000 (6%)]\tLoss: 180.341110\n","Train Epoch: 5 [5120/60000 (9%)]\tLoss: 183.376404\n","Train Epoch: 5 [6400/60000 (11%)]\tLoss: 177.575577\n","Train Epoch: 5 [7680/60000 (13%)]\tLoss: 175.608734\n","Train Epoch: 5 [8960/60000 (15%)]\tLoss: 185.812836\n","Train Epoch: 5 [10240/60000 (17%)]\tLoss: 175.548172\n","Train Epoch: 5 [11520/60000 (19%)]\tLoss: 181.227295\n","Train Epoch: 5 [12800/60000 (21%)]\tLoss: 175.594193\n","Train Epoch: 5 [14080/60000 (23%)]\tLoss: 187.100693\n","Train Epoch: 5 [15360/60000 (26%)]\tLoss: 168.986954\n","Train Epoch: 5 [16640/60000 (28%)]\tLoss: 176.961182\n","Train Epoch: 5 [17920/60000 (30%)]\tLoss: 184.334732\n","Train Epoch: 5 [19200/60000 (32%)]\tLoss: 179.667953\n","Train Epoch: 5 [20480/60000 (34%)]\tLoss: 176.369720\n","Train Epoch: 5 [21760/60000 (36%)]\tLoss: 180.974548\n","Train Epoch: 5 [23040/60000 (38%)]\tLoss: 186.681778\n","Train Epoch: 5 [24320/60000 (41%)]\tLoss: 178.792786\n","Train Epoch: 5 [25600/60000 (43%)]\tLoss: 184.104172\n","Train Epoch: 5 [26880/60000 (45%)]\tLoss: 178.757187\n","Train Epoch: 5 [28160/60000 (47%)]\tLoss: 187.791275\n","Train Epoch: 5 [29440/60000 (49%)]\tLoss: 182.208939\n","Train Epoch: 5 [30720/60000 (51%)]\tLoss: 188.292404\n","Train Epoch: 5 [32000/60000 (53%)]\tLoss: 179.380920\n","Train Epoch: 5 [33280/60000 (55%)]\tLoss: 175.846710\n","Train Epoch: 5 [34560/60000 (58%)]\tLoss: 183.753006\n","Train Epoch: 5 [35840/60000 (60%)]\tLoss: 188.009613\n","Train Epoch: 5 [37120/60000 (62%)]\tLoss: 178.794479\n","Train Epoch: 5 [38400/60000 (64%)]\tLoss: 178.383835\n","Train Epoch: 5 [39680/60000 (66%)]\tLoss: 174.503784\n","Train Epoch: 5 [40960/60000 (68%)]\tLoss: 177.952164\n","Train Epoch: 5 [42240/60000 (70%)]\tLoss: 178.213943\n","Train Epoch: 5 [43520/60000 (72%)]\tLoss: 171.926132\n","Train Epoch: 5 [44800/60000 (75%)]\tLoss: 173.366226\n","Train Epoch: 5 [46080/60000 (77%)]\tLoss: 176.386536\n","Train Epoch: 5 [47360/60000 (79%)]\tLoss: 173.987823\n","Train Epoch: 5 [48640/60000 (81%)]\tLoss: 172.248123\n","Train Epoch: 5 [49920/60000 (83%)]\tLoss: 181.364975\n","Train Epoch: 5 [51200/60000 (85%)]\tLoss: 178.772446\n","Train Epoch: 5 [52480/60000 (87%)]\tLoss: 175.414062\n","Train Epoch: 5 [53760/60000 (90%)]\tLoss: 178.965042\n","Train Epoch: 5 [55040/60000 (92%)]\tLoss: 172.324432\n","Train Epoch: 5 [56320/60000 (94%)]\tLoss: 175.100449\n","Train Epoch: 5 [57600/60000 (96%)]\tLoss: 178.385162\n","Train Epoch: 5 [58880/60000 (98%)]\tLoss: 172.233627\n","====> Epoch: 5 Average loss: 179.2281\n","====> Test set loss: 176.1638\n","Train Epoch: 6 [0/60000 (0%)]\tLoss: 190.555756\n","Train Epoch: 6 [1280/60000 (2%)]\tLoss: 169.810654\n","Train Epoch: 6 [2560/60000 (4%)]\tLoss: 167.196762\n","Train Epoch: 6 [3840/60000 (6%)]\tLoss: 175.484055\n","Train Epoch: 6 [5120/60000 (9%)]\tLoss: 179.800919\n","Train Epoch: 6 [6400/60000 (11%)]\tLoss: 182.025116\n","Train Epoch: 6 [7680/60000 (13%)]\tLoss: 169.025116\n","Train Epoch: 6 [8960/60000 (15%)]\tLoss: 184.656815\n","Train Epoch: 6 [10240/60000 (17%)]\tLoss: 181.098801\n","Train Epoch: 6 [11520/60000 (19%)]\tLoss: 178.006378\n","Train Epoch: 6 [12800/60000 (21%)]\tLoss: 175.873596\n","Train Epoch: 6 [14080/60000 (23%)]\tLoss: 177.190765\n","Train Epoch: 6 [15360/60000 (26%)]\tLoss: 177.585556\n","Train Epoch: 6 [16640/60000 (28%)]\tLoss: 176.394913\n","Train Epoch: 6 [17920/60000 (30%)]\tLoss: 178.133774\n","Train Epoch: 6 [19200/60000 (32%)]\tLoss: 170.251709\n","Train Epoch: 6 [20480/60000 (34%)]\tLoss: 163.235245\n","Train Epoch: 6 [21760/60000 (36%)]\tLoss: 175.604492\n","Train Epoch: 6 [23040/60000 (38%)]\tLoss: 173.820084\n","Train Epoch: 6 [24320/60000 (41%)]\tLoss: 166.550568\n","Train Epoch: 6 [25600/60000 (43%)]\tLoss: 175.236832\n","Train Epoch: 6 [26880/60000 (45%)]\tLoss: 171.179306\n","Train Epoch: 6 [28160/60000 (47%)]\tLoss: 173.626099\n","Train Epoch: 6 [29440/60000 (49%)]\tLoss: 170.827560\n","Train Epoch: 6 [30720/60000 (51%)]\tLoss: 173.163879\n","Train Epoch: 6 [32000/60000 (53%)]\tLoss: 171.525970\n","Train Epoch: 6 [33280/60000 (55%)]\tLoss: 167.106735\n","Train Epoch: 6 [34560/60000 (58%)]\tLoss: 175.919815\n","Train Epoch: 6 [35840/60000 (60%)]\tLoss: 172.228348\n","Train Epoch: 6 [37120/60000 (62%)]\tLoss: 162.411102\n","Train Epoch: 6 [38400/60000 (64%)]\tLoss: 164.550461\n","Train Epoch: 6 [39680/60000 (66%)]\tLoss: 165.802887\n","Train Epoch: 6 [40960/60000 (68%)]\tLoss: 171.470169\n","Train Epoch: 6 [42240/60000 (70%)]\tLoss: 169.940643\n","Train Epoch: 6 [43520/60000 (72%)]\tLoss: 171.044434\n","Train Epoch: 6 [44800/60000 (75%)]\tLoss: 164.343979\n","Train Epoch: 6 [46080/60000 (77%)]\tLoss: 176.776428\n","Train Epoch: 6 [47360/60000 (79%)]\tLoss: 177.073730\n","Train Epoch: 6 [48640/60000 (81%)]\tLoss: 175.939682\n","Train Epoch: 6 [49920/60000 (83%)]\tLoss: 167.289688\n","Train Epoch: 6 [51200/60000 (85%)]\tLoss: 169.998917\n","Train Epoch: 6 [52480/60000 (87%)]\tLoss: 172.032486\n","Train Epoch: 6 [53760/60000 (90%)]\tLoss: 166.696579\n","Train Epoch: 6 [55040/60000 (92%)]\tLoss: 168.454544\n","Train Epoch: 6 [56320/60000 (94%)]\tLoss: 166.314453\n","Train Epoch: 6 [57600/60000 (96%)]\tLoss: 162.441574\n","Train Epoch: 6 [58880/60000 (98%)]\tLoss: 167.139359\n","====> Epoch: 6 Average loss: 172.8412\n","====> Test set loss: 169.5594\n","Train Epoch: 7 [0/60000 (0%)]\tLoss: 163.754776\n","Train Epoch: 7 [1280/60000 (2%)]\tLoss: 171.708069\n","Train Epoch: 7 [2560/60000 (4%)]\tLoss: 170.061371\n","Train Epoch: 7 [3840/60000 (6%)]\tLoss: 172.956863\n","Train Epoch: 7 [5120/60000 (9%)]\tLoss: 180.907028\n","Train Epoch: 7 [6400/60000 (11%)]\tLoss: 165.375534\n","Train Epoch: 7 [7680/60000 (13%)]\tLoss: 171.317841\n","Train Epoch: 7 [8960/60000 (15%)]\tLoss: 170.547409\n","Train Epoch: 7 [10240/60000 (17%)]\tLoss: 173.182251\n","Train Epoch: 7 [11520/60000 (19%)]\tLoss: 165.122635\n","Train Epoch: 7 [12800/60000 (21%)]\tLoss: 160.988754\n","Train Epoch: 7 [14080/60000 (23%)]\tLoss: 164.043671\n","Train Epoch: 7 [15360/60000 (26%)]\tLoss: 173.566666\n","Train Epoch: 7 [16640/60000 (28%)]\tLoss: 171.757370\n","Train Epoch: 7 [17920/60000 (30%)]\tLoss: 175.742477\n","Train Epoch: 7 [19200/60000 (32%)]\tLoss: 173.919907\n","Train Epoch: 7 [20480/60000 (34%)]\tLoss: 172.515549\n","Train Epoch: 7 [21760/60000 (36%)]\tLoss: 178.097565\n","Train Epoch: 7 [23040/60000 (38%)]\tLoss: 175.998566\n","Train Epoch: 7 [24320/60000 (41%)]\tLoss: 171.112366\n","Train Epoch: 7 [25600/60000 (43%)]\tLoss: 176.273361\n","Train Epoch: 7 [26880/60000 (45%)]\tLoss: 180.143494\n","Train Epoch: 7 [28160/60000 (47%)]\tLoss: 158.425552\n","Train Epoch: 7 [29440/60000 (49%)]\tLoss: 176.236267\n","Train Epoch: 7 [30720/60000 (51%)]\tLoss: 162.866638\n","Train Epoch: 7 [32000/60000 (53%)]\tLoss: 166.567413\n","Train Epoch: 7 [33280/60000 (55%)]\tLoss: 173.017487\n","Train Epoch: 7 [34560/60000 (58%)]\tLoss: 168.544632\n","Train Epoch: 7 [35840/60000 (60%)]\tLoss: 175.674255\n","Train Epoch: 7 [37120/60000 (62%)]\tLoss: 164.927673\n","Train Epoch: 7 [38400/60000 (64%)]\tLoss: 175.875565\n","Train Epoch: 7 [39680/60000 (66%)]\tLoss: 175.915329\n","Train Epoch: 7 [40960/60000 (68%)]\tLoss: 170.013626\n","Train Epoch: 7 [42240/60000 (70%)]\tLoss: 171.140701\n","Train Epoch: 7 [43520/60000 (72%)]\tLoss: 170.594711\n","Train Epoch: 7 [44800/60000 (75%)]\tLoss: 168.200256\n","Train Epoch: 7 [46080/60000 (77%)]\tLoss: 166.260834\n","Train Epoch: 7 [47360/60000 (79%)]\tLoss: 162.511108\n","Train Epoch: 7 [48640/60000 (81%)]\tLoss: 163.058228\n","Train Epoch: 7 [49920/60000 (83%)]\tLoss: 166.876785\n","Train Epoch: 7 [51200/60000 (85%)]\tLoss: 169.362503\n","Train Epoch: 7 [52480/60000 (87%)]\tLoss: 171.280792\n","Train Epoch: 7 [53760/60000 (90%)]\tLoss: 165.980423\n","Train Epoch: 7 [55040/60000 (92%)]\tLoss: 164.921799\n","Train Epoch: 7 [56320/60000 (94%)]\tLoss: 177.333557\n","Train Epoch: 7 [57600/60000 (96%)]\tLoss: 159.914642\n","Train Epoch: 7 [58880/60000 (98%)]\tLoss: 168.242645\n","====> Epoch: 7 Average loss: 170.0458\n","====> Test set loss: 167.9785\n","Train Epoch: 8 [0/60000 (0%)]\tLoss: 162.930008\n","Train Epoch: 8 [1280/60000 (2%)]\tLoss: 170.588882\n","Train Epoch: 8 [2560/60000 (4%)]\tLoss: 169.063644\n","Train Epoch: 8 [3840/60000 (6%)]\tLoss: 163.686584\n","Train Epoch: 8 [5120/60000 (9%)]\tLoss: 179.244324\n","Train Epoch: 8 [6400/60000 (11%)]\tLoss: 168.781555\n","Train Epoch: 8 [7680/60000 (13%)]\tLoss: 164.849182\n","Train Epoch: 8 [8960/60000 (15%)]\tLoss: 165.660141\n","Train Epoch: 8 [10240/60000 (17%)]\tLoss: 165.625000\n","Train Epoch: 8 [11520/60000 (19%)]\tLoss: 162.819550\n","Train Epoch: 8 [12800/60000 (21%)]\tLoss: 170.267944\n","Train Epoch: 8 [14080/60000 (23%)]\tLoss: 171.007202\n","Train Epoch: 8 [15360/60000 (26%)]\tLoss: 168.452957\n","Train Epoch: 8 [16640/60000 (28%)]\tLoss: 159.364822\n","Train Epoch: 8 [17920/60000 (30%)]\tLoss: 156.104355\n","Train Epoch: 8 [19200/60000 (32%)]\tLoss: 164.721863\n","Train Epoch: 8 [20480/60000 (34%)]\tLoss: 164.265106\n","Train Epoch: 8 [21760/60000 (36%)]\tLoss: 162.926697\n","Train Epoch: 8 [23040/60000 (38%)]\tLoss: 160.090012\n","Train Epoch: 8 [24320/60000 (41%)]\tLoss: 159.119629\n","Train Epoch: 8 [25600/60000 (43%)]\tLoss: 151.056229\n","Train Epoch: 8 [26880/60000 (45%)]\tLoss: 157.561646\n","Train Epoch: 8 [28160/60000 (47%)]\tLoss: 152.034882\n","Train Epoch: 8 [29440/60000 (49%)]\tLoss: 167.467636\n","Train Epoch: 8 [30720/60000 (51%)]\tLoss: 155.655228\n","Train Epoch: 8 [32000/60000 (53%)]\tLoss: 157.161362\n","Train Epoch: 8 [33280/60000 (55%)]\tLoss: 159.097229\n","Train Epoch: 8 [34560/60000 (58%)]\tLoss: 157.455933\n","Train Epoch: 8 [35840/60000 (60%)]\tLoss: 162.842758\n","Train Epoch: 8 [37120/60000 (62%)]\tLoss: 150.997238\n","Train Epoch: 8 [38400/60000 (64%)]\tLoss: 158.380463\n","Train Epoch: 8 [39680/60000 (66%)]\tLoss: 161.793304\n","Train Epoch: 8 [40960/60000 (68%)]\tLoss: 160.561951\n","Train Epoch: 8 [42240/60000 (70%)]\tLoss: 164.603821\n","Train Epoch: 8 [43520/60000 (72%)]\tLoss: 160.836166\n","Train Epoch: 8 [44800/60000 (75%)]\tLoss: 162.846878\n","Train Epoch: 8 [46080/60000 (77%)]\tLoss: 164.624603\n","Train Epoch: 8 [47360/60000 (79%)]\tLoss: 153.014725\n","Train Epoch: 8 [48640/60000 (81%)]\tLoss: 167.828400\n","Train Epoch: 8 [49920/60000 (83%)]\tLoss: 159.184021\n","Train Epoch: 8 [51200/60000 (85%)]\tLoss: 154.653748\n","Train Epoch: 8 [52480/60000 (87%)]\tLoss: 156.426529\n","Train Epoch: 8 [53760/60000 (90%)]\tLoss: 165.674393\n","Train Epoch: 8 [55040/60000 (92%)]\tLoss: 159.193558\n","Train Epoch: 8 [56320/60000 (94%)]\tLoss: 159.522415\n","Train Epoch: 8 [57600/60000 (96%)]\tLoss: 163.949265\n","Train Epoch: 8 [58880/60000 (98%)]\tLoss: 160.818802\n","====> Epoch: 8 Average loss: 161.3092\n","====> Test set loss: 159.8972\n","Train Epoch: 9 [0/60000 (0%)]\tLoss: 155.211975\n","Train Epoch: 9 [1280/60000 (2%)]\tLoss: 165.804962\n","Train Epoch: 9 [2560/60000 (4%)]\tLoss: 155.335602\n","Train Epoch: 9 [3840/60000 (6%)]\tLoss: 160.461884\n","Train Epoch: 9 [5120/60000 (9%)]\tLoss: 157.864441\n","Train Epoch: 9 [6400/60000 (11%)]\tLoss: 154.286697\n","Train Epoch: 9 [7680/60000 (13%)]\tLoss: 158.513382\n","Train Epoch: 9 [8960/60000 (15%)]\tLoss: 154.247070\n","Train Epoch: 9 [10240/60000 (17%)]\tLoss: 160.278320\n","Train Epoch: 9 [11520/60000 (19%)]\tLoss: 161.830551\n","Train Epoch: 9 [12800/60000 (21%)]\tLoss: 156.734375\n","Train Epoch: 9 [14080/60000 (23%)]\tLoss: 152.115906\n","Train Epoch: 9 [15360/60000 (26%)]\tLoss: 164.362625\n","Train Epoch: 9 [16640/60000 (28%)]\tLoss: 152.438919\n","Train Epoch: 9 [17920/60000 (30%)]\tLoss: 152.520706\n","Train Epoch: 9 [19200/60000 (32%)]\tLoss: 154.947067\n","Train Epoch: 9 [20480/60000 (34%)]\tLoss: 153.293732\n","Train Epoch: 9 [21760/60000 (36%)]\tLoss: 148.641647\n","Train Epoch: 9 [23040/60000 (38%)]\tLoss: 164.392410\n","Train Epoch: 9 [24320/60000 (41%)]\tLoss: 156.598602\n","Train Epoch: 9 [25600/60000 (43%)]\tLoss: 163.815826\n","Train Epoch: 9 [26880/60000 (45%)]\tLoss: 157.619949\n","Train Epoch: 9 [28160/60000 (47%)]\tLoss: 157.333038\n","Train Epoch: 9 [29440/60000 (49%)]\tLoss: 152.087631\n","Train Epoch: 9 [30720/60000 (51%)]\tLoss: 158.239441\n","Train Epoch: 9 [32000/60000 (53%)]\tLoss: 148.865967\n","Train Epoch: 9 [33280/60000 (55%)]\tLoss: 150.324387\n","Train Epoch: 9 [34560/60000 (58%)]\tLoss: 149.965088\n","Train Epoch: 9 [35840/60000 (60%)]\tLoss: 154.150665\n","Train Epoch: 9 [37120/60000 (62%)]\tLoss: 156.334930\n","Train Epoch: 9 [38400/60000 (64%)]\tLoss: 156.387268\n","Train Epoch: 9 [39680/60000 (66%)]\tLoss: 153.319717\n","Train Epoch: 9 [40960/60000 (68%)]\tLoss: 157.270172\n","Train Epoch: 9 [42240/60000 (70%)]\tLoss: 156.587463\n","Train Epoch: 9 [43520/60000 (72%)]\tLoss: 162.731705\n","Train Epoch: 9 [44800/60000 (75%)]\tLoss: 152.126740\n","Train Epoch: 9 [46080/60000 (77%)]\tLoss: 153.572113\n","Train Epoch: 9 [47360/60000 (79%)]\tLoss: 159.713211\n","Train Epoch: 9 [48640/60000 (81%)]\tLoss: 159.525040\n","Train Epoch: 9 [49920/60000 (83%)]\tLoss: 152.827576\n","Train Epoch: 9 [51200/60000 (85%)]\tLoss: 160.561783\n","Train Epoch: 9 [52480/60000 (87%)]\tLoss: 154.052216\n","Train Epoch: 9 [53760/60000 (90%)]\tLoss: 150.965881\n","Train Epoch: 9 [55040/60000 (92%)]\tLoss: 155.568634\n","Train Epoch: 9 [56320/60000 (94%)]\tLoss: 152.904739\n","Train Epoch: 9 [57600/60000 (96%)]\tLoss: 150.148361\n","Train Epoch: 9 [58880/60000 (98%)]\tLoss: 148.077744\n","====> Epoch: 9 Average loss: 156.3816\n","====> Test set loss: 153.5668\n","Train Epoch: 10 [0/60000 (0%)]\tLoss: 149.170761\n","Train Epoch: 10 [1280/60000 (2%)]\tLoss: 146.634003\n","Train Epoch: 10 [2560/60000 (4%)]\tLoss: 153.208145\n","Train Epoch: 10 [3840/60000 (6%)]\tLoss: 154.082489\n","Train Epoch: 10 [5120/60000 (9%)]\tLoss: 161.205109\n","Train Epoch: 10 [6400/60000 (11%)]\tLoss: 149.166306\n","Train Epoch: 10 [7680/60000 (13%)]\tLoss: 154.386581\n","Train Epoch: 10 [8960/60000 (15%)]\tLoss: 156.144012\n","Train Epoch: 10 [10240/60000 (17%)]\tLoss: 154.104660\n","Train Epoch: 10 [11520/60000 (19%)]\tLoss: 155.364197\n","Train Epoch: 10 [12800/60000 (21%)]\tLoss: 150.575073\n","Train Epoch: 10 [14080/60000 (23%)]\tLoss: 145.732101\n","Train Epoch: 10 [15360/60000 (26%)]\tLoss: 149.773392\n","Train Epoch: 10 [16640/60000 (28%)]\tLoss: 153.146164\n","Train Epoch: 10 [17920/60000 (30%)]\tLoss: 151.787628\n","Train Epoch: 10 [19200/60000 (32%)]\tLoss: 153.185349\n","Train Epoch: 10 [20480/60000 (34%)]\tLoss: 154.997772\n","Train Epoch: 10 [21760/60000 (36%)]\tLoss: 157.754974\n","Train Epoch: 10 [23040/60000 (38%)]\tLoss: 152.479187\n","Train Epoch: 10 [24320/60000 (41%)]\tLoss: 151.581711\n","Train Epoch: 10 [25600/60000 (43%)]\tLoss: 153.519150\n","Train Epoch: 10 [26880/60000 (45%)]\tLoss: 162.483551\n","Train Epoch: 10 [28160/60000 (47%)]\tLoss: 161.458023\n","Train Epoch: 10 [29440/60000 (49%)]\tLoss: 155.315414\n","Train Epoch: 10 [30720/60000 (51%)]\tLoss: 161.272690\n","Train Epoch: 10 [32000/60000 (53%)]\tLoss: 159.577759\n","Train Epoch: 10 [33280/60000 (55%)]\tLoss: 148.478104\n","Train Epoch: 10 [34560/60000 (58%)]\tLoss: 147.056534\n","Train Epoch: 10 [35840/60000 (60%)]\tLoss: 158.723602\n","Train Epoch: 10 [37120/60000 (62%)]\tLoss: 155.395035\n","Train Epoch: 10 [38400/60000 (64%)]\tLoss: 157.578888\n","Train Epoch: 10 [39680/60000 (66%)]\tLoss: 149.909973\n","Train Epoch: 10 [40960/60000 (68%)]\tLoss: 154.063034\n","Train Epoch: 10 [42240/60000 (70%)]\tLoss: 144.611679\n","Train Epoch: 10 [43520/60000 (72%)]\tLoss: 148.345642\n","Train Epoch: 10 [44800/60000 (75%)]\tLoss: 157.135269\n","Train Epoch: 10 [46080/60000 (77%)]\tLoss: 157.037750\n","Train Epoch: 10 [47360/60000 (79%)]\tLoss: 156.846817\n","Train Epoch: 10 [48640/60000 (81%)]\tLoss: 152.171692\n","Train Epoch: 10 [49920/60000 (83%)]\tLoss: 149.640884\n","Train Epoch: 10 [51200/60000 (85%)]\tLoss: 153.612244\n","Train Epoch: 10 [52480/60000 (87%)]\tLoss: 156.945908\n","Train Epoch: 10 [53760/60000 (90%)]\tLoss: 154.807251\n","Train Epoch: 10 [55040/60000 (92%)]\tLoss: 149.489792\n","Train Epoch: 10 [56320/60000 (94%)]\tLoss: 150.534500\n","Train Epoch: 10 [57600/60000 (96%)]\tLoss: 150.374359\n","Train Epoch: 10 [58880/60000 (98%)]\tLoss: 143.769028\n","====> Epoch: 10 Average loss: 153.3624\n","====> Test set loss: 150.7030\n","Train Epoch: 11 [0/60000 (0%)]\tLoss: 156.858078\n","Train Epoch: 11 [1280/60000 (2%)]\tLoss: 145.429123\n","Train Epoch: 11 [2560/60000 (4%)]\tLoss: 150.613266\n","Train Epoch: 11 [3840/60000 (6%)]\tLoss: 147.123306\n","Train Epoch: 11 [5120/60000 (9%)]\tLoss: 157.873383\n","Train Epoch: 11 [6400/60000 (11%)]\tLoss: 150.690247\n","Train Epoch: 11 [7680/60000 (13%)]\tLoss: 151.318726\n","Train Epoch: 11 [8960/60000 (15%)]\tLoss: 148.950958\n","Train Epoch: 11 [10240/60000 (17%)]\tLoss: 148.472595\n","Train Epoch: 11 [11520/60000 (19%)]\tLoss: 153.888519\n","Train Epoch: 11 [12800/60000 (21%)]\tLoss: 152.430908\n","Train Epoch: 11 [14080/60000 (23%)]\tLoss: 151.555206\n","Train Epoch: 11 [15360/60000 (26%)]\tLoss: 147.681976\n","Train Epoch: 11 [16640/60000 (28%)]\tLoss: 157.497055\n","Train Epoch: 11 [17920/60000 (30%)]\tLoss: 155.642273\n","Train Epoch: 11 [19200/60000 (32%)]\tLoss: 151.439438\n","Train Epoch: 11 [20480/60000 (34%)]\tLoss: 148.503601\n","Train Epoch: 11 [21760/60000 (36%)]\tLoss: 157.491699\n","Train Epoch: 11 [23040/60000 (38%)]\tLoss: 150.094345\n","Train Epoch: 11 [24320/60000 (41%)]\tLoss: 150.285889\n","Train Epoch: 11 [25600/60000 (43%)]\tLoss: 153.505478\n","Train Epoch: 11 [26880/60000 (45%)]\tLoss: 155.027359\n","Train Epoch: 11 [28160/60000 (47%)]\tLoss: 159.285431\n","Train Epoch: 11 [29440/60000 (49%)]\tLoss: 158.610779\n","Train Epoch: 11 [30720/60000 (51%)]\tLoss: 163.518661\n","Train Epoch: 11 [32000/60000 (53%)]\tLoss: 150.621872\n","Train Epoch: 11 [33280/60000 (55%)]\tLoss: 154.441116\n","Train Epoch: 11 [34560/60000 (58%)]\tLoss: 160.858582\n","Train Epoch: 11 [35840/60000 (60%)]\tLoss: 152.232590\n","Train Epoch: 11 [37120/60000 (62%)]\tLoss: 152.009109\n","Train Epoch: 11 [38400/60000 (64%)]\tLoss: 157.171890\n","Train Epoch: 11 [39680/60000 (66%)]\tLoss: 144.060730\n","Train Epoch: 11 [40960/60000 (68%)]\tLoss: 157.518784\n","Train Epoch: 11 [42240/60000 (70%)]\tLoss: 146.940155\n","Train Epoch: 11 [43520/60000 (72%)]\tLoss: 157.804871\n","Train Epoch: 11 [44800/60000 (75%)]\tLoss: 154.558929\n","Train Epoch: 11 [46080/60000 (77%)]\tLoss: 151.618515\n","Train Epoch: 11 [47360/60000 (79%)]\tLoss: 154.860413\n","Train Epoch: 11 [48640/60000 (81%)]\tLoss: 151.882874\n","Train Epoch: 11 [49920/60000 (83%)]\tLoss: 153.390350\n","Train Epoch: 11 [51200/60000 (85%)]\tLoss: 158.129822\n","Train Epoch: 11 [52480/60000 (87%)]\tLoss: 147.069672\n","Train Epoch: 11 [53760/60000 (90%)]\tLoss: 160.007477\n","Train Epoch: 11 [55040/60000 (92%)]\tLoss: 153.043289\n","Train Epoch: 11 [56320/60000 (94%)]\tLoss: 152.066345\n","Train Epoch: 11 [57600/60000 (96%)]\tLoss: 155.255524\n","Train Epoch: 11 [58880/60000 (98%)]\tLoss: 158.143219\n","====> Epoch: 11 Average loss: 153.4357\n","====> Test set loss: 154.0373\n","Train Epoch: 12 [0/60000 (0%)]\tLoss: 162.338577\n","Train Epoch: 12 [1280/60000 (2%)]\tLoss: 155.530121\n","Train Epoch: 12 [2560/60000 (4%)]\tLoss: 159.615509\n","Train Epoch: 12 [3840/60000 (6%)]\tLoss: 151.556625\n","Train Epoch: 12 [5120/60000 (9%)]\tLoss: 153.268555\n","Train Epoch: 12 [6400/60000 (11%)]\tLoss: 153.672913\n","Train Epoch: 12 [7680/60000 (13%)]\tLoss: 145.739349\n","Train Epoch: 12 [8960/60000 (15%)]\tLoss: 145.679489\n","Train Epoch: 12 [10240/60000 (17%)]\tLoss: 156.110962\n","Train Epoch: 12 [11520/60000 (19%)]\tLoss: 146.697128\n","Train Epoch: 12 [12800/60000 (21%)]\tLoss: 148.523315\n","Train Epoch: 12 [14080/60000 (23%)]\tLoss: 152.748825\n","Train Epoch: 12 [15360/60000 (26%)]\tLoss: 153.559204\n","Train Epoch: 12 [16640/60000 (28%)]\tLoss: 150.187775\n","Train Epoch: 12 [17920/60000 (30%)]\tLoss: 154.868591\n","Train Epoch: 12 [19200/60000 (32%)]\tLoss: 149.664490\n","Train Epoch: 12 [20480/60000 (34%)]\tLoss: 150.844131\n","Train Epoch: 12 [21760/60000 (36%)]\tLoss: 154.165649\n","Train Epoch: 12 [23040/60000 (38%)]\tLoss: 153.592056\n","Train Epoch: 12 [24320/60000 (41%)]\tLoss: 151.394867\n","Train Epoch: 12 [25600/60000 (43%)]\tLoss: 148.927139\n","Train Epoch: 12 [26880/60000 (45%)]\tLoss: 147.727203\n","Train Epoch: 12 [28160/60000 (47%)]\tLoss: 144.671722\n","Train Epoch: 12 [29440/60000 (49%)]\tLoss: 151.102524\n","Train Epoch: 12 [30720/60000 (51%)]\tLoss: 148.270035\n","Train Epoch: 12 [32000/60000 (53%)]\tLoss: 149.495163\n","Train Epoch: 12 [33280/60000 (55%)]\tLoss: 151.195923\n","Train Epoch: 12 [34560/60000 (58%)]\tLoss: 144.992981\n","Train Epoch: 12 [35840/60000 (60%)]\tLoss: 146.790817\n","Train Epoch: 12 [37120/60000 (62%)]\tLoss: 149.325394\n","Train Epoch: 12 [38400/60000 (64%)]\tLoss: 154.536301\n","Train Epoch: 12 [39680/60000 (66%)]\tLoss: 152.965805\n","Train Epoch: 12 [40960/60000 (68%)]\tLoss: 156.094330\n","Train Epoch: 12 [42240/60000 (70%)]\tLoss: 151.770691\n","Train Epoch: 12 [43520/60000 (72%)]\tLoss: 154.192520\n","Train Epoch: 12 [44800/60000 (75%)]\tLoss: 151.532593\n","Train Epoch: 12 [46080/60000 (77%)]\tLoss: 146.986099\n","Train Epoch: 12 [47360/60000 (79%)]\tLoss: 154.679367\n","Train Epoch: 12 [48640/60000 (81%)]\tLoss: 153.286118\n","Train Epoch: 12 [49920/60000 (83%)]\tLoss: 152.470001\n","Train Epoch: 12 [51200/60000 (85%)]\tLoss: 149.925049\n","Train Epoch: 12 [52480/60000 (87%)]\tLoss: 159.857010\n","Train Epoch: 12 [53760/60000 (90%)]\tLoss: 148.550278\n","Train Epoch: 12 [55040/60000 (92%)]\tLoss: 155.056046\n","Train Epoch: 12 [56320/60000 (94%)]\tLoss: 151.574844\n","Train Epoch: 12 [57600/60000 (96%)]\tLoss: 151.397995\n","Train Epoch: 12 [58880/60000 (98%)]\tLoss: 152.957108\n","====> Epoch: 12 Average loss: 151.4405\n","====> Test set loss: 154.0928\n","Train Epoch: 13 [0/60000 (0%)]\tLoss: 150.372284\n","Train Epoch: 13 [1280/60000 (2%)]\tLoss: 152.151505\n","Train Epoch: 13 [2560/60000 (4%)]\tLoss: 159.609131\n","Train Epoch: 13 [3840/60000 (6%)]\tLoss: 157.669220\n","Train Epoch: 13 [5120/60000 (9%)]\tLoss: 148.348251\n","Train Epoch: 13 [6400/60000 (11%)]\tLoss: 149.731689\n","Train Epoch: 13 [7680/60000 (13%)]\tLoss: 153.844421\n","Train Epoch: 13 [8960/60000 (15%)]\tLoss: 156.748337\n","Train Epoch: 13 [10240/60000 (17%)]\tLoss: 154.349976\n","Train Epoch: 13 [11520/60000 (19%)]\tLoss: 152.959595\n","Train Epoch: 13 [12800/60000 (21%)]\tLoss: 150.610367\n","Train Epoch: 13 [14080/60000 (23%)]\tLoss: 152.279434\n","Train Epoch: 13 [15360/60000 (26%)]\tLoss: 149.687012\n","Train Epoch: 13 [16640/60000 (28%)]\tLoss: 153.821487\n","Train Epoch: 13 [17920/60000 (30%)]\tLoss: 148.202408\n","Train Epoch: 13 [19200/60000 (32%)]\tLoss: 152.893539\n","Train Epoch: 13 [20480/60000 (34%)]\tLoss: 154.888870\n","Train Epoch: 13 [21760/60000 (36%)]\tLoss: 154.899231\n","Train Epoch: 13 [23040/60000 (38%)]\tLoss: 152.023605\n","Train Epoch: 13 [24320/60000 (41%)]\tLoss: 152.634979\n","Train Epoch: 13 [25600/60000 (43%)]\tLoss: 157.313843\n","Train Epoch: 13 [26880/60000 (45%)]\tLoss: 150.107971\n","Train Epoch: 13 [28160/60000 (47%)]\tLoss: 144.511948\n","Train Epoch: 13 [29440/60000 (49%)]\tLoss: 151.647156\n","Train Epoch: 13 [30720/60000 (51%)]\tLoss: 157.832306\n","Train Epoch: 13 [32000/60000 (53%)]\tLoss: 154.382248\n","Train Epoch: 13 [33280/60000 (55%)]\tLoss: 152.164780\n","Train Epoch: 13 [34560/60000 (58%)]\tLoss: 152.757141\n","Train Epoch: 13 [35840/60000 (60%)]\tLoss: 147.174225\n","Train Epoch: 13 [37120/60000 (62%)]\tLoss: 149.348389\n","Train Epoch: 13 [38400/60000 (64%)]\tLoss: 146.823593\n","Train Epoch: 13 [39680/60000 (66%)]\tLoss: 149.890762\n","Train Epoch: 13 [40960/60000 (68%)]\tLoss: 150.156021\n","Train Epoch: 13 [42240/60000 (70%)]\tLoss: 156.153320\n","Train Epoch: 13 [43520/60000 (72%)]\tLoss: 147.539047\n","Train Epoch: 13 [44800/60000 (75%)]\tLoss: 153.679688\n","Train Epoch: 13 [46080/60000 (77%)]\tLoss: 154.401016\n","Train Epoch: 13 [47360/60000 (79%)]\tLoss: 155.073883\n","Train Epoch: 13 [48640/60000 (81%)]\tLoss: 147.222153\n","Train Epoch: 13 [49920/60000 (83%)]\tLoss: 155.905746\n","Train Epoch: 13 [51200/60000 (85%)]\tLoss: 155.268799\n","Train Epoch: 13 [52480/60000 (87%)]\tLoss: 150.926147\n","Train Epoch: 13 [53760/60000 (90%)]\tLoss: 138.529739\n","Train Epoch: 13 [55040/60000 (92%)]\tLoss: 143.158554\n","Train Epoch: 13 [56320/60000 (94%)]\tLoss: 153.445709\n","Train Epoch: 13 [57600/60000 (96%)]\tLoss: 148.876953\n","Train Epoch: 13 [58880/60000 (98%)]\tLoss: 147.520081\n","====> Epoch: 13 Average loss: 151.8543\n","====> Test set loss: 150.3720\n","Train Epoch: 14 [0/60000 (0%)]\tLoss: 144.027115\n","Train Epoch: 14 [1280/60000 (2%)]\tLoss: 149.035492\n","Train Epoch: 14 [2560/60000 (4%)]\tLoss: 146.856110\n","Train Epoch: 14 [3840/60000 (6%)]\tLoss: 152.794556\n","Train Epoch: 14 [5120/60000 (9%)]\tLoss: 155.047119\n","Train Epoch: 14 [6400/60000 (11%)]\tLoss: 158.099014\n","Train Epoch: 14 [7680/60000 (13%)]\tLoss: 142.351273\n","Train Epoch: 14 [8960/60000 (15%)]\tLoss: 153.621704\n","Train Epoch: 14 [10240/60000 (17%)]\tLoss: 145.257339\n","Train Epoch: 14 [11520/60000 (19%)]\tLoss: 145.326767\n","Train Epoch: 14 [12800/60000 (21%)]\tLoss: 146.164673\n","Train Epoch: 14 [14080/60000 (23%)]\tLoss: 151.230835\n","Train Epoch: 14 [15360/60000 (26%)]\tLoss: 151.741562\n","Train Epoch: 14 [16640/60000 (28%)]\tLoss: 149.714249\n","Train Epoch: 14 [17920/60000 (30%)]\tLoss: 139.819931\n","Train Epoch: 14 [19200/60000 (32%)]\tLoss: 143.418152\n","Train Epoch: 14 [20480/60000 (34%)]\tLoss: 147.188522\n","Train Epoch: 14 [21760/60000 (36%)]\tLoss: 153.503448\n","Train Epoch: 14 [23040/60000 (38%)]\tLoss: 149.961624\n","Train Epoch: 14 [24320/60000 (41%)]\tLoss: 149.391449\n","Train Epoch: 14 [25600/60000 (43%)]\tLoss: 153.846878\n","Train Epoch: 14 [26880/60000 (45%)]\tLoss: 142.124786\n","Train Epoch: 14 [28160/60000 (47%)]\tLoss: 148.200775\n","Train Epoch: 14 [29440/60000 (49%)]\tLoss: 149.779007\n","Train Epoch: 14 [30720/60000 (51%)]\tLoss: 148.219162\n","Train Epoch: 14 [32000/60000 (53%)]\tLoss: 145.235443\n","Train Epoch: 14 [33280/60000 (55%)]\tLoss: 149.047668\n","Train Epoch: 14 [34560/60000 (58%)]\tLoss: 140.438019\n","Train Epoch: 14 [35840/60000 (60%)]\tLoss: 148.197021\n","Train Epoch: 14 [37120/60000 (62%)]\tLoss: 153.601669\n","Train Epoch: 14 [38400/60000 (64%)]\tLoss: 149.411072\n","Train Epoch: 14 [39680/60000 (66%)]\tLoss: 151.450470\n","Train Epoch: 14 [40960/60000 (68%)]\tLoss: 144.819901\n","Train Epoch: 14 [42240/60000 (70%)]\tLoss: 155.300903\n","Train Epoch: 14 [43520/60000 (72%)]\tLoss: 139.904053\n","Train Epoch: 14 [44800/60000 (75%)]\tLoss: 144.077026\n","Train Epoch: 14 [46080/60000 (77%)]\tLoss: 144.293976\n","Train Epoch: 14 [47360/60000 (79%)]\tLoss: 141.650894\n","Train Epoch: 14 [48640/60000 (81%)]\tLoss: 144.358353\n","Train Epoch: 14 [49920/60000 (83%)]\tLoss: 135.658203\n","Train Epoch: 14 [51200/60000 (85%)]\tLoss: 146.120636\n","Train Epoch: 14 [52480/60000 (87%)]\tLoss: 145.009888\n","Train Epoch: 14 [53760/60000 (90%)]\tLoss: 143.845947\n","Train Epoch: 14 [55040/60000 (92%)]\tLoss: 144.669571\n","Train Epoch: 14 [56320/60000 (94%)]\tLoss: 143.737732\n","Train Epoch: 14 [57600/60000 (96%)]\tLoss: 143.974655\n","Train Epoch: 14 [58880/60000 (98%)]\tLoss: 142.931076\n","====> Epoch: 14 Average loss: 147.8254\n","====> Test set loss: 145.7539\n","Train Epoch: 15 [0/60000 (0%)]\tLoss: 149.025879\n","Train Epoch: 15 [1280/60000 (2%)]\tLoss: 146.957108\n","Train Epoch: 15 [2560/60000 (4%)]\tLoss: 137.211075\n","Train Epoch: 15 [3840/60000 (6%)]\tLoss: 145.733200\n","Train Epoch: 15 [5120/60000 (9%)]\tLoss: 149.443268\n","Train Epoch: 15 [6400/60000 (11%)]\tLoss: 153.146149\n","Train Epoch: 15 [7680/60000 (13%)]\tLoss: 149.348572\n","Train Epoch: 15 [8960/60000 (15%)]\tLoss: 152.619278\n","Train Epoch: 15 [10240/60000 (17%)]\tLoss: 146.819489\n","Train Epoch: 15 [11520/60000 (19%)]\tLoss: 144.968018\n","Train Epoch: 15 [12800/60000 (21%)]\tLoss: 150.813095\n","Train Epoch: 15 [14080/60000 (23%)]\tLoss: 156.137314\n","Train Epoch: 15 [15360/60000 (26%)]\tLoss: 147.833405\n","Train Epoch: 15 [16640/60000 (28%)]\tLoss: 148.741821\n","Train Epoch: 15 [17920/60000 (30%)]\tLoss: 147.093994\n","Train Epoch: 15 [19200/60000 (32%)]\tLoss: 144.992798\n","Train Epoch: 15 [20480/60000 (34%)]\tLoss: 141.616531\n","Train Epoch: 15 [21760/60000 (36%)]\tLoss: 146.522095\n","Train Epoch: 15 [23040/60000 (38%)]\tLoss: 149.219177\n","Train Epoch: 15 [24320/60000 (41%)]\tLoss: 147.599365\n","Train Epoch: 15 [25600/60000 (43%)]\tLoss: 152.983185\n","Train Epoch: 15 [26880/60000 (45%)]\tLoss: 137.663177\n","Train Epoch: 15 [28160/60000 (47%)]\tLoss: 147.707428\n","Train Epoch: 15 [29440/60000 (49%)]\tLoss: 151.531754\n","Train Epoch: 15 [30720/60000 (51%)]\tLoss: 146.057816\n","Train Epoch: 15 [32000/60000 (53%)]\tLoss: 152.395966\n","Train Epoch: 15 [33280/60000 (55%)]\tLoss: 146.827957\n","Train Epoch: 15 [34560/60000 (58%)]\tLoss: 149.393158\n","Train Epoch: 15 [35840/60000 (60%)]\tLoss: 152.821793\n","Train Epoch: 15 [37120/60000 (62%)]\tLoss: 148.957031\n","Train Epoch: 15 [38400/60000 (64%)]\tLoss: 153.548706\n","Train Epoch: 15 [39680/60000 (66%)]\tLoss: 142.026779\n","Train Epoch: 15 [40960/60000 (68%)]\tLoss: 146.991562\n","Train Epoch: 15 [42240/60000 (70%)]\tLoss: 152.667557\n","Train Epoch: 15 [43520/60000 (72%)]\tLoss: 142.684998\n","Train Epoch: 15 [44800/60000 (75%)]\tLoss: 147.015869\n","Train Epoch: 15 [46080/60000 (77%)]\tLoss: 148.585434\n","Train Epoch: 15 [47360/60000 (79%)]\tLoss: 145.951141\n","Train Epoch: 15 [48640/60000 (81%)]\tLoss: 146.578979\n","Train Epoch: 15 [49920/60000 (83%)]\tLoss: 150.938995\n","Train Epoch: 15 [51200/60000 (85%)]\tLoss: 145.074356\n","Train Epoch: 15 [52480/60000 (87%)]\tLoss: 151.024292\n","Train Epoch: 15 [53760/60000 (90%)]\tLoss: 146.927551\n","Train Epoch: 15 [55040/60000 (92%)]\tLoss: 146.096527\n","Train Epoch: 15 [56320/60000 (94%)]\tLoss: 149.827148\n","Train Epoch: 15 [57600/60000 (96%)]\tLoss: 146.469009\n","Train Epoch: 15 [58880/60000 (98%)]\tLoss: 144.322266\n","====> Epoch: 15 Average loss: 146.0091\n","====> Test set loss: 146.8091\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"FontO_nxqatm"},"execution_count":null,"outputs":[]}]}