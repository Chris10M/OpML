{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"vae_gumbelSoftmax.ipynb","provenance":[],"authorship_tag":"ABX9TyOqEGILrNFMejOQZ7s/xfxh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cp-gJNVtZcx4","executionInfo":{"status":"ok","timestamp":1655468361025,"user_tz":-120,"elapsed":23618,"user":{"displayName":"Rajmohan Tumarada","userId":"18412595153668734700"}},"outputId":"da02fa47-7670-43fb-ad73-0a40e7b1e3e8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["%cd '/content/drive/MyDrive/Colab Notebooks/optMl'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vgXaBLG6Z1lv","executionInfo":{"status":"ok","timestamp":1655468376600,"user_tz":-120,"elapsed":279,"user":{"displayName":"Rajmohan Tumarada","userId":"18412595153668734700"}},"outputId":"5594b463-52f6-4244-e519-644470f8d58a"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/optMl\n"]}]},{"cell_type":"code","source":["# Code to implement VAE-gumple_softmax in pytorch\n","# author: Devinder Kumar (devinder.kumar@uwaterloo.ca), modified by Yongfei Yan\n","# The code has been modified from pytorch example vae code and inspired by the origianl \\\n","# tensorflow implementation of gumble-softmax by Eric Jang.\n","\n","import sys\n","import argparse\n","import numpy as np\n","\n","import torch\n","import torch.nn.functional as F\n","from torch import nn, optim\n","from torch.nn import functional as F\n","from torchvision import datasets, transforms\n","from torchvision.utils import save_image"],"metadata":{"id":"-naHZ21NZ1w2","executionInfo":{"status":"ok","timestamp":1655468410502,"user_tz":-120,"elapsed":2952,"user":{"displayName":"Rajmohan Tumarada","userId":"18412595153668734700"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["parser = argparse.ArgumentParser(description='VAE MNIST Example')\n","parser.add_argument('--batch-size', type=int, default=128, metavar='N',\n","                    help='input batch size for training (default: 100)')\n","parser.add_argument('--epochs', type=int, default=15, metavar='N',\n","                    help='number of epochs to train (default: 10)')\n","parser.add_argument('--temp', type=float, default=1.0, metavar='S',\n","                    help='tau(temperature) (default: 1.0)')\n","parser.add_argument('--no-cuda', action='store_true', default=False,\n","                    help='enables CUDA training')\n","parser.add_argument('--seed', type=int, default=1, metavar='S',\n","                    help='random seed (default: 1)')\n","parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n","                    help='how many batches to wait before logging training status')\n","parser.add_argument('--hard', action='store_true', default=False,\n","                    help='hard Gumbel softmax')\n","\n","sys.argv=['']\n","del sys\n","\n","args = parser.parse_args()\n","args.cuda = not args.no_cuda and torch.cuda.is_available()\n","torch.manual_seed(args.seed)\n","if args.cuda:\n","    torch.cuda.manual_seed(args.seed)\n","\n","kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}"],"metadata":{"id":"s873CPbTaw2_","executionInfo":{"status":"ok","timestamp":1655468413259,"user_tz":-120,"elapsed":285,"user":{"displayName":"Rajmohan Tumarada","userId":"18412595153668734700"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["train_loader = torch.utils.data.DataLoader(\n","    datasets.MNIST('./data/MNIST', train=True, download=True,\n","                   transform=transforms.ToTensor()),\n","    batch_size=args.batch_size, shuffle=True, **kwargs)\n","test_loader = torch.utils.data.DataLoader(\n","    datasets.MNIST('./data/MNIST', train=False, transform=transforms.ToTensor()),\n","    batch_size=args.batch_size, shuffle=True, **kwargs)"],"metadata":{"id":"Thu9KaWdaxBK","executionInfo":{"status":"ok","timestamp":1655468426488,"user_tz":-120,"elapsed":2146,"user":{"displayName":"Rajmohan Tumarada","userId":"18412595153668734700"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def sample_gumbel(shape, eps=1e-20):\n","    U = torch.rand(shape)\n","    if args.cuda:\n","        U = U.cuda()\n","    return -torch.log(-torch.log(U + eps) + eps)\n","\n","\n","def gumbel_softmax_sample(logits, temperature):\n","    y = logits + sample_gumbel(logits.size())\n","    return F.softmax(y / temperature, dim=-1)\n","\n","\n","def gumbel_softmax(logits, temperature, hard=False):\n","    \"\"\"\n","    ST-gumple-softmax\n","    input: [*, n_class]\n","    return: flatten --> [*, n_class] an one-hot vector\n","    \"\"\"\n","    y = gumbel_softmax_sample(logits, temperature)\n","    \n","    if not hard:\n","        return y.view(-1, latent_dim * categorical_dim)\n","\n","    shape = y.size()\n","    _, ind = y.max(dim=-1)\n","    y_hard = torch.zeros_like(y).view(-1, shape[-1])\n","    y_hard.scatter_(1, ind.view(-1, 1), 1)\n","    y_hard = y_hard.view(*shape)\n","    # Set gradients w.r.t. y_hard gradients w.r.t. y\n","    y_hard = (y_hard - y).detach() + y\n","    return y_hard.view(-1, latent_dim * categorical_dim)\n","\n","\n","class VAE_gumbel(nn.Module):\n","    def __init__(self, temp):\n","        super(VAE_gumbel, self).__init__()\n","\n","        self.fc1 = nn.Linear(784, 512)\n","        self.fc2 = nn.Linear(512, 256)\n","        self.fc3 = nn.Linear(256, latent_dim * categorical_dim)\n","\n","        self.fc4 = nn.Linear(latent_dim * categorical_dim, 256)\n","        self.fc5 = nn.Linear(256, 512)\n","        self.fc6 = nn.Linear(512, 784)\n","\n","        self.relu = nn.ReLU()\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def encode(self, x):\n","        h1 = self.relu(self.fc1(x))\n","        h2 = self.relu(self.fc2(h1))\n","        return self.relu(self.fc3(h2))\n","\n","    def decode(self, z):\n","        h4 = self.relu(self.fc4(z))\n","        h5 = self.relu(self.fc5(h4))\n","        return self.sigmoid(self.fc6(h5))\n","\n","    def forward(self, x, temp, hard):\n","        q = self.encode(x.view(-1, 784))\n","        q_y = q.view(q.size(0), latent_dim, categorical_dim)\n","        z = gumbel_softmax(q_y, temp, hard)\n","        return self.decode(z), F.softmax(q_y, dim=-1).reshape(*q.size())"],"metadata":{"id":"1F0E1-PZaxF_","executionInfo":{"status":"ok","timestamp":1655468446311,"user_tz":-120,"elapsed":262,"user":{"displayName":"Rajmohan Tumarada","userId":"18412595153668734700"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["latent_dim = 30\n","categorical_dim = 10  # one-of-K vector\n","\n","temp_min = 0.5\n","ANNEAL_RATE = 0.00003\n","\n","model = VAE_gumbel(args.temp)\n","if args.cuda:\n","    model.cuda()\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)"],"metadata":{"id":"YOkt6qw-axJD","executionInfo":{"status":"ok","timestamp":1655468461396,"user_tz":-120,"elapsed":11559,"user":{"displayName":"Rajmohan Tumarada","userId":"18412595153668734700"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Reconstruction + KL divergence losses summed over all elements and batch\n","def loss_function(recon_x, x, qy):\n","    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), size_average=False) / x.shape[0]\n","\n","    log_ratio = torch.log(qy * categorical_dim + 1e-20)\n","    KLD = torch.sum(qy * log_ratio, dim=-1).mean()\n","\n","    return BCE + KLD\n","\n","\n","def train(epoch):\n","    model.train()\n","    train_loss = 0\n","    temp = args.temp\n","    for batch_idx, (data, _) in enumerate(train_loader):\n","        if args.cuda:\n","            data = data.cuda()\n","        optimizer.zero_grad()\n","        recon_batch, qy = model(data, temp, args.hard)\n","        loss = loss_function(recon_batch, data, qy)\n","        loss.backward()\n","        train_loss += loss.item() * len(data)\n","        optimizer.step()\n","        if batch_idx % 100 == 1:\n","            temp = np.maximum(temp * np.exp(-ANNEAL_RATE * batch_idx), temp_min)\n","\n","        if batch_idx % args.log_interval == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                       100. * batch_idx / len(train_loader),\n","                       loss.item()))\n","\n","    print('====> Epoch: {} Average loss: {:.4f}'.format(\n","        epoch, train_loss / len(train_loader.dataset)))\n"],"metadata":{"id":"-DYnX7-WaxMH","executionInfo":{"status":"ok","timestamp":1655468474407,"user_tz":-120,"elapsed":354,"user":{"displayName":"Rajmohan Tumarada","userId":"18412595153668734700"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def test(epoch):\n","    model.eval()\n","    test_loss = 0\n","    temp = args.temp\n","    for i, (data, _) in enumerate(test_loader):\n","        if args.cuda:\n","            data = data.cuda()\n","        recon_batch, qy = model(data, temp, args.hard)\n","        test_loss += loss_function(recon_batch, data, qy).item() * len(data)\n","        if i % 100 == 1:\n","            temp = np.maximum(temp * np.exp(-ANNEAL_RATE * i), temp_min)\n","        if i == 0:\n","            n = min(data.size(0), 8)\n","            comparison = torch.cat([data[:n],\n","                                    recon_batch.view(args.batch_size, 1, 28, 28)[:n]])\n","            save_image(comparison.data.cpu(),\n","                       'data/reconstruction_' + str(epoch) + '.png', nrow=n)\n","\n","    test_loss /= len(test_loader.dataset)\n","    print('====> Test set loss: {:.4f}'.format(test_loss))"],"metadata":{"id":"FVSf1pdNdjE5","executionInfo":{"status":"ok","timestamp":1655468477146,"user_tz":-120,"elapsed":7,"user":{"displayName":"Rajmohan Tumarada","userId":"18412595153668734700"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def run():\n","    for epoch in range(1, args.epochs + 1):\n","        train(epoch)\n","        test(epoch)\n","\n","        M = 64 * latent_dim\n","        np_y = np.zeros((M, categorical_dim), dtype=np.float32)\n","        np_y[range(M), np.random.choice(categorical_dim, M)] = 1\n","        np_y = np.reshape(np_y, [M // latent_dim, latent_dim, categorical_dim])\n","        sample = torch.from_numpy(np_y).view(M // latent_dim, latent_dim * categorical_dim)\n","        if args.cuda:\n","            sample = sample.cuda()\n","        sample = model.decode(sample).cpu()\n","        save_image(sample.data.view(M // latent_dim, 1, 28, 28),\n","                   'data/sample_' + str(epoch) + '.png')\n","\n","\n","if __name__ == '__main__':\n","    run()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e_BLlJvjdl5X","executionInfo":{"status":"ok","timestamp":1655468618320,"user_tz":-120,"elapsed":138802,"user":{"displayName":"Rajmohan Tumarada","userId":"18412595153668734700"}},"outputId":"b13fe9fc-3dbc-4d46-bd51-976cae8ec994"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"output_type":"stream","name":"stdout","text":["Train Epoch: 1 [0/60000 (0%)]\tLoss: 542.559753\n","Train Epoch: 1 [1280/60000 (2%)]\tLoss: 281.165833\n","Train Epoch: 1 [2560/60000 (4%)]\tLoss: 226.424255\n","Train Epoch: 1 [3840/60000 (6%)]\tLoss: 207.667480\n","Train Epoch: 1 [5120/60000 (9%)]\tLoss: 206.207428\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 205.758667\n","Train Epoch: 1 [7680/60000 (13%)]\tLoss: 212.109299\n","Train Epoch: 1 [8960/60000 (15%)]\tLoss: 203.964706\n","Train Epoch: 1 [10240/60000 (17%)]\tLoss: 208.789856\n","Train Epoch: 1 [11520/60000 (19%)]\tLoss: 201.697739\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 199.356979\n","Train Epoch: 1 [14080/60000 (23%)]\tLoss: 202.948593\n","Train Epoch: 1 [15360/60000 (26%)]\tLoss: 197.505981\n","Train Epoch: 1 [16640/60000 (28%)]\tLoss: 198.627228\n","Train Epoch: 1 [17920/60000 (30%)]\tLoss: 208.866867\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 202.497330\n","Train Epoch: 1 [20480/60000 (34%)]\tLoss: 197.852875\n","Train Epoch: 1 [21760/60000 (36%)]\tLoss: 198.207977\n","Train Epoch: 1 [23040/60000 (38%)]\tLoss: 197.305695\n","Train Epoch: 1 [24320/60000 (41%)]\tLoss: 200.269531\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 196.117065\n","Train Epoch: 1 [26880/60000 (45%)]\tLoss: 190.136337\n","Train Epoch: 1 [28160/60000 (47%)]\tLoss: 190.779495\n","Train Epoch: 1 [29440/60000 (49%)]\tLoss: 192.418976\n","Train Epoch: 1 [30720/60000 (51%)]\tLoss: 192.466003\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 189.837036\n","Train Epoch: 1 [33280/60000 (55%)]\tLoss: 190.465149\n","Train Epoch: 1 [34560/60000 (58%)]\tLoss: 187.182800\n","Train Epoch: 1 [35840/60000 (60%)]\tLoss: 197.583191\n","Train Epoch: 1 [37120/60000 (62%)]\tLoss: 189.549423\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 189.279312\n","Train Epoch: 1 [39680/60000 (66%)]\tLoss: 200.879684\n","Train Epoch: 1 [40960/60000 (68%)]\tLoss: 190.300415\n","Train Epoch: 1 [42240/60000 (70%)]\tLoss: 186.649002\n","Train Epoch: 1 [43520/60000 (72%)]\tLoss: 186.829819\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 198.049576\n","Train Epoch: 1 [46080/60000 (77%)]\tLoss: 178.390213\n","Train Epoch: 1 [47360/60000 (79%)]\tLoss: 193.026077\n","Train Epoch: 1 [48640/60000 (81%)]\tLoss: 188.520126\n","Train Epoch: 1 [49920/60000 (83%)]\tLoss: 191.040054\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 189.898468\n","Train Epoch: 1 [52480/60000 (87%)]\tLoss: 185.917068\n","Train Epoch: 1 [53760/60000 (90%)]\tLoss: 193.937561\n","Train Epoch: 1 [55040/60000 (92%)]\tLoss: 178.719406\n","Train Epoch: 1 [56320/60000 (94%)]\tLoss: 182.521088\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 174.685791\n","Train Epoch: 1 [58880/60000 (98%)]\tLoss: 182.036957\n","====> Epoch: 1 Average loss: 201.8639\n","====> Test set loss: 180.8551\n","Train Epoch: 2 [0/60000 (0%)]\tLoss: 184.267120\n","Train Epoch: 2 [1280/60000 (2%)]\tLoss: 182.606339\n","Train Epoch: 2 [2560/60000 (4%)]\tLoss: 179.550446\n","Train Epoch: 2 [3840/60000 (6%)]\tLoss: 170.890320\n","Train Epoch: 2 [5120/60000 (9%)]\tLoss: 175.867325\n","Train Epoch: 2 [6400/60000 (11%)]\tLoss: 176.288406\n","Train Epoch: 2 [7680/60000 (13%)]\tLoss: 179.109482\n","Train Epoch: 2 [8960/60000 (15%)]\tLoss: 166.693161\n","Train Epoch: 2 [10240/60000 (17%)]\tLoss: 171.196579\n","Train Epoch: 2 [11520/60000 (19%)]\tLoss: 176.459930\n","Train Epoch: 2 [12800/60000 (21%)]\tLoss: 169.002426\n","Train Epoch: 2 [14080/60000 (23%)]\tLoss: 171.964386\n","Train Epoch: 2 [15360/60000 (26%)]\tLoss: 171.038773\n","Train Epoch: 2 [16640/60000 (28%)]\tLoss: 169.088104\n","Train Epoch: 2 [17920/60000 (30%)]\tLoss: 172.185577\n","Train Epoch: 2 [19200/60000 (32%)]\tLoss: 166.783173\n","Train Epoch: 2 [20480/60000 (34%)]\tLoss: 164.130356\n","Train Epoch: 2 [21760/60000 (36%)]\tLoss: 169.569275\n","Train Epoch: 2 [23040/60000 (38%)]\tLoss: 160.964447\n","Train Epoch: 2 [24320/60000 (41%)]\tLoss: 165.035446\n","Train Epoch: 2 [25600/60000 (43%)]\tLoss: 166.304337\n","Train Epoch: 2 [26880/60000 (45%)]\tLoss: 163.737564\n","Train Epoch: 2 [28160/60000 (47%)]\tLoss: 160.649902\n","Train Epoch: 2 [29440/60000 (49%)]\tLoss: 162.763840\n","Train Epoch: 2 [30720/60000 (51%)]\tLoss: 156.552048\n","Train Epoch: 2 [32000/60000 (53%)]\tLoss: 163.973145\n","Train Epoch: 2 [33280/60000 (55%)]\tLoss: 160.354980\n","Train Epoch: 2 [34560/60000 (58%)]\tLoss: 159.128510\n","Train Epoch: 2 [35840/60000 (60%)]\tLoss: 164.353073\n","Train Epoch: 2 [37120/60000 (62%)]\tLoss: 161.982590\n","Train Epoch: 2 [38400/60000 (64%)]\tLoss: 162.904724\n","Train Epoch: 2 [39680/60000 (66%)]\tLoss: 155.585449\n","Train Epoch: 2 [40960/60000 (68%)]\tLoss: 151.734039\n","Train Epoch: 2 [42240/60000 (70%)]\tLoss: 157.845993\n","Train Epoch: 2 [43520/60000 (72%)]\tLoss: 159.619156\n","Train Epoch: 2 [44800/60000 (75%)]\tLoss: 160.651062\n","Train Epoch: 2 [46080/60000 (77%)]\tLoss: 159.786209\n","Train Epoch: 2 [47360/60000 (79%)]\tLoss: 154.502472\n","Train Epoch: 2 [48640/60000 (81%)]\tLoss: 153.074936\n","Train Epoch: 2 [49920/60000 (83%)]\tLoss: 163.682663\n","Train Epoch: 2 [51200/60000 (85%)]\tLoss: 151.879425\n","Train Epoch: 2 [52480/60000 (87%)]\tLoss: 151.573669\n","Train Epoch: 2 [53760/60000 (90%)]\tLoss: 151.877716\n","Train Epoch: 2 [55040/60000 (92%)]\tLoss: 142.835526\n","Train Epoch: 2 [56320/60000 (94%)]\tLoss: 142.387436\n","Train Epoch: 2 [57600/60000 (96%)]\tLoss: 144.711517\n","Train Epoch: 2 [58880/60000 (98%)]\tLoss: 157.096725\n","====> Epoch: 2 Average loss: 163.3492\n","====> Test set loss: 150.0185\n","Train Epoch: 3 [0/60000 (0%)]\tLoss: 146.916595\n","Train Epoch: 3 [1280/60000 (2%)]\tLoss: 145.916412\n","Train Epoch: 3 [2560/60000 (4%)]\tLoss: 154.193619\n","Train Epoch: 3 [3840/60000 (6%)]\tLoss: 152.048340\n","Train Epoch: 3 [5120/60000 (9%)]\tLoss: 144.496384\n","Train Epoch: 3 [6400/60000 (11%)]\tLoss: 158.234283\n","Train Epoch: 3 [7680/60000 (13%)]\tLoss: 149.591202\n","Train Epoch: 3 [8960/60000 (15%)]\tLoss: 148.884796\n","Train Epoch: 3 [10240/60000 (17%)]\tLoss: 151.494324\n","Train Epoch: 3 [11520/60000 (19%)]\tLoss: 148.323288\n","Train Epoch: 3 [12800/60000 (21%)]\tLoss: 145.942154\n","Train Epoch: 3 [14080/60000 (23%)]\tLoss: 147.251450\n","Train Epoch: 3 [15360/60000 (26%)]\tLoss: 145.260757\n","Train Epoch: 3 [16640/60000 (28%)]\tLoss: 138.602875\n","Train Epoch: 3 [17920/60000 (30%)]\tLoss: 146.825226\n","Train Epoch: 3 [19200/60000 (32%)]\tLoss: 148.639404\n","Train Epoch: 3 [20480/60000 (34%)]\tLoss: 151.294250\n","Train Epoch: 3 [21760/60000 (36%)]\tLoss: 151.845413\n","Train Epoch: 3 [23040/60000 (38%)]\tLoss: 145.227814\n","Train Epoch: 3 [24320/60000 (41%)]\tLoss: 143.172882\n","Train Epoch: 3 [25600/60000 (43%)]\tLoss: 141.658875\n","Train Epoch: 3 [26880/60000 (45%)]\tLoss: 138.860825\n","Train Epoch: 3 [28160/60000 (47%)]\tLoss: 143.162201\n","Train Epoch: 3 [29440/60000 (49%)]\tLoss: 141.568069\n","Train Epoch: 3 [30720/60000 (51%)]\tLoss: 143.783386\n","Train Epoch: 3 [32000/60000 (53%)]\tLoss: 140.906967\n","Train Epoch: 3 [33280/60000 (55%)]\tLoss: 139.418854\n","Train Epoch: 3 [34560/60000 (58%)]\tLoss: 147.863052\n","Train Epoch: 3 [35840/60000 (60%)]\tLoss: 143.338699\n","Train Epoch: 3 [37120/60000 (62%)]\tLoss: 137.244400\n","Train Epoch: 3 [38400/60000 (64%)]\tLoss: 139.192032\n","Train Epoch: 3 [39680/60000 (66%)]\tLoss: 138.809189\n","Train Epoch: 3 [40960/60000 (68%)]\tLoss: 138.439087\n","Train Epoch: 3 [42240/60000 (70%)]\tLoss: 142.413940\n","Train Epoch: 3 [43520/60000 (72%)]\tLoss: 138.769424\n","Train Epoch: 3 [44800/60000 (75%)]\tLoss: 139.092865\n","Train Epoch: 3 [46080/60000 (77%)]\tLoss: 137.241806\n","Train Epoch: 3 [47360/60000 (79%)]\tLoss: 134.941986\n","Train Epoch: 3 [48640/60000 (81%)]\tLoss: 137.283737\n","Train Epoch: 3 [49920/60000 (83%)]\tLoss: 133.536240\n","Train Epoch: 3 [51200/60000 (85%)]\tLoss: 138.267548\n","Train Epoch: 3 [52480/60000 (87%)]\tLoss: 141.485687\n","Train Epoch: 3 [53760/60000 (90%)]\tLoss: 135.279022\n","Train Epoch: 3 [55040/60000 (92%)]\tLoss: 136.722214\n","Train Epoch: 3 [56320/60000 (94%)]\tLoss: 136.924149\n","Train Epoch: 3 [57600/60000 (96%)]\tLoss: 132.456390\n","Train Epoch: 3 [58880/60000 (98%)]\tLoss: 139.368439\n","====> Epoch: 3 Average loss: 142.0124\n","====> Test set loss: 134.8937\n","Train Epoch: 4 [0/60000 (0%)]\tLoss: 139.557480\n","Train Epoch: 4 [1280/60000 (2%)]\tLoss: 137.222687\n","Train Epoch: 4 [2560/60000 (4%)]\tLoss: 143.110687\n","Train Epoch: 4 [3840/60000 (6%)]\tLoss: 137.473999\n","Train Epoch: 4 [5120/60000 (9%)]\tLoss: 129.931015\n","Train Epoch: 4 [6400/60000 (11%)]\tLoss: 126.630974\n","Train Epoch: 4 [7680/60000 (13%)]\tLoss: 134.428635\n","Train Epoch: 4 [8960/60000 (15%)]\tLoss: 137.339523\n","Train Epoch: 4 [10240/60000 (17%)]\tLoss: 125.582672\n","Train Epoch: 4 [11520/60000 (19%)]\tLoss: 130.125793\n","Train Epoch: 4 [12800/60000 (21%)]\tLoss: 131.480942\n","Train Epoch: 4 [14080/60000 (23%)]\tLoss: 133.139816\n","Train Epoch: 4 [15360/60000 (26%)]\tLoss: 133.086868\n","Train Epoch: 4 [16640/60000 (28%)]\tLoss: 133.620316\n","Train Epoch: 4 [17920/60000 (30%)]\tLoss: 132.868896\n","Train Epoch: 4 [19200/60000 (32%)]\tLoss: 134.711472\n","Train Epoch: 4 [20480/60000 (34%)]\tLoss: 131.270813\n","Train Epoch: 4 [21760/60000 (36%)]\tLoss: 131.677780\n","Train Epoch: 4 [23040/60000 (38%)]\tLoss: 139.411758\n","Train Epoch: 4 [24320/60000 (41%)]\tLoss: 129.552277\n","Train Epoch: 4 [25600/60000 (43%)]\tLoss: 130.965576\n","Train Epoch: 4 [26880/60000 (45%)]\tLoss: 126.825615\n","Train Epoch: 4 [28160/60000 (47%)]\tLoss: 127.052818\n","Train Epoch: 4 [29440/60000 (49%)]\tLoss: 136.026474\n","Train Epoch: 4 [30720/60000 (51%)]\tLoss: 132.798706\n","Train Epoch: 4 [32000/60000 (53%)]\tLoss: 135.404602\n","Train Epoch: 4 [33280/60000 (55%)]\tLoss: 126.866974\n","Train Epoch: 4 [34560/60000 (58%)]\tLoss: 125.824623\n","Train Epoch: 4 [35840/60000 (60%)]\tLoss: 129.511703\n","Train Epoch: 4 [37120/60000 (62%)]\tLoss: 133.137787\n","Train Epoch: 4 [38400/60000 (64%)]\tLoss: 126.271729\n","Train Epoch: 4 [39680/60000 (66%)]\tLoss: 130.593216\n","Train Epoch: 4 [40960/60000 (68%)]\tLoss: 127.850670\n","Train Epoch: 4 [42240/60000 (70%)]\tLoss: 135.102402\n","Train Epoch: 4 [43520/60000 (72%)]\tLoss: 121.502441\n","Train Epoch: 4 [44800/60000 (75%)]\tLoss: 123.597435\n","Train Epoch: 4 [46080/60000 (77%)]\tLoss: 123.472046\n","Train Epoch: 4 [47360/60000 (79%)]\tLoss: 135.597366\n","Train Epoch: 4 [48640/60000 (81%)]\tLoss: 123.299683\n","Train Epoch: 4 [49920/60000 (83%)]\tLoss: 129.855026\n","Train Epoch: 4 [51200/60000 (85%)]\tLoss: 132.107224\n","Train Epoch: 4 [52480/60000 (87%)]\tLoss: 126.448891\n","Train Epoch: 4 [53760/60000 (90%)]\tLoss: 129.201324\n","Train Epoch: 4 [55040/60000 (92%)]\tLoss: 128.447754\n","Train Epoch: 4 [56320/60000 (94%)]\tLoss: 128.735214\n","Train Epoch: 4 [57600/60000 (96%)]\tLoss: 123.645569\n","Train Epoch: 4 [58880/60000 (98%)]\tLoss: 123.930298\n","====> Epoch: 4 Average loss: 130.9002\n","====> Test set loss: 125.9809\n","Train Epoch: 5 [0/60000 (0%)]\tLoss: 125.713158\n","Train Epoch: 5 [1280/60000 (2%)]\tLoss: 125.316910\n","Train Epoch: 5 [2560/60000 (4%)]\tLoss: 129.332184\n","Train Epoch: 5 [3840/60000 (6%)]\tLoss: 127.640915\n","Train Epoch: 5 [5120/60000 (9%)]\tLoss: 126.294548\n","Train Epoch: 5 [6400/60000 (11%)]\tLoss: 125.200768\n","Train Epoch: 5 [7680/60000 (13%)]\tLoss: 122.353752\n","Train Epoch: 5 [8960/60000 (15%)]\tLoss: 129.070358\n","Train Epoch: 5 [10240/60000 (17%)]\tLoss: 125.995277\n","Train Epoch: 5 [11520/60000 (19%)]\tLoss: 123.447464\n","Train Epoch: 5 [12800/60000 (21%)]\tLoss: 122.815735\n","Train Epoch: 5 [14080/60000 (23%)]\tLoss: 126.950684\n","Train Epoch: 5 [15360/60000 (26%)]\tLoss: 123.100548\n","Train Epoch: 5 [16640/60000 (28%)]\tLoss: 125.208519\n","Train Epoch: 5 [17920/60000 (30%)]\tLoss: 124.309906\n","Train Epoch: 5 [19200/60000 (32%)]\tLoss: 126.771072\n","Train Epoch: 5 [20480/60000 (34%)]\tLoss: 123.257790\n","Train Epoch: 5 [21760/60000 (36%)]\tLoss: 119.616859\n","Train Epoch: 5 [23040/60000 (38%)]\tLoss: 126.393051\n","Train Epoch: 5 [24320/60000 (41%)]\tLoss: 123.804024\n","Train Epoch: 5 [25600/60000 (43%)]\tLoss: 121.702179\n","Train Epoch: 5 [26880/60000 (45%)]\tLoss: 123.418945\n","Train Epoch: 5 [28160/60000 (47%)]\tLoss: 125.469803\n","Train Epoch: 5 [29440/60000 (49%)]\tLoss: 124.045944\n","Train Epoch: 5 [30720/60000 (51%)]\tLoss: 128.904755\n","Train Epoch: 5 [32000/60000 (53%)]\tLoss: 128.947052\n","Train Epoch: 5 [33280/60000 (55%)]\tLoss: 122.891632\n","Train Epoch: 5 [34560/60000 (58%)]\tLoss: 123.449707\n","Train Epoch: 5 [35840/60000 (60%)]\tLoss: 122.323402\n","Train Epoch: 5 [37120/60000 (62%)]\tLoss: 122.921776\n","Train Epoch: 5 [38400/60000 (64%)]\tLoss: 125.513138\n","Train Epoch: 5 [39680/60000 (66%)]\tLoss: 127.645332\n","Train Epoch: 5 [40960/60000 (68%)]\tLoss: 129.894485\n","Train Epoch: 5 [42240/60000 (70%)]\tLoss: 121.508774\n","Train Epoch: 5 [43520/60000 (72%)]\tLoss: 123.646347\n","Train Epoch: 5 [44800/60000 (75%)]\tLoss: 127.739037\n","Train Epoch: 5 [46080/60000 (77%)]\tLoss: 118.979156\n","Train Epoch: 5 [47360/60000 (79%)]\tLoss: 124.720276\n","Train Epoch: 5 [48640/60000 (81%)]\tLoss: 113.469360\n","Train Epoch: 5 [49920/60000 (83%)]\tLoss: 125.995270\n","Train Epoch: 5 [51200/60000 (85%)]\tLoss: 123.067909\n","Train Epoch: 5 [52480/60000 (87%)]\tLoss: 121.125061\n","Train Epoch: 5 [53760/60000 (90%)]\tLoss: 126.264061\n","Train Epoch: 5 [55040/60000 (92%)]\tLoss: 124.187035\n","Train Epoch: 5 [56320/60000 (94%)]\tLoss: 126.775375\n","Train Epoch: 5 [57600/60000 (96%)]\tLoss: 128.285141\n","Train Epoch: 5 [58880/60000 (98%)]\tLoss: 123.655945\n","====> Epoch: 5 Average loss: 124.1573\n","====> Test set loss: 120.9603\n","Train Epoch: 6 [0/60000 (0%)]\tLoss: 119.171753\n","Train Epoch: 6 [1280/60000 (2%)]\tLoss: 126.854813\n","Train Epoch: 6 [2560/60000 (4%)]\tLoss: 123.125198\n","Train Epoch: 6 [3840/60000 (6%)]\tLoss: 123.297234\n","Train Epoch: 6 [5120/60000 (9%)]\tLoss: 127.385330\n","Train Epoch: 6 [6400/60000 (11%)]\tLoss: 123.044991\n","Train Epoch: 6 [7680/60000 (13%)]\tLoss: 124.076988\n","Train Epoch: 6 [8960/60000 (15%)]\tLoss: 119.622040\n","Train Epoch: 6 [10240/60000 (17%)]\tLoss: 118.388275\n","Train Epoch: 6 [11520/60000 (19%)]\tLoss: 122.118210\n","Train Epoch: 6 [12800/60000 (21%)]\tLoss: 120.792496\n","Train Epoch: 6 [14080/60000 (23%)]\tLoss: 119.782684\n","Train Epoch: 6 [15360/60000 (26%)]\tLoss: 122.740845\n","Train Epoch: 6 [16640/60000 (28%)]\tLoss: 118.202164\n","Train Epoch: 6 [17920/60000 (30%)]\tLoss: 120.266685\n","Train Epoch: 6 [19200/60000 (32%)]\tLoss: 120.138908\n","Train Epoch: 6 [20480/60000 (34%)]\tLoss: 115.164528\n","Train Epoch: 6 [21760/60000 (36%)]\tLoss: 124.579308\n","Train Epoch: 6 [23040/60000 (38%)]\tLoss: 122.170952\n","Train Epoch: 6 [24320/60000 (41%)]\tLoss: 117.549805\n","Train Epoch: 6 [25600/60000 (43%)]\tLoss: 120.836533\n","Train Epoch: 6 [26880/60000 (45%)]\tLoss: 119.334854\n","Train Epoch: 6 [28160/60000 (47%)]\tLoss: 122.976135\n","Train Epoch: 6 [29440/60000 (49%)]\tLoss: 120.785469\n","Train Epoch: 6 [30720/60000 (51%)]\tLoss: 120.501244\n","Train Epoch: 6 [32000/60000 (53%)]\tLoss: 121.739403\n","Train Epoch: 6 [33280/60000 (55%)]\tLoss: 119.498787\n","Train Epoch: 6 [34560/60000 (58%)]\tLoss: 118.389000\n","Train Epoch: 6 [35840/60000 (60%)]\tLoss: 119.633743\n","Train Epoch: 6 [37120/60000 (62%)]\tLoss: 118.773315\n","Train Epoch: 6 [38400/60000 (64%)]\tLoss: 120.811256\n","Train Epoch: 6 [39680/60000 (66%)]\tLoss: 120.172295\n","Train Epoch: 6 [40960/60000 (68%)]\tLoss: 119.610275\n","Train Epoch: 6 [42240/60000 (70%)]\tLoss: 120.157875\n","Train Epoch: 6 [43520/60000 (72%)]\tLoss: 118.991432\n","Train Epoch: 6 [44800/60000 (75%)]\tLoss: 121.002251\n","Train Epoch: 6 [46080/60000 (77%)]\tLoss: 120.227051\n","Train Epoch: 6 [47360/60000 (79%)]\tLoss: 116.708633\n","Train Epoch: 6 [48640/60000 (81%)]\tLoss: 117.036499\n","Train Epoch: 6 [49920/60000 (83%)]\tLoss: 122.292061\n","Train Epoch: 6 [51200/60000 (85%)]\tLoss: 118.637497\n","Train Epoch: 6 [52480/60000 (87%)]\tLoss: 121.304749\n","Train Epoch: 6 [53760/60000 (90%)]\tLoss: 117.816711\n","Train Epoch: 6 [55040/60000 (92%)]\tLoss: 115.616936\n","Train Epoch: 6 [56320/60000 (94%)]\tLoss: 119.171341\n","Train Epoch: 6 [57600/60000 (96%)]\tLoss: 121.695175\n","Train Epoch: 6 [58880/60000 (98%)]\tLoss: 118.967216\n","====> Epoch: 6 Average loss: 120.1828\n","====> Test set loss: 117.8851\n","Train Epoch: 7 [0/60000 (0%)]\tLoss: 123.422661\n","Train Epoch: 7 [1280/60000 (2%)]\tLoss: 115.441437\n","Train Epoch: 7 [2560/60000 (4%)]\tLoss: 116.223236\n","Train Epoch: 7 [3840/60000 (6%)]\tLoss: 117.144295\n","Train Epoch: 7 [5120/60000 (9%)]\tLoss: 119.228943\n","Train Epoch: 7 [6400/60000 (11%)]\tLoss: 117.221603\n","Train Epoch: 7 [7680/60000 (13%)]\tLoss: 115.894257\n","Train Epoch: 7 [8960/60000 (15%)]\tLoss: 119.918839\n","Train Epoch: 7 [10240/60000 (17%)]\tLoss: 119.229073\n","Train Epoch: 7 [11520/60000 (19%)]\tLoss: 111.828087\n","Train Epoch: 7 [12800/60000 (21%)]\tLoss: 119.662300\n","Train Epoch: 7 [14080/60000 (23%)]\tLoss: 119.447800\n","Train Epoch: 7 [15360/60000 (26%)]\tLoss: 117.914719\n","Train Epoch: 7 [16640/60000 (28%)]\tLoss: 126.157158\n","Train Epoch: 7 [17920/60000 (30%)]\tLoss: 117.754234\n","Train Epoch: 7 [19200/60000 (32%)]\tLoss: 118.359604\n","Train Epoch: 7 [20480/60000 (34%)]\tLoss: 116.260696\n","Train Epoch: 7 [21760/60000 (36%)]\tLoss: 115.987114\n","Train Epoch: 7 [23040/60000 (38%)]\tLoss: 117.680061\n","Train Epoch: 7 [24320/60000 (41%)]\tLoss: 114.821548\n","Train Epoch: 7 [25600/60000 (43%)]\tLoss: 123.163048\n","Train Epoch: 7 [26880/60000 (45%)]\tLoss: 120.632736\n","Train Epoch: 7 [28160/60000 (47%)]\tLoss: 118.460228\n","Train Epoch: 7 [29440/60000 (49%)]\tLoss: 114.755005\n","Train Epoch: 7 [30720/60000 (51%)]\tLoss: 119.369240\n","Train Epoch: 7 [32000/60000 (53%)]\tLoss: 117.682205\n","Train Epoch: 7 [33280/60000 (55%)]\tLoss: 121.418968\n","Train Epoch: 7 [34560/60000 (58%)]\tLoss: 115.793297\n","Train Epoch: 7 [35840/60000 (60%)]\tLoss: 116.494980\n","Train Epoch: 7 [37120/60000 (62%)]\tLoss: 122.237717\n","Train Epoch: 7 [38400/60000 (64%)]\tLoss: 118.152115\n","Train Epoch: 7 [39680/60000 (66%)]\tLoss: 113.250694\n","Train Epoch: 7 [40960/60000 (68%)]\tLoss: 119.631966\n","Train Epoch: 7 [42240/60000 (70%)]\tLoss: 111.163269\n","Train Epoch: 7 [43520/60000 (72%)]\tLoss: 110.582748\n","Train Epoch: 7 [44800/60000 (75%)]\tLoss: 113.208931\n","Train Epoch: 7 [46080/60000 (77%)]\tLoss: 116.011688\n","Train Epoch: 7 [47360/60000 (79%)]\tLoss: 110.488358\n","Train Epoch: 7 [48640/60000 (81%)]\tLoss: 117.826248\n","Train Epoch: 7 [49920/60000 (83%)]\tLoss: 116.591072\n","Train Epoch: 7 [51200/60000 (85%)]\tLoss: 112.218483\n","Train Epoch: 7 [52480/60000 (87%)]\tLoss: 113.684990\n","Train Epoch: 7 [53760/60000 (90%)]\tLoss: 121.337334\n","Train Epoch: 7 [55040/60000 (92%)]\tLoss: 113.998322\n","Train Epoch: 7 [56320/60000 (94%)]\tLoss: 117.793526\n","Train Epoch: 7 [57600/60000 (96%)]\tLoss: 108.740547\n","Train Epoch: 7 [58880/60000 (98%)]\tLoss: 119.057198\n","====> Epoch: 7 Average loss: 117.2559\n","====> Test set loss: 115.1282\n","Train Epoch: 8 [0/60000 (0%)]\tLoss: 116.389420\n","Train Epoch: 8 [1280/60000 (2%)]\tLoss: 111.808128\n","Train Epoch: 8 [2560/60000 (4%)]\tLoss: 121.796265\n","Train Epoch: 8 [3840/60000 (6%)]\tLoss: 109.664986\n","Train Epoch: 8 [5120/60000 (9%)]\tLoss: 118.880547\n","Train Epoch: 8 [6400/60000 (11%)]\tLoss: 112.588058\n","Train Epoch: 8 [7680/60000 (13%)]\tLoss: 117.057983\n","Train Epoch: 8 [8960/60000 (15%)]\tLoss: 115.956703\n","Train Epoch: 8 [10240/60000 (17%)]\tLoss: 115.522583\n","Train Epoch: 8 [11520/60000 (19%)]\tLoss: 114.232407\n","Train Epoch: 8 [12800/60000 (21%)]\tLoss: 119.438339\n","Train Epoch: 8 [14080/60000 (23%)]\tLoss: 116.917801\n","Train Epoch: 8 [15360/60000 (26%)]\tLoss: 114.223450\n","Train Epoch: 8 [16640/60000 (28%)]\tLoss: 114.849548\n","Train Epoch: 8 [17920/60000 (30%)]\tLoss: 115.270500\n","Train Epoch: 8 [19200/60000 (32%)]\tLoss: 111.516220\n","Train Epoch: 8 [20480/60000 (34%)]\tLoss: 115.494751\n","Train Epoch: 8 [21760/60000 (36%)]\tLoss: 114.332161\n","Train Epoch: 8 [23040/60000 (38%)]\tLoss: 116.085320\n","Train Epoch: 8 [24320/60000 (41%)]\tLoss: 113.368935\n","Train Epoch: 8 [25600/60000 (43%)]\tLoss: 121.459976\n","Train Epoch: 8 [26880/60000 (45%)]\tLoss: 116.678131\n","Train Epoch: 8 [28160/60000 (47%)]\tLoss: 114.963448\n","Train Epoch: 8 [29440/60000 (49%)]\tLoss: 114.020660\n","Train Epoch: 8 [30720/60000 (51%)]\tLoss: 118.313461\n","Train Epoch: 8 [32000/60000 (53%)]\tLoss: 115.928497\n","Train Epoch: 8 [33280/60000 (55%)]\tLoss: 116.884201\n","Train Epoch: 8 [34560/60000 (58%)]\tLoss: 113.994232\n","Train Epoch: 8 [35840/60000 (60%)]\tLoss: 113.801163\n","Train Epoch: 8 [37120/60000 (62%)]\tLoss: 113.665161\n","Train Epoch: 8 [38400/60000 (64%)]\tLoss: 111.897736\n","Train Epoch: 8 [39680/60000 (66%)]\tLoss: 116.330772\n","Train Epoch: 8 [40960/60000 (68%)]\tLoss: 117.233398\n","Train Epoch: 8 [42240/60000 (70%)]\tLoss: 110.773468\n","Train Epoch: 8 [43520/60000 (72%)]\tLoss: 116.466507\n","Train Epoch: 8 [44800/60000 (75%)]\tLoss: 109.747887\n","Train Epoch: 8 [46080/60000 (77%)]\tLoss: 114.296997\n","Train Epoch: 8 [47360/60000 (79%)]\tLoss: 114.011932\n","Train Epoch: 8 [48640/60000 (81%)]\tLoss: 115.011810\n","Train Epoch: 8 [49920/60000 (83%)]\tLoss: 109.836624\n","Train Epoch: 8 [51200/60000 (85%)]\tLoss: 117.444984\n","Train Epoch: 8 [52480/60000 (87%)]\tLoss: 110.404854\n","Train Epoch: 8 [53760/60000 (90%)]\tLoss: 117.716888\n","Train Epoch: 8 [55040/60000 (92%)]\tLoss: 113.849808\n","Train Epoch: 8 [56320/60000 (94%)]\tLoss: 109.042015\n","Train Epoch: 8 [57600/60000 (96%)]\tLoss: 116.322601\n","Train Epoch: 8 [58880/60000 (98%)]\tLoss: 117.743958\n","====> Epoch: 8 Average loss: 114.7573\n","====> Test set loss: 113.0897\n","Train Epoch: 9 [0/60000 (0%)]\tLoss: 108.148315\n","Train Epoch: 9 [1280/60000 (2%)]\tLoss: 112.115494\n","Train Epoch: 9 [2560/60000 (4%)]\tLoss: 112.583046\n","Train Epoch: 9 [3840/60000 (6%)]\tLoss: 110.959579\n","Train Epoch: 9 [5120/60000 (9%)]\tLoss: 116.335266\n","Train Epoch: 9 [6400/60000 (11%)]\tLoss: 112.237457\n","Train Epoch: 9 [7680/60000 (13%)]\tLoss: 113.844475\n","Train Epoch: 9 [8960/60000 (15%)]\tLoss: 112.996674\n","Train Epoch: 9 [10240/60000 (17%)]\tLoss: 116.012726\n","Train Epoch: 9 [11520/60000 (19%)]\tLoss: 111.341232\n","Train Epoch: 9 [12800/60000 (21%)]\tLoss: 112.363152\n","Train Epoch: 9 [14080/60000 (23%)]\tLoss: 116.313980\n","Train Epoch: 9 [15360/60000 (26%)]\tLoss: 111.603661\n","Train Epoch: 9 [16640/60000 (28%)]\tLoss: 112.256737\n","Train Epoch: 9 [17920/60000 (30%)]\tLoss: 118.101921\n","Train Epoch: 9 [19200/60000 (32%)]\tLoss: 114.632874\n","Train Epoch: 9 [20480/60000 (34%)]\tLoss: 114.054573\n","Train Epoch: 9 [21760/60000 (36%)]\tLoss: 114.629478\n","Train Epoch: 9 [23040/60000 (38%)]\tLoss: 115.691704\n","Train Epoch: 9 [24320/60000 (41%)]\tLoss: 117.147606\n","Train Epoch: 9 [25600/60000 (43%)]\tLoss: 107.966820\n","Train Epoch: 9 [26880/60000 (45%)]\tLoss: 113.325974\n","Train Epoch: 9 [28160/60000 (47%)]\tLoss: 114.674263\n","Train Epoch: 9 [29440/60000 (49%)]\tLoss: 113.884834\n","Train Epoch: 9 [30720/60000 (51%)]\tLoss: 110.936966\n","Train Epoch: 9 [32000/60000 (53%)]\tLoss: 115.247833\n","Train Epoch: 9 [33280/60000 (55%)]\tLoss: 116.268997\n","Train Epoch: 9 [34560/60000 (58%)]\tLoss: 110.351219\n","Train Epoch: 9 [35840/60000 (60%)]\tLoss: 110.492981\n","Train Epoch: 9 [37120/60000 (62%)]\tLoss: 118.233055\n","Train Epoch: 9 [38400/60000 (64%)]\tLoss: 109.324272\n","Train Epoch: 9 [39680/60000 (66%)]\tLoss: 111.568062\n","Train Epoch: 9 [40960/60000 (68%)]\tLoss: 118.527512\n","Train Epoch: 9 [42240/60000 (70%)]\tLoss: 108.341614\n","Train Epoch: 9 [43520/60000 (72%)]\tLoss: 111.134552\n","Train Epoch: 9 [44800/60000 (75%)]\tLoss: 110.881386\n","Train Epoch: 9 [46080/60000 (77%)]\tLoss: 110.618103\n","Train Epoch: 9 [47360/60000 (79%)]\tLoss: 113.447090\n","Train Epoch: 9 [48640/60000 (81%)]\tLoss: 107.911560\n","Train Epoch: 9 [49920/60000 (83%)]\tLoss: 107.556580\n","Train Epoch: 9 [51200/60000 (85%)]\tLoss: 114.084763\n","Train Epoch: 9 [52480/60000 (87%)]\tLoss: 109.613365\n","Train Epoch: 9 [53760/60000 (90%)]\tLoss: 111.550186\n","Train Epoch: 9 [55040/60000 (92%)]\tLoss: 112.553864\n","Train Epoch: 9 [56320/60000 (94%)]\tLoss: 111.131302\n","Train Epoch: 9 [57600/60000 (96%)]\tLoss: 110.440521\n","Train Epoch: 9 [58880/60000 (98%)]\tLoss: 108.264336\n","====> Epoch: 9 Average loss: 112.9200\n","====> Test set loss: 111.6223\n","Train Epoch: 10 [0/60000 (0%)]\tLoss: 106.681976\n","Train Epoch: 10 [1280/60000 (2%)]\tLoss: 116.066498\n","Train Epoch: 10 [2560/60000 (4%)]\tLoss: 113.787033\n","Train Epoch: 10 [3840/60000 (6%)]\tLoss: 104.719284\n","Train Epoch: 10 [5120/60000 (9%)]\tLoss: 113.589874\n","Train Epoch: 10 [6400/60000 (11%)]\tLoss: 109.584969\n","Train Epoch: 10 [7680/60000 (13%)]\tLoss: 107.700264\n","Train Epoch: 10 [8960/60000 (15%)]\tLoss: 110.214691\n","Train Epoch: 10 [10240/60000 (17%)]\tLoss: 103.398613\n","Train Epoch: 10 [11520/60000 (19%)]\tLoss: 107.403534\n","Train Epoch: 10 [12800/60000 (21%)]\tLoss: 107.291733\n","Train Epoch: 10 [14080/60000 (23%)]\tLoss: 108.007004\n","Train Epoch: 10 [15360/60000 (26%)]\tLoss: 112.604950\n","Train Epoch: 10 [16640/60000 (28%)]\tLoss: 108.525887\n","Train Epoch: 10 [17920/60000 (30%)]\tLoss: 109.497757\n","Train Epoch: 10 [19200/60000 (32%)]\tLoss: 115.061852\n","Train Epoch: 10 [20480/60000 (34%)]\tLoss: 111.476860\n","Train Epoch: 10 [21760/60000 (36%)]\tLoss: 104.341835\n","Train Epoch: 10 [23040/60000 (38%)]\tLoss: 104.717148\n","Train Epoch: 10 [24320/60000 (41%)]\tLoss: 113.269653\n","Train Epoch: 10 [25600/60000 (43%)]\tLoss: 112.699394\n","Train Epoch: 10 [26880/60000 (45%)]\tLoss: 111.836403\n","Train Epoch: 10 [28160/60000 (47%)]\tLoss: 104.739563\n","Train Epoch: 10 [29440/60000 (49%)]\tLoss: 111.492889\n","Train Epoch: 10 [30720/60000 (51%)]\tLoss: 115.308281\n","Train Epoch: 10 [32000/60000 (53%)]\tLoss: 112.144135\n","Train Epoch: 10 [33280/60000 (55%)]\tLoss: 108.484604\n","Train Epoch: 10 [34560/60000 (58%)]\tLoss: 108.062393\n","Train Epoch: 10 [35840/60000 (60%)]\tLoss: 113.678856\n","Train Epoch: 10 [37120/60000 (62%)]\tLoss: 114.874725\n","Train Epoch: 10 [38400/60000 (64%)]\tLoss: 108.903091\n","Train Epoch: 10 [39680/60000 (66%)]\tLoss: 112.446381\n","Train Epoch: 10 [40960/60000 (68%)]\tLoss: 112.725861\n","Train Epoch: 10 [42240/60000 (70%)]\tLoss: 111.611488\n","Train Epoch: 10 [43520/60000 (72%)]\tLoss: 109.227554\n","Train Epoch: 10 [44800/60000 (75%)]\tLoss: 110.740158\n","Train Epoch: 10 [46080/60000 (77%)]\tLoss: 113.230270\n","Train Epoch: 10 [47360/60000 (79%)]\tLoss: 109.805138\n","Train Epoch: 10 [48640/60000 (81%)]\tLoss: 105.285347\n","Train Epoch: 10 [49920/60000 (83%)]\tLoss: 109.683777\n","Train Epoch: 10 [51200/60000 (85%)]\tLoss: 109.643456\n","Train Epoch: 10 [52480/60000 (87%)]\tLoss: 110.226700\n","Train Epoch: 10 [53760/60000 (90%)]\tLoss: 106.262283\n","Train Epoch: 10 [55040/60000 (92%)]\tLoss: 108.018341\n","Train Epoch: 10 [56320/60000 (94%)]\tLoss: 111.056580\n","Train Epoch: 10 [57600/60000 (96%)]\tLoss: 114.241592\n","Train Epoch: 10 [58880/60000 (98%)]\tLoss: 118.959076\n","====> Epoch: 10 Average loss: 111.2571\n","====> Test set loss: 110.0822\n","Train Epoch: 11 [0/60000 (0%)]\tLoss: 105.054581\n","Train Epoch: 11 [1280/60000 (2%)]\tLoss: 111.275146\n","Train Epoch: 11 [2560/60000 (4%)]\tLoss: 105.567345\n","Train Epoch: 11 [3840/60000 (6%)]\tLoss: 104.168144\n","Train Epoch: 11 [5120/60000 (9%)]\tLoss: 109.343758\n","Train Epoch: 11 [6400/60000 (11%)]\tLoss: 109.358345\n","Train Epoch: 11 [7680/60000 (13%)]\tLoss: 111.977242\n","Train Epoch: 11 [8960/60000 (15%)]\tLoss: 101.429047\n","Train Epoch: 11 [10240/60000 (17%)]\tLoss: 115.005386\n","Train Epoch: 11 [11520/60000 (19%)]\tLoss: 110.514488\n","Train Epoch: 11 [12800/60000 (21%)]\tLoss: 108.557877\n","Train Epoch: 11 [14080/60000 (23%)]\tLoss: 107.950294\n","Train Epoch: 11 [15360/60000 (26%)]\tLoss: 114.914085\n","Train Epoch: 11 [16640/60000 (28%)]\tLoss: 104.443230\n","Train Epoch: 11 [17920/60000 (30%)]\tLoss: 112.871185\n","Train Epoch: 11 [19200/60000 (32%)]\tLoss: 110.305214\n","Train Epoch: 11 [20480/60000 (34%)]\tLoss: 111.303322\n","Train Epoch: 11 [21760/60000 (36%)]\tLoss: 113.740517\n","Train Epoch: 11 [23040/60000 (38%)]\tLoss: 114.476791\n","Train Epoch: 11 [24320/60000 (41%)]\tLoss: 114.318741\n","Train Epoch: 11 [25600/60000 (43%)]\tLoss: 109.658195\n","Train Epoch: 11 [26880/60000 (45%)]\tLoss: 107.035614\n","Train Epoch: 11 [28160/60000 (47%)]\tLoss: 111.239838\n","Train Epoch: 11 [29440/60000 (49%)]\tLoss: 114.372437\n","Train Epoch: 11 [30720/60000 (51%)]\tLoss: 106.204048\n","Train Epoch: 11 [32000/60000 (53%)]\tLoss: 112.582169\n","Train Epoch: 11 [33280/60000 (55%)]\tLoss: 108.497795\n","Train Epoch: 11 [34560/60000 (58%)]\tLoss: 110.495827\n","Train Epoch: 11 [35840/60000 (60%)]\tLoss: 114.175125\n","Train Epoch: 11 [37120/60000 (62%)]\tLoss: 108.390724\n","Train Epoch: 11 [38400/60000 (64%)]\tLoss: 109.487434\n","Train Epoch: 11 [39680/60000 (66%)]\tLoss: 105.657326\n","Train Epoch: 11 [40960/60000 (68%)]\tLoss: 107.245026\n","Train Epoch: 11 [42240/60000 (70%)]\tLoss: 115.411400\n","Train Epoch: 11 [43520/60000 (72%)]\tLoss: 108.007225\n","Train Epoch: 11 [44800/60000 (75%)]\tLoss: 114.090286\n","Train Epoch: 11 [46080/60000 (77%)]\tLoss: 111.502182\n","Train Epoch: 11 [47360/60000 (79%)]\tLoss: 113.245430\n","Train Epoch: 11 [48640/60000 (81%)]\tLoss: 103.939255\n","Train Epoch: 11 [49920/60000 (83%)]\tLoss: 111.023155\n","Train Epoch: 11 [51200/60000 (85%)]\tLoss: 104.919441\n","Train Epoch: 11 [52480/60000 (87%)]\tLoss: 112.281342\n","Train Epoch: 11 [53760/60000 (90%)]\tLoss: 110.040863\n","Train Epoch: 11 [55040/60000 (92%)]\tLoss: 109.668785\n","Train Epoch: 11 [56320/60000 (94%)]\tLoss: 109.821884\n","Train Epoch: 11 [57600/60000 (96%)]\tLoss: 104.510376\n","Train Epoch: 11 [58880/60000 (98%)]\tLoss: 108.386383\n","====> Epoch: 11 Average loss: 109.9506\n","====> Test set loss: 108.7318\n","Train Epoch: 12 [0/60000 (0%)]\tLoss: 109.069077\n","Train Epoch: 12 [1280/60000 (2%)]\tLoss: 103.764763\n","Train Epoch: 12 [2560/60000 (4%)]\tLoss: 110.375114\n","Train Epoch: 12 [3840/60000 (6%)]\tLoss: 113.477203\n","Train Epoch: 12 [5120/60000 (9%)]\tLoss: 109.876587\n","Train Epoch: 12 [6400/60000 (11%)]\tLoss: 116.516846\n","Train Epoch: 12 [7680/60000 (13%)]\tLoss: 104.713730\n","Train Epoch: 12 [8960/60000 (15%)]\tLoss: 109.130272\n","Train Epoch: 12 [10240/60000 (17%)]\tLoss: 111.580116\n","Train Epoch: 12 [11520/60000 (19%)]\tLoss: 108.788788\n","Train Epoch: 12 [12800/60000 (21%)]\tLoss: 109.751366\n","Train Epoch: 12 [14080/60000 (23%)]\tLoss: 111.261856\n","Train Epoch: 12 [15360/60000 (26%)]\tLoss: 108.617165\n","Train Epoch: 12 [16640/60000 (28%)]\tLoss: 107.809227\n","Train Epoch: 12 [17920/60000 (30%)]\tLoss: 105.455833\n","Train Epoch: 12 [19200/60000 (32%)]\tLoss: 111.574097\n","Train Epoch: 12 [20480/60000 (34%)]\tLoss: 105.399422\n","Train Epoch: 12 [21760/60000 (36%)]\tLoss: 111.076744\n","Train Epoch: 12 [23040/60000 (38%)]\tLoss: 111.872513\n","Train Epoch: 12 [24320/60000 (41%)]\tLoss: 106.489609\n","Train Epoch: 12 [25600/60000 (43%)]\tLoss: 107.078880\n","Train Epoch: 12 [26880/60000 (45%)]\tLoss: 106.951485\n","Train Epoch: 12 [28160/60000 (47%)]\tLoss: 107.835800\n","Train Epoch: 12 [29440/60000 (49%)]\tLoss: 106.630272\n","Train Epoch: 12 [30720/60000 (51%)]\tLoss: 107.833862\n","Train Epoch: 12 [32000/60000 (53%)]\tLoss: 109.010674\n","Train Epoch: 12 [33280/60000 (55%)]\tLoss: 113.395370\n","Train Epoch: 12 [34560/60000 (58%)]\tLoss: 107.634506\n","Train Epoch: 12 [35840/60000 (60%)]\tLoss: 105.479149\n","Train Epoch: 12 [37120/60000 (62%)]\tLoss: 107.290207\n","Train Epoch: 12 [38400/60000 (64%)]\tLoss: 111.949646\n","Train Epoch: 12 [39680/60000 (66%)]\tLoss: 106.468575\n","Train Epoch: 12 [40960/60000 (68%)]\tLoss: 108.346413\n","Train Epoch: 12 [42240/60000 (70%)]\tLoss: 110.041702\n","Train Epoch: 12 [43520/60000 (72%)]\tLoss: 112.552414\n","Train Epoch: 12 [44800/60000 (75%)]\tLoss: 108.389488\n","Train Epoch: 12 [46080/60000 (77%)]\tLoss: 107.160873\n","Train Epoch: 12 [47360/60000 (79%)]\tLoss: 106.026749\n","Train Epoch: 12 [48640/60000 (81%)]\tLoss: 106.039734\n","Train Epoch: 12 [49920/60000 (83%)]\tLoss: 110.160309\n","Train Epoch: 12 [51200/60000 (85%)]\tLoss: 105.830063\n","Train Epoch: 12 [52480/60000 (87%)]\tLoss: 109.956299\n","Train Epoch: 12 [53760/60000 (90%)]\tLoss: 104.313759\n","Train Epoch: 12 [55040/60000 (92%)]\tLoss: 112.310547\n","Train Epoch: 12 [56320/60000 (94%)]\tLoss: 110.921120\n","Train Epoch: 12 [57600/60000 (96%)]\tLoss: 112.521690\n","Train Epoch: 12 [58880/60000 (98%)]\tLoss: 107.361542\n","====> Epoch: 12 Average loss: 108.9986\n","====> Test set loss: 108.3090\n","Train Epoch: 13 [0/60000 (0%)]\tLoss: 109.895119\n","Train Epoch: 13 [1280/60000 (2%)]\tLoss: 107.065750\n","Train Epoch: 13 [2560/60000 (4%)]\tLoss: 108.314255\n","Train Epoch: 13 [3840/60000 (6%)]\tLoss: 106.488632\n","Train Epoch: 13 [5120/60000 (9%)]\tLoss: 107.635712\n","Train Epoch: 13 [6400/60000 (11%)]\tLoss: 111.151268\n","Train Epoch: 13 [7680/60000 (13%)]\tLoss: 111.315620\n","Train Epoch: 13 [8960/60000 (15%)]\tLoss: 105.982010\n","Train Epoch: 13 [10240/60000 (17%)]\tLoss: 108.734390\n","Train Epoch: 13 [11520/60000 (19%)]\tLoss: 106.318344\n","Train Epoch: 13 [12800/60000 (21%)]\tLoss: 104.543610\n","Train Epoch: 13 [14080/60000 (23%)]\tLoss: 109.337204\n","Train Epoch: 13 [15360/60000 (26%)]\tLoss: 104.692368\n","Train Epoch: 13 [16640/60000 (28%)]\tLoss: 106.293221\n","Train Epoch: 13 [17920/60000 (30%)]\tLoss: 109.228447\n","Train Epoch: 13 [19200/60000 (32%)]\tLoss: 105.690811\n","Train Epoch: 13 [20480/60000 (34%)]\tLoss: 111.708145\n","Train Epoch: 13 [21760/60000 (36%)]\tLoss: 112.065407\n","Train Epoch: 13 [23040/60000 (38%)]\tLoss: 111.252876\n","Train Epoch: 13 [24320/60000 (41%)]\tLoss: 110.356354\n","Train Epoch: 13 [25600/60000 (43%)]\tLoss: 106.286560\n","Train Epoch: 13 [26880/60000 (45%)]\tLoss: 107.947090\n","Train Epoch: 13 [28160/60000 (47%)]\tLoss: 106.630455\n","Train Epoch: 13 [29440/60000 (49%)]\tLoss: 106.844048\n","Train Epoch: 13 [30720/60000 (51%)]\tLoss: 107.758881\n","Train Epoch: 13 [32000/60000 (53%)]\tLoss: 106.217461\n","Train Epoch: 13 [33280/60000 (55%)]\tLoss: 108.532883\n","Train Epoch: 13 [34560/60000 (58%)]\tLoss: 104.178810\n","Train Epoch: 13 [35840/60000 (60%)]\tLoss: 107.768982\n","Train Epoch: 13 [37120/60000 (62%)]\tLoss: 106.082741\n","Train Epoch: 13 [38400/60000 (64%)]\tLoss: 110.982300\n","Train Epoch: 13 [39680/60000 (66%)]\tLoss: 107.687271\n","Train Epoch: 13 [40960/60000 (68%)]\tLoss: 106.964874\n","Train Epoch: 13 [42240/60000 (70%)]\tLoss: 109.750961\n","Train Epoch: 13 [43520/60000 (72%)]\tLoss: 109.535522\n","Train Epoch: 13 [44800/60000 (75%)]\tLoss: 107.829765\n","Train Epoch: 13 [46080/60000 (77%)]\tLoss: 108.103577\n","Train Epoch: 13 [47360/60000 (79%)]\tLoss: 105.841339\n","Train Epoch: 13 [48640/60000 (81%)]\tLoss: 111.058907\n","Train Epoch: 13 [49920/60000 (83%)]\tLoss: 109.954025\n","Train Epoch: 13 [51200/60000 (85%)]\tLoss: 105.196381\n","Train Epoch: 13 [52480/60000 (87%)]\tLoss: 111.382507\n","Train Epoch: 13 [53760/60000 (90%)]\tLoss: 107.158653\n","Train Epoch: 13 [55040/60000 (92%)]\tLoss: 110.197861\n","Train Epoch: 13 [56320/60000 (94%)]\tLoss: 105.143219\n","Train Epoch: 13 [57600/60000 (96%)]\tLoss: 108.448006\n","Train Epoch: 13 [58880/60000 (98%)]\tLoss: 102.916618\n","====> Epoch: 13 Average loss: 107.9204\n","====> Test set loss: 107.2122\n","Train Epoch: 14 [0/60000 (0%)]\tLoss: 108.230965\n","Train Epoch: 14 [1280/60000 (2%)]\tLoss: 108.179703\n","Train Epoch: 14 [2560/60000 (4%)]\tLoss: 104.332451\n","Train Epoch: 14 [3840/60000 (6%)]\tLoss: 109.943649\n","Train Epoch: 14 [5120/60000 (9%)]\tLoss: 107.345833\n","Train Epoch: 14 [6400/60000 (11%)]\tLoss: 107.180351\n","Train Epoch: 14 [7680/60000 (13%)]\tLoss: 109.643364\n","Train Epoch: 14 [8960/60000 (15%)]\tLoss: 108.173004\n","Train Epoch: 14 [10240/60000 (17%)]\tLoss: 108.172585\n","Train Epoch: 14 [11520/60000 (19%)]\tLoss: 104.108910\n","Train Epoch: 14 [12800/60000 (21%)]\tLoss: 103.603722\n","Train Epoch: 14 [14080/60000 (23%)]\tLoss: 108.190567\n","Train Epoch: 14 [15360/60000 (26%)]\tLoss: 105.594017\n","Train Epoch: 14 [16640/60000 (28%)]\tLoss: 105.255775\n","Train Epoch: 14 [17920/60000 (30%)]\tLoss: 107.109970\n","Train Epoch: 14 [19200/60000 (32%)]\tLoss: 107.818687\n","Train Epoch: 14 [20480/60000 (34%)]\tLoss: 105.372360\n","Train Epoch: 14 [21760/60000 (36%)]\tLoss: 113.334732\n","Train Epoch: 14 [23040/60000 (38%)]\tLoss: 106.945038\n","Train Epoch: 14 [24320/60000 (41%)]\tLoss: 104.191063\n","Train Epoch: 14 [25600/60000 (43%)]\tLoss: 106.056641\n","Train Epoch: 14 [26880/60000 (45%)]\tLoss: 107.636368\n","Train Epoch: 14 [28160/60000 (47%)]\tLoss: 108.341301\n","Train Epoch: 14 [29440/60000 (49%)]\tLoss: 107.265137\n","Train Epoch: 14 [30720/60000 (51%)]\tLoss: 106.207870\n","Train Epoch: 14 [32000/60000 (53%)]\tLoss: 105.886887\n","Train Epoch: 14 [33280/60000 (55%)]\tLoss: 109.439117\n","Train Epoch: 14 [34560/60000 (58%)]\tLoss: 104.802658\n","Train Epoch: 14 [35840/60000 (60%)]\tLoss: 107.312607\n","Train Epoch: 14 [37120/60000 (62%)]\tLoss: 110.148537\n","Train Epoch: 14 [38400/60000 (64%)]\tLoss: 109.974731\n","Train Epoch: 14 [39680/60000 (66%)]\tLoss: 104.965736\n","Train Epoch: 14 [40960/60000 (68%)]\tLoss: 100.660469\n","Train Epoch: 14 [42240/60000 (70%)]\tLoss: 104.242500\n","Train Epoch: 14 [43520/60000 (72%)]\tLoss: 104.071465\n","Train Epoch: 14 [44800/60000 (75%)]\tLoss: 107.961418\n","Train Epoch: 14 [46080/60000 (77%)]\tLoss: 108.424545\n","Train Epoch: 14 [47360/60000 (79%)]\tLoss: 104.138496\n","Train Epoch: 14 [48640/60000 (81%)]\tLoss: 105.061195\n","Train Epoch: 14 [49920/60000 (83%)]\tLoss: 106.425980\n","Train Epoch: 14 [51200/60000 (85%)]\tLoss: 111.178345\n","Train Epoch: 14 [52480/60000 (87%)]\tLoss: 109.370956\n","Train Epoch: 14 [53760/60000 (90%)]\tLoss: 105.374504\n","Train Epoch: 14 [55040/60000 (92%)]\tLoss: 105.387207\n","Train Epoch: 14 [56320/60000 (94%)]\tLoss: 107.638634\n","Train Epoch: 14 [57600/60000 (96%)]\tLoss: 108.073570\n","Train Epoch: 14 [58880/60000 (98%)]\tLoss: 111.029640\n","====> Epoch: 14 Average loss: 107.1730\n","====> Test set loss: 106.5693\n","Train Epoch: 15 [0/60000 (0%)]\tLoss: 108.072067\n","Train Epoch: 15 [1280/60000 (2%)]\tLoss: 111.855881\n","Train Epoch: 15 [2560/60000 (4%)]\tLoss: 110.311279\n","Train Epoch: 15 [3840/60000 (6%)]\tLoss: 110.320343\n","Train Epoch: 15 [5120/60000 (9%)]\tLoss: 102.910583\n","Train Epoch: 15 [6400/60000 (11%)]\tLoss: 109.373093\n","Train Epoch: 15 [7680/60000 (13%)]\tLoss: 104.355034\n","Train Epoch: 15 [8960/60000 (15%)]\tLoss: 103.543137\n","Train Epoch: 15 [10240/60000 (17%)]\tLoss: 104.547821\n","Train Epoch: 15 [11520/60000 (19%)]\tLoss: 107.598152\n","Train Epoch: 15 [12800/60000 (21%)]\tLoss: 104.792023\n","Train Epoch: 15 [14080/60000 (23%)]\tLoss: 108.024445\n","Train Epoch: 15 [15360/60000 (26%)]\tLoss: 109.441223\n","Train Epoch: 15 [16640/60000 (28%)]\tLoss: 106.353348\n","Train Epoch: 15 [17920/60000 (30%)]\tLoss: 105.991043\n","Train Epoch: 15 [19200/60000 (32%)]\tLoss: 104.420639\n","Train Epoch: 15 [20480/60000 (34%)]\tLoss: 105.235016\n","Train Epoch: 15 [21760/60000 (36%)]\tLoss: 109.333847\n","Train Epoch: 15 [23040/60000 (38%)]\tLoss: 102.673630\n","Train Epoch: 15 [24320/60000 (41%)]\tLoss: 104.111916\n","Train Epoch: 15 [25600/60000 (43%)]\tLoss: 104.160339\n","Train Epoch: 15 [26880/60000 (45%)]\tLoss: 107.683319\n","Train Epoch: 15 [28160/60000 (47%)]\tLoss: 105.268860\n","Train Epoch: 15 [29440/60000 (49%)]\tLoss: 103.334740\n","Train Epoch: 15 [30720/60000 (51%)]\tLoss: 105.994713\n","Train Epoch: 15 [32000/60000 (53%)]\tLoss: 109.515579\n","Train Epoch: 15 [33280/60000 (55%)]\tLoss: 109.907501\n","Train Epoch: 15 [34560/60000 (58%)]\tLoss: 107.196281\n","Train Epoch: 15 [35840/60000 (60%)]\tLoss: 106.211220\n","Train Epoch: 15 [37120/60000 (62%)]\tLoss: 105.334824\n","Train Epoch: 15 [38400/60000 (64%)]\tLoss: 109.024910\n","Train Epoch: 15 [39680/60000 (66%)]\tLoss: 110.647415\n","Train Epoch: 15 [40960/60000 (68%)]\tLoss: 107.397400\n","Train Epoch: 15 [42240/60000 (70%)]\tLoss: 104.238068\n","Train Epoch: 15 [43520/60000 (72%)]\tLoss: 104.622826\n","Train Epoch: 15 [44800/60000 (75%)]\tLoss: 104.418823\n","Train Epoch: 15 [46080/60000 (77%)]\tLoss: 101.221588\n","Train Epoch: 15 [47360/60000 (79%)]\tLoss: 106.298988\n","Train Epoch: 15 [48640/60000 (81%)]\tLoss: 103.459999\n","Train Epoch: 15 [49920/60000 (83%)]\tLoss: 104.358551\n","Train Epoch: 15 [51200/60000 (85%)]\tLoss: 107.179977\n","Train Epoch: 15 [52480/60000 (87%)]\tLoss: 111.293686\n","Train Epoch: 15 [53760/60000 (90%)]\tLoss: 103.761391\n","Train Epoch: 15 [55040/60000 (92%)]\tLoss: 104.011887\n","Train Epoch: 15 [56320/60000 (94%)]\tLoss: 106.789200\n","Train Epoch: 15 [57600/60000 (96%)]\tLoss: 110.621948\n","Train Epoch: 15 [58880/60000 (98%)]\tLoss: 104.149147\n","====> Epoch: 15 Average loss: 106.4435\n","====> Test set loss: 105.8580\n"]}]}]}